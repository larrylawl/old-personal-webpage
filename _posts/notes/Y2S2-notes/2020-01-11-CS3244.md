---
layout: post
title: "NUS, CS3244: Machine Learning"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: Low Kian Hsiang <br>

<!-- omit in toc -->
# Table of Contents
- [Introduction](#introduction)
	- [What is Learning?](#what-is-learning)
- [Concept Learning and the General-To-Specific Ordering](#concept-learning-and-the-general-to-specific-ordering)
	- [Introduction](#introduction-1)
	- [How to represent a Hypothesis?](#how-to-represent-a-hypothesis)
	- [A Concept Learning Task](#a-concept-learning-task)
		- [Notations](#notations)
		- [The Inductive Learning Hypothesis](#the-inductive-learning-hypothesis)
	- [Concept Learning as Search](#concept-learning-as-search)
		- [General-To-Specific Ordering of Hypotheses](#general-to-specific-ordering-of-hypotheses)
	- [Find-S: Finding a maximally specific hypothesis](#find-s-finding-a-maximally-specific-hypothesis)
	- [Version Spaces and the Candidate-Elimination Algorithm](#version-spaces-and-the-candidate-elimination-algorithm)
		- [The List-Then-Eliminate Algorithm](#the-list-then-eliminate-algorithm)
		- [A More compact Representation for Version Spaces](#a-more-compact-representation-for-version-spaces)
		- [Candidate-Elimination Learning Algorithm](#candidate-elimination-learning-algorithm)
	- [Inductive Bias](#inductive-bias)
		- [An Unbiased Learner](#an-unbiased-learner)
		- [Inductive Bias](#inductive-bias-1)
		- [Comparing Learners with Different Inductive Biases](#comparing-learners-with-different-inductive-biases)
- [Decision Trees](#decision-trees)
	- [Expressive Power](#expressive-power)
	- [Hypothesis/Search Space](#hypothesissearch-space)
	- [Decision-Tree-Learning](#decision-tree-learning)
		- [Which Attribute is the Best Classifier?](#which-attribute-is-the-best-classifier)
		- [Entropy Measures Homogeneity of Examples](#entropy-measures-homogeneity-of-examples)
		- [Information Gain](#information-gain)
		- [Inductive Bias of Decision-Tree-Learning](#inductive-bias-of-decision-tree-learning)
	- [Occam's Razor](#occams-razor)
		- [Overfitting](#overfitting)
			- [Reduced-Error Pruning](#reduced-error-pruning)
			- [Rule Post-Pruning](#rule-post-pruning)
		- [Continuous-Valued Attributes](#continuous-valued-attributes)
		- [Attributes with Many Values](#attributes-with-many-values)
		- [Attributes with Differing Costs](#attributes-with-differing-costs)
		- [Missing Attribute Values](#missing-attribute-values)

# Introduction
## What is Learning?
An agent is said to be *learning* if it improves its *performance P* on *task T* based on *experience/observations/data E*.

> T must be fixed, P must be measurable, E must in.

# Concept Learning and the General-To-Specific Ordering
1. Learning from Examples
2. General-to-specific ordering over hypothesis
3. Version spaces and candidate elimination algorithm
4. Picking new examples
5. The need for inductive bias

## Introduction
**Concept Learning:** Inferring a boolean-valued fn from training examples of its input and output

> Concept learning is also a form of supervised learning

## How to represent a Hypothesis?

Quadratic is more expressive than linear, but it is the expense of a larger hypothesis space

## A Concept Learning Task
### Notations
![Concept Learning Task](/assets/img/2020-16-1-CS3244/concept-learning-task.png)

> Hypothesis *h* has a tradeoff between **expressive power vs. smaller hypothesis space.** For eg, compare a linear function *f(x) = mx + c* w a quadratic function *f(x) = ax^2 + bx + c*. The latter is more expressive than the former (it can still rep linear functions), but it has a larger hypothesis space (parameters can take on {A x B x C} set of values, as opposed to {M X C})

**Definition.** An input instance \$ x \in X \$ satisfies (all constraints of) a hypothesis \$ h \in H \$ iff *h(x) = 1*. In other words, *h* classifies *x* as a +ve example.

**Objective:** Determine a hypothesis \$ h \in H \$ that is **consistent** with *D*

**Definition.** A hypothesis *h* is **consistent** with a set of training examples *D* iff \$ \forall \langle x, c(x) \rangle \in D, h(x) = c(x) \$.

### The Inductive Learning Hypothesis
**Definition.** Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved
examples 

## Concept Learning as Search
**Goal.** Search for a hypothesis \$ h \in H \$ that is **consistent** with *D*

Since hypothesis space *H* is much larger and possibly infinite, we need to *exploit structure* to search efficiently.

>Synthetically vs Semantically distinct
1. **Synthetically Distinct:** Include 1) *?*: don't care and 2) \$ \emptyset \$. "Looks different." *(3x2 -> 5x4)*
2. **Sementically Distinct:** 
   1. Include *?* 
   2. Every hypothesis containing 1 or more \$ \emptyset \$ is equivalent to an empty set of input instances, hence classifying every instance as a negative example. *(3x2 -> 4x3 + 1)*

### General-To-Specific Ordering of Hypotheses
**Definition.** \$ h_j \$ is more general than or equal to \$ h_k \$ (denoted by \$ h_j ≥_g h_k \$) iff any input instance x that satisfies \$ h_k \$ also satisfies \$ h_j \$

$$
\forall x \in X (h_{k}(x) = 1) \rightarrow (h_{j}(x) = 1)
$$

> \$ ≥_g \$ defines a [partial order](http://mathworld.wolfram.com/PartialOrder.html)(reflexive, antisymmetric, transitive) over *H* and not [total order](http://mathworld.wolfram.com/TotallyOrderedSet.html) (partial order and comparability condition).

**Definition.** \$ h_j \$ is more general than \$ h_k \$ (denoted by \$ h_j >_g h_k \$) iff \$ h_j ≥_g h_k \$ and \$ h_k ≱_g h_j \$

**Definition.** \$ h_j \$ is more specific than \$ h_k \$ iff \$ h_k \$ is more general than \$ h_j \$.

## Find-S: Finding a maximally specific hypothesis

**Intuition.** Start with most specific hypothesis. Whenever it wrongly classifies a +ve training example as −ve, “minimally” generalize it to satisfy its input instance.

1. Initialize *h* to most specific hypothesis in *H*
2. For each +ve training instance *x*
   1. For each attribute constraint \$ a_i \$ in *h*
      1. If *x* satisfies constraint \$ a_i \$ in *h*, then do nothing.
      2. Else, replace \$ a_i \$ in *h* by the next more general constraint that is satisfied by x
3. Output hypothesis *h*

**Proposition 1.** *h* is consistent with *D* iff every +ve training instance satisfies *h* and every -ve training instance does not satisfy *h*

> If there are no -ve instance, then the latter will be vacuously true.

**Proposition 2.** Suppose that \$ c \in H \$. Then, \$ h_n \$ is consistent with 
$$
D=\left\{\left\langle x_{k}, c\left(x_{k}\right)\right\rangle\right\}_{k=1, \ldots, n}
$$

> \$ h_n \$ is consistent with every preceding instances, inclduing negative training instance which was skipped.

Limitations
1. Can't tell whether Find-S has learned target concept
2. Can't tell when training examples are inconsistent (ie contains errors or noise)
3. Picks a maximally spsecific *h*
4. Depending on *H*, there might be several

## Version Spaces and the Candidate-Elimination Algorithm

**Definition.** The **version space** \$ VS_{H,D} \$ wrt hypothesis space *H* and training examples *D*, is the subset of hypothesis from *H* consistent with *D*:

$$
VS_{H,D} = \{ h \in H \mid \text{h is consistent with D} \}
$$

1. If \$ c \ in H \$, then a large enough *D* can reduce \$ VS_{H,D} \$ to *{c}*.
2. If *D* is insufficient, then \$ VS_{H,D} \$ represents the **uncertainty** of what the target concept is 
   1. If *D* is insufficient, then \$ VS_{H,D} \$ will be too general, thus it represents the uncertainty.
3. \$ VS_{H,D} \$ contains all consistent hypothesis, including the maximally specific hypotheses

### The List-Then-Eliminate Algorithm

**Intuition.** List all hypotheses in *H*. Then, eliminate any hypothesis found inconsistent with any training example.

1. *VS*: a list containing every hypothesis in *H*
2. For each training example \$ \langle x, c(x) \rangle \$
   1. Remove from VS any hypothesis *h* for which *h(x) ≠ c(x)*
3. Output the list of hypothesis in VS

**Limitation.** Prohibitively expensive to exhaustively enumerate all hypothesis in finite *H*

### A More compact Representation for Version Spaces
![Version Space Compact Representation](/assets/img/2020-16-1-CS3244/version-space-compact-rep.png)

**Definition.** The **general boundary** *G* of \$ VS_{H,D} \$ is the set of maximally general members of *H* consistent with *D:*

$$
G=\left\{g \in H \mid g \text { consistent with } D \wedge\left(\neg \exists g^{\prime} \in H g^{\prime}>_{g} g \wedge g^{\prime} \text { consistent with } D\right)\right\}
$$

**Definition.** The **specific boundary** *S* of \$ VS_{H,D} \$ is the set of maximally specific members of *H* consistent with *D:*

$$
S=\left\{s \in H \mid s \text { consistent with } D \wedge\left(\neg \exists s^{\prime} \in H s>_{g} s^{\prime} \wedge s^{\prime} \text { consistent with } D\right)\right\}
$$

Every member of version space lies between these boundaries. **Version Space Representation Theorem:** (VSRT)

$$
V S_{H, D}=\left\{h \in H \mid \exists s \in S \exists g \in G g \geq_{g} h \geq_{g} s\right\}
$$

### Candidate-Elimination Learning Algorithm
**Intutiion.** Start with most general and specific hypotheses. Each training example "minimally" generalizes *S* and specialises *G* to remove inconsistent hypothesis from version space.

1. For each training example *d*
   1. if *d* is a +ve example
      1. Remove from *G* any hypothesis inconsistent with *d*
      2. for each \$ s \in S \$ not consistent with *d*
         1. Remove *s* from S
         2. Add to *S* all minimal generalisations of *h* of *s* s.t. *h* is consistent with *d,* and some member of *G* is more general than *h* 
         3. Remove from *S* any hypothesis that is more general than another hypothesis in *S*
   2. If *d* is a -ve example
      1. Remove from *S* any hypothesis inconsistent with *d*
      2. For each \$ g \in G \$  not consistent with *d*
         1. Remove *g* from *G*
         2. Add to *G* all minimal specialisations *h* of *g* st *h* is consistent with *d*, and some member of *S* is more specific than *h*.
         3. Remove from *G* any hypothesis that is more specifc that another hypothesis in G

> Why does +ve eg minimally generalise S and -ve eg specialise G? Why can't it be the other way round? (ie +ve specialise G and -ve generalise S)

The addition of +ve example is more likely to make \$ s \in S \$ inconsistent (which rejects more than accepts), thus +ve examples are used to minimally generalise S. vice versa.

**Remarks on Candidate-Elimination**
1. **Does not handle error/noise in training data**
   1. Suppose *i* training example is wrongly labeled as -ve
   2. Hypothesis inconsistent with *i* removed (which includes target concept *c*)
   3. S and G reduced to \$ \emptyset \$ with sufficiently large data
2. **Insufficiently expressive hypothesis representation** (ie insufficient *x*) --> Biased hypothesis space st \$ c \notin H \$ --> S and G reduced to \$ \emptyset \$ with sufficiently large data
3. **Active learner should query input instance that satisfies exactly half of the hypotheses**
   1. Reason:
      1. Assume that the learner does not have the label for the input instance; it needs to query the instance to get the label
      2. Every instance can either be labelled as positive or negative
      3. Picking exactly half of the hypothesis will eliminate half of them regardless of the label of the instance
   2. At most \$ \lceil log_2{(VS_{H,D})} \rceil \$ examples to find target concept *c*
4. **Proposition 3.** An input instance *x* satisfies every hypothesis in \$ VS_{H,D} \$ iff *x* satisfies every member of *S*.
5. **Proposition 4.** An input instance *x* satisfies none of the hypothesis in \$ VS_{H,D} \$ iff *x* satisfies none of the members of *G*.
6. How to classify new unobserved input distance?
   2. Majority vote, assuming all hypotheses in *H* are equally probable *a priori*

## Inductive Bias
### An Unbiased Learner
**Intution.** Choose *H* that can express every teachable concept (i.e. *H'* is the power set of *X*)

> **Power set**, \$ P(S) \$ of any set *S* is the set of all subsets of *S*, including the empty set itself and S itself.

**Limitation (of using H').** Overfitting - cannot classify new unobserved input instances (aka *generalize* beyond observed training examples)

### Inductive Bias

Given 
- Concept learning algorithm *L*
- Input instances *X*, unkown target concept *c*
- Noise-free training examples \$ D_c = \\{ \langle x_k, c(x_k) \rangle \\}_{k=1, \ldots, n} \$

Let \$ L(x, D_c) \$ denote the classification of input instance *x* by *L* after learning from training examples \$ D_c \$

**Definition.** The **inductive bias** of *L* is any minimal *set* of assertions *B* s.t. for any target concept *c* and corresponding training examples \$ D_c \$,

**Inductive Inference Step performed by L**
<!-- TODO: add inductive step -->

**Drawback.** \$ L(x_i, D_c) \$ that L infers will not in general be provably correct. Thus, it's interesting to ask what additional assumptions could be added to \$ D_c \wedge x_i \$ st \$ L(x_i, D_c) \$ would follow deductively.

**Deductive Step performed by L**

**Definition.** The **inductive bias** of *L* is any minimal set of assertions *B* s.t. for any target concept c and corresponding training examples \$ D_c \$,

$$
\forall x \in X\left(B \wedge D_{c} \wedge x\right) \vDash\left(c(x)=L\left(x, D_{c}\right)\right)
$$

> B is added to justify its inductive inferences as deductive inferences

> \$ y \vDash z \$: z can be deductively proven from y

**Inductive Bias of Candidate Elimination**
$$
B = \{ c \in H \}
$$

<!-- Q: Step 2 of proof; why is it true for every h? -->

### Comparing Learners with Different Inductive Biases
1. **Rote Learner.** Store examples & classify input instance *x* iff it matches that of previously observed example. No inductive bias
2. **Candidate-Elimination.** Inductive bias: \$ c \in H \$
3. **Find-S.** Inductive bias: \$ c \in H \$ and all instances are -ve unless the opposite is entailed by its other knowledge

<!-- Q: Why all instances are -ve? -->

# Decision Trees
**Why Study Decision Tree (DT) Learning?**
![Why decision trees](/assets/img/2020-16-1-CS3244/why-decision-trees.png)

## Expressive Power
Decision Trees can express *any* function of the input attributes
- Trivially, there is a consistent decision tree for any training set with one path to leaf for each example
- Drawback: Overfitting
- Solution: Find compact decision trees

<!-- Q: Don't understand slide 8 on expressive power -->

## Hypothesis/Search Space

Number of distinct binary decision trees with *m* Boolean attributes <br />
= number of boolean-values functions <br />
= number of distinct truth tables with \$ 2^m \$ rows
= \$ 2^2^m \$ 

<!-- Q: Don't understand computation -->

## Decision-Tree-Learning
**Aim.** Find a small tree consistent with the training examples <br />
**Idea.** Greedily choose "most important" attribute as root of (sub)tree

![Decision Tree Learning Algo](/assets/img/2020-16-1-CS3244/decision-tree-algo.png)

### Which Attribute is the Best Classifier?

**Intuition.** A good attribute splits the examples into subsets that are (ideally) "all +ve" or "all -ve" (ie. classified exactly)

**Definition.** Information gain measures how well a given attribute separates the training examples according to their target classification

### Entropy Measures Homogeneity of Examples

**Intuition.** Entropy, *H*, measures the **uncertainty of classification**, \$ C \in \\{ c_1, \ldots , c_k \\} \$

$$
H(C) = -\sum^k_{i=1}P(c_i)log_2P(c_i)
$$

Thus, entropy for a boolean r.v (ie C = 2)

$$
H(C)=B\left(\frac{p}{p+n}\right)=-\frac{p}{p+n} \log _{2} \frac{p}{p+n}-\frac{n}{p+n} \log _{2} \frac{n}{p+n} = -(qlog_2q + (1-q)log_2(1-q)), where
$$

$$
q = \frac{p}{p+n} = \text{probability that boolean r.v. C is true}
$$

1. If all members of the collection belong to the same class, entropy = 0 (no uncertainty)
2. If the collection contains an equal no. of +ve and -ve examples, entropy = 1. (maximum uncertainty)
3. If the collection contains unequal no. of +ve and -ve eg, then the entropy \$ \in (0, 1) \$ (some uncertainty)

![Entropy Curve](/assets/img/2020-16-1-CS3244/entropy-curve.png)

### Information Gain
![Information Gain Slide](/assets/img/2020-16-1-CS3244/information-gain.png)

### Inductive Bias of Decision-Tree-Learning
**Motivation.** 
1. Given a collection of training examples, there are typically many decision trees consistent with these examples. 
2. Describing the inductive bias therefore consists of describing the basis by which it chooses one of these consistent hypotheses over the others.

**Approximate Inductive Bias of Decision-Tree-Learning:**
1. **Shorter** trees are preferred
2. Trees that place **high information gain attributes close to the root** are preferred
  
> If only (1) is considered, it is exactly the approximate inductive bias for **BFS** for the shortest consistent DT, which can be preohibitively expensive

## Occam's Razor
Prefer **shortest/simplest** hypothesis that **fits the data.**

Arguments in favor:
- Fewer short hypotheses than long hypotheses
	- Short/simple hypothesis that fits data unlikely to be coincidence
	- Long/complex hypothesis that fits data may be coincidence

Arguments opposed:
- Many ways to define small sets of hypothesis
- Small sets of short/simple hypothesis can be obtained using different hypothesis representations

<!-- Q: Don't understand 2nd opposing arg -->

### Overfitting
**Definition.** Hypothesis \$ h \in H \$ **overfits** the set *D* of training examples iff

$$
\exists h^{\prime} \in H \backslash \{h\}\left(e r r o r_{D}(h)<\text { error}_{D}\left(h^{\prime}\right)\right) \wedge\left(\text {error}_{D_X}(h)>\text {error }_{D_X}\left(h^{\prime}\right)\right)
$$

where \$ error_D(h) \$ and \$ error_{D_X}(h) \$ denotes errors of *h* over *D*  and set \$ D_x \$ of examples corresponding to instance space *X* respectively.

**How to avoid overfitting?**
1. Stop growing DT when expanding a node is not statistically significant
2. Allow DT to grow and overfit the data, then post-prune it

**How to select "best" DT?**
1. Measure performance over training examples/data
2. Measure performance over a separate validation dataset
3. MDL: minimize \$ size(tree) \$ & \$ size(misclassifications(tree)) \$

#### Reduced-Error Pruning

1. Partition data into *training* and *validation* sets
2. Do until further pruning is harmful
    1. Evaluate impact on validation set of pruning each possible node
    2. Greedily remove the one that most improves validation set accuracy

Doing so produces smallest version of most accurate subtree. <br />
**Drawback.** What if data is limited?

#### Rule Post-Pruning
1. Convert learned DT to an equivalent set of rules by creating one rule for each path from the root to a leaf
2. Prune (generalise) each rule by removing any precondition that improves its estimated accuracy
3. Sort pruned rules by estimated accuracy into desired sequence for use when classifying unobserved input instances

<!-- Q: Don't understand -->
<!-- Consider adding in why rule post-pruning -->
### Continuous-Valued Attributes
Define a **discrete**-valued input attribute to partition the values of a **continuous** input attribute into a discrete set of intervals for testing.

### Attributes with Many Values
**Problem.** *Gain* will select attribute with many values (e.g. Date). (Why?)

**Solution.** Use *GainRatio* instead:

$$
GainRatio(C,A) = \frac{Gain(C, A)}{SplitInformation(C, A)} \\
SplitInformation(C, A) = -\sum_{i=1}^{d} \frac{\left|E_{i}\right|}{|E|} \log _{2} \frac{\left|E_{i}\right|}{|E|}
$$

<!-- Q: Don't understand -->

### Attributes with Differing Costs
**Problem.** How to learn consistent DT with low expected cost?

**Solution.** Replace *Gain* by

$$
\frac{Gain^2(C,A)}{Cost(A)} \\
\frac{2^{Gain(C,A)} - 1}{(Cost(A) + 1)^w}
$$

### Missing Attribute Values
TODO