---
layout: post
title: "NUS, CS3244, Tutorial 1: Concept Learning"
categories: archive
hidden: true
---
(Larry Law, A0189883A, T05)

### BL1

> Prove Proposition 1: h is consistent with D iff every +ve training instance satisfies h and every -ve training instance does not satisfy h.

Show \$ LHS \rightarrow RHS \$
1. Since \$ c: X \rightarrow \\{0, 1 \\} \$, \$ D = \\{ \langle x, c(x) \rangle: c(x) = \text{0 or 1} \\} \$ (page 14 of the "Concept Learning" lecture slides)
2. \$ \forall \langle x, c(x) \rangle \in D, h(x) = c(x) \$ (by definition of consistent).
3. From 1. and 2.,
   1. \$ \forall \langle x, c(x) \rangle: c(x) = 1, h(x) = 1 \$
   2. \$ \forall \langle x, c(x) \rangle: c(x) = 0, h(x) = 0. (\bullet) \$ 

> \$ h(x) = 1 \$ iff an input instance \$ x \in X \$ satisfies a hypothesis \$ h \in H \$ (page 7 of the "Concept Learning" lecture slides)

> \$ c(x) = 1 \$ implies a positive training example (page 8 of the "Concept Learning" lecture slides)

Show \$ RHS \rightarrow LHS \$
1. From RHS,
   1. \$ \forall \langle x, c(x) \rangle: c(x) = 1, h(x) = 1 \$
   2. \$ \forall \langle x, c(x) \rangle: c(x) = 0, h(x) = 0. \$  
2. Since \$ c: X \rightarrow \\{0, 1 \\} \$, \$ D = \\{ \langle x, c(x) \rangle: c(x) = \text{0 or 1} \\} \$
> *D* is the the union of *{x: c(x) = 0} and {x: c(x) = 1}* 
1. \$ \forall \langle x, c(x) \rangle \in D, h(x) = c(x) = \text{0 or 1} \implies \forall \langle x, c(x) \rangle \in D, h(x) = c(x). (\bullet) \$ (by definition of consistent)

### TM 2.1
**(a)**
1. \$ \vert X \vert = 3^2 \cdot 2^5 = 288 \$
2. \$ \vert H \vert = 4^2 \cdot 3^5 + 1 = 3889 \$

**(b)**
1. \$ \vert X' \vert = \vert X \vert \cdot k \$
2. \$ \vert H' \vert =  \vert H \vert \cdot k + \vert H \vert - k \$

### T2.2
**b**
> Why will the final version space be the same?

Because in the end the version space contains **all** hypotheses consistent with the set of examples.

> Heuristics for ordering the training examples to minimize sizes of the intermediate S and G sets

Observe that size of sets S increases and G decreases with the algorithm. Thus possible heuristics include
1. Order the negative training examples in front (since they decrease set G)
2. Active learner should query input instance that satisfies exactly half of the hypotheses

### TM2.4
**(a)**
$$
S = \{ <4 ≤ x ≤ 6, 3 ≤ y ≤ 5> \}
$$

**(b)**
$$
G = \{ <2 ≤ x ≤ 8, 2 ≤ y ≤ 5>, <3 ≤ x ≤ 8, 2 ≤ y ≤ 7> \}
$$

**(c)**
1. (3,3) does not. More generally, any point that is within the boundaries of S and G.
2. (5, 4). More generally, any point that is within S or outside of G.

**(d)**
1. 6