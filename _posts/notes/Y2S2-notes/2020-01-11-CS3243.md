---
layout: post
title: "NUS, CS3243: Introduction to AI"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: Zick Yair <br>

<!-- omit in toc -->
# Table of Contents
- [Math Primers](#math-primers)
  - [NP and inherently hard problems](#np-and-inherently-hard-problems)
- [Introduction](#introduction)
  - [When Can a Machine Truly Think?](#when-can-a-machine-truly-think)
  - [Rational Agents](#rational-agents)
  - [Specifying Task Environment: PEAS](#specifying-task-environment-peas)
  - [Agenty Types](#agenty-types)
    - [Simple Reflex Agent](#simple-reflex-agent)
    - [Model-based Reflex Agent](#model-based-reflex-agent)
    - [Goal-based agent](#goal-based-agent)
    - [Utility-based agent](#utility-based-agent)
    - [Learning Agent](#learning-agent)
    - [Tradeoff between Exploitation and Exploration](#tradeoff-between-exploitation-and-exploration)
- [Searching for Solutions](#searching-for-solutions)
  - [Notations](#notations)
  - [Tree-Search vs Graph-Search](#tree-search-vs-graph-search)
- [Uninformed Search](#uninformed-search)
  - [Problem-Solving Agents](#problem-solving-agents)
  - [Uninformed Search Strategies](#uninformed-search-strategies)
    - [Breadth-First-Search (BFS)](#breadth-first-search-bfs)
    - [Uniform-Cost Search (UCS)](#uniform-cost-search-ucs)
    - [Depth-First Search](#depth-first-search)
    - [Depth-Limited Search (DLS)](#depth-limited-search-dls)
    - [Iterative Deepending Search (IDS)](#iterative-deepending-search-ids)
    - [Summary](#summary)
- [Informed Search](#informed-search)
  - [Best-First-Search](#best-first-search)
  - [Greedy best-first search](#greedy-best-first-search)
    - [Conditions for optimality: Admissibility and consistency](#conditions-for-optimality-admissibility-and-consistency)
  - [A* Search](#a-search)
  - [Dominance](#dominance)
  - [Generating admissible heuristics from relaxed problems](#generating-admissible-heuristics-from-relaxed-problems)
- [Local Search](#local-search)
  - [Local Search Algorithms](#local-search-algorithms)

# Math Primers
## NP and inherently hard problems
1. Complexity analysis analyzes problems rather than algos
2. **P:** Class of polynomial problems that can be solved in time \$ O(n^k) \$ for some k
3. **NP:** Class of nondeterministic polynomial problems
4. **NP-complete:** Subclass of NP. "Complete" is used in the sense of "most extreme", and thus refers to the hardest problems in the class NP. It has been proven that either the NP-complete problems are in P or none of them is.
5. **co-NP:** Complement of NP (ie for every decision problem in NP, there is a corresponding problem in co-NP with the "yes" and "no" answers reversed)
6. **co-NP-complete:** Hardest problems in co-NP

> Is P = NP? That is, do NP problems have polynomial-time algorithms? This has never been proven.



# Introduction
## When Can a Machine Truly Think?

**Turing Test:**  A computer passes the test if a human interrogator, after
posing some *written* questions, cannot tell whether the written responses come from a person or from a computer. 

**Winograd Schema:** 
1. You are given *m* Winograd schema, with the context word chosen uniformly at random.
2. Design an AI that can correctly resolve a significant number of them.

Above examples show that single test for Intelligence is...
1. Difficult to resolve
2. Tests tend to be 1) over-specified or 2) very subjective (tradeoff!)
3. Results will be debatable

## Rational Agents

**Agent:** Function maps *percept histories* to *actions*

**Rational agent:**
1. For each possible percept sequence, select an action that is expected to *maximize its performance measure*
2. given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.

## Specifying Task Environment: PEAS

Task environment is used for intelligent agent design. PEAS stands for
1. Performance measure: objective criterion for measuring success of an agent's behaviour.
2. Environment
3. Actuators
4. Sensors

Properties of Task Environments include
1. **Fully observable (vs. partially observable):** sensors provide access to the complete state of the environment at each point in time.
2. **Deterministic (vs. stochastic):** The next state of the environment is completely determined by the current state and the action executed by the agent
3. **Episodic (or memorylessness) (vs. sequential):** The choice of current action does not depend on actions in past episodes
4. **Static (vs. dynamic):** The environment is unchanged while an agent is
deliberating.
5. **Discrete (vs. continuous):** A finite no. of distinct states, percepts, and actions. (turn based)
6. **Single agent (vs. multi-agent):** An agent operating by itself in an env.

## Agenty Types
### Simple Reflex Agent
1. Passive: only acts when it observes a percept
2. Updates *state* based on *percept* only.
3. Easy to implement.

### Model-based Reflex Agent
1. Passive: only acts when it observes a percept
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Goal-based agent
1. Has **goals**, acts to achieve them (not passive)
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Utility-based agent
1. Has **utility function**, acts to achieve them (not passive)
   1. **Motivation:** Goals alone are not enough to generate high quality behaviour (ie many action sequences can get taxi to its goal destination, but some are quicker, safer, more reliable, or cheaper)
   2. **Utility function allows for a more general performance measure**
> Utility Function vs Performance Measure: Function internally used by the agent to evaluate performance vs Function to evaluate agent's external behaviour
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Learning Agent

> Distinction between learning element, which is responsible for making improvements, and the performance element, which is responsible for selecting external actions. 

The performance element is what we have previously considered
to be the entire agent: it takes in percepts and decides on actions. The learning element uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future.

Learning element is responsible for making improvement

### Tradeoff between Exploitation and Exploration
Tradeoff between
1. **Exploitation**: maximizing its expected utility according to its current knowledge of the world
2. **Exploration**: trying to learn more about the world since this may improve its future gains

# Searching for Solutions

## Notations
1. **State:** the state in the state space to which the node coressponds
2. **Frontier:** nodes that we have seen but have yet to explore (at initialisation, the frontier is just the source)
3. **Node:** data structure constituting part of sesarch tree. It includes state, parent node, action, and path cost *g(n)*.

> Nodes vs States: Node is a bookkeeping data structure used to represent the search tree. A state corresponds to a configuration of the world. Thus nodes are on particular paths whereas states are not. Furthermore, two different nodes are allowed to contain the same world state (if that state is generated via two different search paths)

## Tree-Search vs Graph-Search
> Algorithms that forget their history are doomed to repeat it.

Recall that any two vertices in a tree is connected by a unique simple path. Thus Tree-Search algorithm is applied to Trees, wherein there will be no repeated states. In contrast, graph-search algorithm is applied to graphs, thus it handles repeated states (in bold)

![Tree vs Graph Search](/assets/img/2020-16-1-CS3243/tree-vs-graph-search.png)

# Uninformed Search
Uninformed search strategies use only information available in the problem definition

## Problem-Solving Agents
**Environment:** Fully observable, deterministic, discrete

**Problem Formulation** <br />
A problem can be defined formally by 5 components
1. **Initial State** that the agent starts in.
2. **Actions:** A description of the possible actions available to the agent (`Actions(s)`)
3. **Transition Model:** A description of what each action does (`Result(s,a)`)
> Together, the initial state, actions, and transition model define the **state space** of the problem: the set of all states reachable from the initial state by any sequence of actions.
4. **Goal Test:**
   1. Is the state *s* equal the goal state?
   2. Explicit set of goal states
   3. Implicit function (eg `isCheckmate(s)`)
5. **Path cost**
   1. *c(s, a, s'):* the step cost of taking action a in state s to reach state s'.
   2. Additive: Sum of the step costs

> There can be more than 1 goal nodes

## Uninformed Search Strategies
**Assumption:** Search in a *tree* data structure

**Evaluation Criteria:**
1. **Completeness:** always find a solution if exists
2. **optimality:** find a least-cost soln
3. **time complexity:** no. of nodes generated
4. **space complexity:** max no. of nodes in memory

**Problem Parameters**
1. *b*: maximum # of successors of any node
2. *d*: depth of shallowest goal node
3. *m*: max depth of search tree


### Breadth-First-Search (BFS)
**Idea:** Expand shallowest unexpanded node <br />
**Implementation:** Frontier is a FIFO queue

Properties of BFS
1. **Complete?:** Yes (if *b* is finite)
2. **Optimal:** No (unless step costs are equal, ie unweighted). Counterexample: 2 goal nodes, \$ v_1 \$ \$ v_2 \$, with cost of 100 and 1. BFS will return \$ v_1 \$, which is suboptimal.
3. **Time:** \$ O(b) + O(b^2) + ... + O(b^d) = O(b^d) \$
4. **Space:** Max size of frontier \$ O(b^d) \$

> Memory requirements are a bigger problem: one might wait 13 days for the solution to an important problem with search depth *d* 12, but no personal computer has the petabyte of memory it would take.

### Uniform-Cost Search (UCS)
**Idea:** Expand least-path-cost, unexpanded node <br />
**Frontier:** Priority Queue ordered by path cost *g* <br />
Equivalent to BFS if all step costs are equal

> <s', c>, where c refers to the additive sum of step costs from source node to s'

> Dijkstra's algorithm, which is perhaps better-known, can be regarded as a variant of uniform-cost search, where there is no goal state and processing continues until all nodes have been removed from the priority queue, i.e. until shortest paths to all nodes (not just a goal node) have been determined

> When a node is explored, the path there is guaranteed to be the cheapest.

1. **Complete?:** Yes (if all step costs are ≥ \$ \epsilon \$)
2. **Optimal:** Yes (shortest path nodes expanded first)
3. **Time:** \$ O(b^{1 + \lfloor \frac{C^*}{\epsilon} \rfloor}) \$, where _C_ is the optimal cost
4. **Space:** \$ O(b^{1 + \lfloor \frac{C^*}{\epsilon} \rfloor}) \$

> Derivation of time complexity

Let e denote \$ \epsilon \$

|Steps  |Worst Case Time Complexity  |Distance  |
|---|---|---|
|1  |b  |≥e  |
|2  |b^2  |≥2e  |
|...  |  |  |
|floor(C*/e)  |b^floor(C*/e)  |≥floor(C*/e)e  |

Note that the lower bound for step floor(C\*/e) is floor(C\*/e)e, which is lesser than C\* in the worse case time scenario. Thus, in order to achieve a distance of at least C\*, the max no. of steps will be floor(C\*/e) + 1.

> At step k, keep ≤ \$ b^k \$ nodes in frontier.

Worse case scenario (upper bound):
1. Step 1: source node expands *b* successors
2. Step 2: For each of these *b* successors, they'll have *b* successors. Total of \$ b^2 \$ successors
3. ...
4. Step k: \$ b^k \$ successors

### Depth-First Search
**Idea:** Expand deepest unexpanded node <br />
**Implementation:** Frontier is a LIFO queue

1. **Complete?:** No on infinite depth graphs
2. **Optimal:** No (unless step costs are equal, ie unweighted)
3. **Time:** \$ O(b^m) \$ (ie all of the nodes in the search tree)
4. **Space:** Max size of frontier \$ O(bm) \$ (can be \$ O(m) \$)

> What is the advantage of DFS > BFS?

DFS has a better space complexity for tree search, not graph search. (for graph search, worse case scenario would be the entire graph)

> Why is DFS space complexity \$ O(bm) \$
Depth first tree search needs to store only a single path from the root to a leaf node (*m*), along with the remaining unexpanded sibling nodes for each node on the path (*b*).

However, the runtime can be further improved from *O(bm)* to *O(m)* by using **backtracking search.**

In bactracking, only one successor is generated than all successors; each partially expanded node remembers which successor to generate next (eliminate *b*). The idea of generating a successor by *modifying the current state description* instead of directly copying it.

### Depth-Limited Search (DLS)
**Idea:** run DFS with depth limit *l* <br />

### Iterative Deepending Search (IDS)
**Idea:** perform DLSs with increasing depth limit until goal node is found <br />; better if state space is large and depth of soln is unknown

1. **Complete?:** Yes (if *b* is finite)
2. **Optimal:** No (unless step costs is 1)
3. **Time:** \$ O(b^d) \$ 
4. **Space:** \$ O(bd) \$ (can be \$ O(d) \$)

> Though IDS may seem wasteful because states are generated multiple times, asymptotically the time cplexity is the same as BFS. Intuitively, this is because most of the nodes are in the bottom level, so it does not matter much that the upper levels are generated multiple times.

### Summary
![uninformed search summary](/assets/img/2020-16-1-CS3243/uninformed-search-summary.png)

# Informed Search
Exploit problem-specific knowledge (beyond the problem definition) to obtain heuristics to guide search

## Best-First-Search
1. **Idea:** Use an evaluation function *f(n)* for each node *n*
   1. Cost Estimate: Expand node with lowest evaluation/cost first
2. **Implementation:** Identical to uniform-cost search, except for the use of *f* instead of *g* to order the priority queue. Most best-first algo include as a component of *f* a heuristic function denote *h(n):*

$$
h(n) = \text{estimated cost of the cheapest path from the state at node n to a goal state}
$$

> Notice that *h(n)* takes a node as inpute, but unlike *g(n)* it depends only on the *state* at the node.

3. **Special Cases** (different choices of *f*):
   1. Greedy best-first search
   2. A* search

## Greedy best-first search

**Intuition:** Greedy best-first search expands the node that appears to be closest to goal. Thus, it evaluates nodes by just using the heuristic function (ie *f(n) = h(n)*)

1. **Complete?:** Yes (if *b* is finite)
2. **Optimal:** No (shortest path to Bucharest: 418km)
3. **Time:** \$ O(b^m) \$ (ie entire tree), but a good heuristic can reduce complexity substantially
4. **Space:** Max size of frontier \$ O(b^m) \$ 

### Conditions for optimality: Admissibility and consistency

**h(n) to be an admissible heuristic:** heuristic that never overestimates the cost to reach the goal. (ie *f(n) = g(n) + h(n)* never overestimates the true cost of a soln along the current path through *n*).

> E.g: Straight-line distance, which is by definition the shortest path, thus it cannot be an overestimate.

**h(n) is consistent:** if, for every node *n* and every successor *n'* of *n* generated by action *a*, the estimated cost of reaching the goal from *n* is no greater than the step cost of getting to *n'* plus the estimated cost of reaching the goal from *n':*

$$
h(n) ≤ c(n, a, n') + h(n')
$$

> Every consistent heuristic is also admissible.

## A* Search
**Intuition:** Improves on uniform cost search (*g(n)*), by avoiding expanding paths that are already expensive (which is given by *h(n)*)

**Evaluation function:** *f(n) = g(n) + h(n)*, where
1. *g(n)* = cost of reaching *n* from start node
2. *h(n)* = cost estimate from *n* to goal
3. *f(n)* = estimated cost of cheapest path **through** n to goal

**Theorem:** If *h(n)* is admissible, then A* using *Tree-Search* is optimal <br />
**Theorem:** If *h(n)* is consistent, then A* using *graph-search* is optimal (proof in page 95 of textbook)
> Lemma 1: if *h(n)* is consistent, then the values of *f(n)* along any path are nondecreasing

> Lemma 2: (Given that *h(n)* is consistent) Whenever A* selects a node *n* for expansion, the optimal path to that node has been found.

1. **Complete?:** Yes (if there is a finite no. of nodes with \$ f(n) ≤ C^*, C^* \text{ is the cost of the optimal solution path} \$)
> 1) \$ A^* \$ expands all nodes with \$ f(n) < C^* \$ 2) \$ A^* \$ expands some nodes right on the 'goal contour' before selecting a goal node
1. **Optimal:** Yes
2. **Time:** \$ O(b^{h^{\ast}(s_0) - h(s_0)}) \$ where \$ h^{\ast}(s_0) \$ is the actual cost of getting from root to goal
3. **Space:** Max size of frontier \$ O(b^m) \$ 

## Dominance
If \$ h_2(n) ≥ h_1(n) \$ for all n (both admissible), then \$ h_2 \$ dominates \$ h_1 \$. It follows that \$ h_2 \$ incurs lower search cost than \$ h_1 \$.

> Proof in page 104 of textbook

## Generating admissible heuristics from relaxed problems
1. A problem with fewer restrictions on the actions is called a **relaxed problem**
2. The cost of an optimal solution to a relaxed problem is also an admissible heuristic for the original problem

# Local Search
1. The **path** to a goal is irrelevant (unlike (un)informed search); the goal staste itself is the solution.
2. State sapce = set of "complete" configurations
3. Find final configuration satisfying constraints

## Local Search Algorithms
Maintain single "current best" state and try to improve it

Advantages
1. Very little/constant memory
2. Find reasonable solutions in large state space

<!-- Page 104-->
