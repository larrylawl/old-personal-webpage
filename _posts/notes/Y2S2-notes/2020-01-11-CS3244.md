---
layout: post
title: "NUS, CS3244: Machine Learning"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: Low Kian Hsiang <br>

<!-- omit in toc -->
# Table of Contents
- [Introduction](#introduction)
  - [What is Learning?](#what-is-learning)
- [Concept Learning and the General-To-Specific Ordering](#concept-learning-and-the-general-to-specific-ordering)
  - [Introduction](#introduction-1)
  - [How to represent a Hypothesis?](#how-to-represent-a-hypothesis)
  - [A Concept Learning Task](#a-concept-learning-task)
    - [Notations](#notations)
    - [The Inductive Learning Hypothesis](#the-inductive-learning-hypothesis)
  - [Concept Learning as Search](#concept-learning-as-search)
    - [General-To-Specific Ordering of Hypotheses](#general-to-specific-ordering-of-hypotheses)
  - [Find-S: Finding a maximally specific hypothesis](#find-s-finding-a-maximally-specific-hypothesis)
  - [Version Spaces and the Candidate-Elimination Algorithm](#version-spaces-and-the-candidate-elimination-algorithm)
    - [The List-Then-Eliminate Algorithm](#the-list-then-eliminate-algorithm)
    - [A More compact Representation for Version Spaces](#a-more-compact-representation-for-version-spaces)
    - [Candidate-Elimination Learning Algorithm](#candidate-elimination-learning-algorithm)
  - [Inductive Bias](#inductive-bias)
    - [An Unbiased Learner](#an-unbiased-learner)

# Introduction
## What is Learning?
An agent is said to be *learning* if it improves its *performance P* on *task T* based on *experience/observations/data E*.

> T must be fixed, P must be measurable, E must in.

# Concept Learning and the General-To-Specific Ordering
1. Learning from Examples
2. General-to-specific ordering over hypothesis
3. Version spaces and candidate elimination algorithm
4. Picking new examples
5. The need for inductive bias

## Introduction
**Concept Learning:** Inferring a boolean-valued fn from training examples of its input and output

> Concept learning is also a form of supervised learning

## How to represent a Hypothesis?

Quadratic is more expressive than linear, but it is the expense of a larger hypothesis space

## A Concept Learning Task
### Notations
![Concept Learning Task](/assets/img/2020-16-1-CS3244/concept-learning-task.png)

> Hypothesis *h* has a tradeoff between **expressive power vs. smaller hypothesis space.** For eg, compare a linear function *f(x) = mx + c* w a quadratic function *f(x) = ax^2 + bx + c*. The latter is more expressive than the former (it can still rep linear functions), but it has a larger hypothesis space (parameters can take on {A x B x C} set of values, as opposed to {M X C})

**Definition.** An input instance \$ x \in X \$ satisfies (all constraints of) a hypothesis \$ h \in H \$ iff *h(x) = 1*. In other words, *h* classifies *x* as a +ve example.

**Objective:** Determine a hypothesis \$ h \in H \$ that is **consistent** with *D*

**Definition.** A hypothesis *h* is **consistent** with a set of training examples *D* iff \$ \forall \langle x, c(x) \rangle \in D, h(x) = c(x) \$.

### The Inductive Learning Hypothesis
**Definition.** Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved
examples 

## Concept Learning as Search
**Goal.** Search for a hypothesis \$ h \in H \$ that is **consistent** with *D*

Since hypothesis space *H* is much larger and possibly infinite, we need to *exploit structure* to search efficiently.

>Synthetically vs Semantically distinct
1. **Synthetically Distinct:** Include 1) *?*: don't care and 2) \$ \emptyset \$. "Looks different." *(3x2 -> 5x4)*
2. **Sementically Distinct:** 
   1. Include *?* 
   2. Every hypothesis containing 1 or more \$ \emptyset \$ is equivalent to an empty set of input instances, hence classifying every instance as a negative example. *(3x2 -> 4x3 + 1)*

### General-To-Specific Ordering of Hypotheses
**Definition.** \$ h_j \$ is more general than or equal to \$ h_k \$ (denoted by \$ h_j ≥_g h_k \$) iff any input instance x that satisfies \$ h_k \$ also satisfies \$ h_j \$

$$
\forall x \in X (h_{k}(x) = 1) \rightarrow (h_{j}(x) = 1)
$$

> \$ ≥_g \$ defines a [partial order](http://mathworld.wolfram.com/PartialOrder.html)(reflexive, antisymmetric, transitive) over *H* and not [total order](http://mathworld.wolfram.com/TotallyOrderedSet.html) (partial order and comparability condition).

**Definition.** \$ h_j \$ is more general than \$ h_k \$ (denoted by \$ h_j >_g h_k \$) iff \$ h_j ≥_g h_k \$ and \$ h_k ≱_g h_j \$

**Definition.** \$ h_j \$ is more specific than \$ h_k \$ iff \$ h_k \$ is more general than \$ h_j \$.

## Find-S: Finding a maximally specific hypothesis

**Intuition.** Start with most specific hypothesis. Whenever it wrongly classifies a +ve training example as −ve, “minimally” generalize it to satisfy its input instance.

1. Initialize *h* to most specific hypothesis in *H*
2. For each +ve training instance *x*
   1. For each attribute constraint \$ a_i \$ in *h*
      1. If *x* satisfies constraint \$ a_i \$ in *h*, then do nothing.
      2. Else, replace \$ a_i \$ in *h* by the next more general constraint that is satisfied by x
3. Output hypothesis *h*

**Proposition 1.** *h* is consistent with *D* iff every +ve training instance satisfies *h* and every -ve training instance does not satisfy *h*

> If there are no -ve instance, then the latter will be vacuously true.

**Proposition 2.** Suppose that \$ c \in H \$. Then, \$ h_n \$ is consistent with 
$$
D=\left\{\left\langle x_{k}, c\left(x_{k}\right)\right\rangle\right\}_{k=1, \ldots, n}
$$

> \$ h_n \$ is consistent with every preceding instances, inclduing negative training instance which was skipped.

Limitations
1. Can't tell whether Find-S has learned target concept
2. Can't tell when training examples are inconsistent (ie contains errors or noise)
3. Picks a maximally spsecific *h*
4. Depending on *H*, there might be several

## Version Spaces and the Candidate-Elimination Algorithm

**Definition.** The **version space** \$ VS_{H,D} \$ wrt hypothesis space *H* and training examples *D*, is the subset of hypothesis from *H* consistent with *D*:

$$
VS_{H,D} = \{ h \in H \mid \text{h is consistent with D} \}
$$

1. If \$ c \ in H \$, then a large enough *D* can reduce \$ VS_{H,D} \$ to *{c}*.
2. If *D* is insufficient, then \$ VS_{H,D} \$ represents the **uncertainty** of what the target concept is 
   1. If *D* is insufficient, then \$ VS_{H,D} \$ will be too general, thus it represents the uncertainty.
3. \$ VS_{H,D} \$ contains all consistent hypothesis, including the maximally specific hypotheses

### The List-Then-Eliminate Algorithm

**Intuition.** List all hypotheses in *H*. Then, eliminate any hypothesis found inconsistent with any training example.

1. *VS*: a list containing every hypothesis in *H*
2. For each training example \$ \langle x, c(x) \rangle \$
   1. Remove from VS any hypothesis *h* for which *h(x) ≠ c(x)*
3. Output the list of hypothesis in VS

**Limitation.** Prohibitively expensive to exhaustively enumerate all hypothesis in finite *H*

### A More compact Representation for Version Spaces
![Version Space Compact Representation](/assets/img/2020-16-1-CS3244/version-space-compact-rep.png)

**Definition.** The **general boundary** *G* of \$ VS_{H,D} \$ is the set of maximally general members of *H* consistent with *D:*

$$
G=\left\{g \in H \mid g \text { consistent with } D \wedge\left(\neg \exists g^{\prime} \in H g^{\prime}>_{g} g \wedge g^{\prime} \text { consistent with } D\right)\right\}
$$

**Definition.** The **specific boundary** *S* of \$ VS_{H,D} \$ is the set of maximally specific members of *H* consistent with *D:*

$$
S=\left\{s \in H \mid s \text { consistent with } D \wedge\left(\neg \exists s^{\prime} \in H s>_{g} s^{\prime} \wedge s^{\prime} \text { consistent with } D\right)\right\}
$$

Every member of version space lies between these boundaries. **Version Space Representation Theorem:** (VSRT)

$$
V S_{H, D}=\left\{h \in H \mid \exists s \in S \exists g \in G g \geq_{g} h \geq_{g} s\right\}
$$

### Candidate-Elimination Learning Algorithm
**Intutiion.** Start with most general and specific hypotheses. Each training example "minimally" generalizes *S* and specialises *G* to remove inconsistent hypothesis from version space.

1. For each training example *d*
   1. if *d* is a +ve example
      1. Remove from *G* any hypothesis inconsistent with *d*
      2. for each \$ s \in S \$ not consistent with *d*
         1. Remove *s* from S
         2. Add to *S* all minimal generalisations of *h* of *s* s.t. *h* is consistent with *d,* and some member of *G* is more general than *h* 
         3. Remove from *S* any hypothesis that is more general than another hypothesis in *S*
   2. If *d* is a -ve example
      1. Remove from *S* any hypothesis inconsistent with *d*
      2. For each \$ g \in G \$  not consistent with *d*
         1. Remove *g* from *G*
         2. Add to *G* all minimal specialisations *h* of *g* st *h* is consistent with *d*, and some member of *S* is more specific than *h*.
         3. Remove from *G* any hypothesis that is more specifc that another hypothesis in G

> Why does +ve eg minimally generalise S and -ve eg specialise G? Why can't it be the other way round? (ie +ve specialise G and -ve generalise S)

The addition of +ve example is more likely to make \$ s \in S \$ inconsistent (which rejects more than accepts), thus +ve examples are used to minimally generalise S. vice versa.

**Remarks on Candidate-Elimination**
1. **Does not handle error/noise in training data**
   1. Suppose *i* training example is wrongly labeled as -ve
   2. Hypothesis inconsistent with *i* removed (which includes target concept *c*)
   3. S and G reduced to \$ \emptyset \$ with sufficiently large data
2. **Insufficiently expressive hypothesis representation** (ie insufficient *x*) --> Biased hypothesis space st \$ c \notin H \$ --> S and G reduced to \$ \emptyset \$ with sufficiently large data
3. **Active learner should query input instance that satisfies exactly half of the hypotheses**
   1. Reason:
      1. Assume that the learner does not have the label for the input instance; it needs to query the instance to get the label
      2. Every instance can either be labelled as positive or negative
      3. Picking exactly half of the hypothesis will eliminate half of them regardless of the label of the instance
   2. At most \$ \lceil log_2{(VS_{H,D})} \rceil \$ examples to find target concept *c*
4. **Proposition 3.** An input instance *x* satisfies every hypothesis in \$ VS_{H,D} \$ iff *x* satisfies every member of *S*.
5. **Proposition 4.** An input instance *x* satisfies none of the hypothesis in \$ VS_{H,D} \$ iff *x* satisfies none of the members of *G*.
6. How to classify new unobserved input distance?
   2. Majority vote, assuming all hypotheses in *H* are equally probable *a priori*

## Inductive Bias
### An Unbiased Learner
**Intution.** Choose *H* that can express every teachable concept (i.e. *H'* is the power set of *X*)

> **Power set**, \$ P(S) \$ of any set *S* is the set of all subsets of *S*, including the empty set itself and S itself.

**Limitation (of using H').** Overfitting - cannot classify new unobserved input instances.