---
layout: post
title: "Coursera, Stanford: Deep Learning Notes"
author: "Larry Law"
categories: notes
tags: [notes, Deep Learning]
image: neural-network.jpeg
hidden: true
---
Lecturer: Professor Andrew Ng <br>
Course available [here](https://www.coursera.org/specializations/deep-learning).


<!-- ## Table of Contents
Week 1:

1. Rectify: Taking a max of 0 which is why you get a function shape like this
2. Online advertising: lucrative app
3. Why sigmoid over relu? 
   1. Gradient for sigmoid is very small when x >> 0 --> Gradient descent is slow --> Takes longer to train theta
   2. relu gradient is 1 throughout -->