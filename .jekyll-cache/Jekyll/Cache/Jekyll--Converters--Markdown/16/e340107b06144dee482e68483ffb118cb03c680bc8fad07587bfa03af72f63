I"€<h2 id="learning-outcomes">Learning outcomes</h2>
<ol>
  <li>Purpose for Random Initialisation of Parameters in Neural Networks</li>
  <li>Calculus Justifying (1)</li>
</ol>

<h2 id="purpose-for-random-initialisation-of-parameters-in-neural-networks">Purpose for Random Initialisation of Parameters in Neural Networks</h2>
<p>In Backpropagation, parameters are randomly initialised in order to avoid the case wherein they are <em>symmetrical</em> (ie have the same values).</p>

<h3 id="what-is-wrong-with-symmetrical-initialisation">What is wrong with symmetrical initialisation?</h3>

<p>The claim is that <strong>symmetrical initialisation will lead to all units of any layer <em>l</em> having the same output from the activation function.</strong> Let us look at the calculus justifying this claim.</p>

<h2 id="caculus-justifying-random-initialisation">Caculus Justifying Random Initialisation</h2>
<p>Suppose that all our parameters are initialised to the same value.</p>

<p>Recall that our activation function is defined as such</p>

<script type="math/tex; mode=display">a^{(l)} := g(z^{(l)}) = \Theta^{(l - 1)}a^{(l - 1)}</script>

<p>For all units of layer <em>l</em>, $ \Theta^{(l - 1)} $ is the same (by assumption) and $ a^{(l - 1)} $ is the same as well (by the neural networks model). Thus $ a^{(l)} $ will be the same.</p>

<h3 id="but-will-the-output-of-the-activation-function-remain-the-same-after-backpropagation">But will the output of the activation function remain the same after Backpropagation?</h3>

<p>Let‚Äôs find out.</p>

<p>Recall that the Backpropagation learns a particular parameter using the partial derivative of the cost function with respect to that parameter. More concretely,</p>

<script type="math/tex; mode=display">\frac{\partial{J(\Theta)}}{\partial \Theta^{(l)}_{i, j}} =a^{(l)}_{j} \delta^{(l+1)}_{i}, \delta \text{ to be defined}</script>

<p>where $ \delta $, also known as the ‚Äúerror term‚Äù, is a recurrence relation such that</p>

<script type="math/tex; mode=display">\\ \delta^{(l+1)} := \frac{\partial{J(\Theta)}}{\partial z^{(l+1)}} = \delta^{(l+2)} \Theta^{(l+1)} g^{\prime}\left(z^{(l+1)}\right),
\\ \delta^{(L)} = a^{(L)} - y,</script>

<blockquote>
  <p>Proof can be found in my article <a href="2019-12-18-calculus-for-backpropagation.md">here</a>.</p>
</blockquote>

<p>Lets look at the partial derivative term. Since</p>

<p><img src="/assets/img/neural-network-model.png" alt="neural network model" /></p>

<!-- Disclaimer that this explains Andrew Ng's course -->
<p><em>(This article explains the calculus of the backpropagation algorithm in Andrew Ng‚Äôs machine learning course, which can be found <a href="https://www.coursera.org/learn/machine-learning">here</a>.)</em>
<img src="/assets/img/backpropagation-algo.png" alt="Andrew Ng's course: Backpropagation Algorithm" /></p>

<h2 id="prerequisite">Prerequisite</h2>
<p>Standard notations for neural networks as used in Andrew Ng‚Äôs course. With some discrepancies, definition of the notations can be found in this <a href="https://cs230.stanford.edu/files/Notation.pdf">link</a>.</p>

<h2 id="intuition-of-backpropagation">Intuition of Backpropagation</h2>
<p>Recall that the the purpose of Backpropagation is to <em>learn the parameters</em> for a <em>neural network.</em> Backpropagation learns the parameters of layer $ l $, from the error terms of the layer $ l + 1 $. Thus, the parameter learning is propagated from the back.</p>

<h2 id="proof-for-the-partial-derivative-term-in-backpropagation">Proof for the Partial Derivative Term in Backpropagation</h2>
<p>Suppose that the activation function chosen is the sigmoid function.
For simplicity, let us assume that the size of the training set is 1 (ie <em>m</em> = 1).</p>

<script type="math/tex; mode=display">a = g(z) := \sigma(z)</script>

<p>The claim:</p>

<script type="math/tex; mode=display">\frac{\partial{J(\Theta)}}{\partial \Theta^{(l)}_{i, j}} =a^{(l)}_{j} \delta^{(l+1)}_{i}</script>

<p>where $ \delta $, also known as the ‚Äúerror term‚Äù, is a recurrence relation such that</p>

<script type="math/tex; mode=display">\\ \delta^{(l+1)} := \frac{\partial{J(\Theta)}}{\partial z^{(l+1)}} = \delta^{(l+2)} \Theta^{(l+1)} g^{\prime}\left(z^{(l+1)}\right),
\\ \delta^{(L)} = a^{(L)} - y,</script>

<p>Proof can be found <a href="https://www.coursera.org/learn/machine-learning/discussions/weeks/5/threads/MVwN-LpLEeiBxhK7qjbMkg">here</a>. Alternatively, I have included pictures of the proof at the bottom of the page.</p>

<h2 id="calculus-of-the-gradient-vector-of-backpropagation">Calculus of the Gradient Vector of Backpropagation</h2>
<p>Since the neural network needs to be trained on <em>m</em> training samples, the partial derivative term needs to be averaged out over all these samples.</p>

<script type="math/tex; mode=display">{\frac{\partial J(\Theta)}{\partial \Theta^{(l)}_{i, j}}} 
= D^{(l)}_{i, j}
= \frac{1}{m} \sum_{k=1}^{m} \frac{\partial J_{k}}{\partial \Theta^{(l)}_{i, j}}</script>

<p>The summation term is precisely the update rule in the algorithm.</p>

<script type="math/tex; mode=display">\Delta_{i, j}^{(l)}:=\Delta_{i, j}^{(l)}+a_{j}^{(l)} \delta_{i}^{(l+1)}</script>

<p>Adding the regularisation terms give us</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \cdot & D_{i, j}^{(l)}:=\frac{1}{m}\left(\Delta_{i, j}^{(l)}+\lambda \Theta_{i, j}^{(l)}\right), \text { if } j \neq 0 \\ \cdot & D_{i, j}^{(l)}:=\frac{1}{m} \Delta_{i, j}^{(l)} \text { lf } j=0 \end{aligned} %]]></script>

<p>Finally, let‚Äôs compute the gradient vector, which will be passed as an argument to parameter learning methods.</p>

<blockquote>
  <p>Recall that gradient vector of a multivariable function packages the partial derivative of its parameters into a vector.</p>
</blockquote>

<p>Fitting the partial derivatives into the gradient vector, we get</p>

<script type="math/tex; mode=display">\nabla J=\left[\begin{array}{c}{\frac{\partial J}{\partial \Theta^{(1)}_{1, 1}}} \\ {\frac{\partial J}{\partial \Theta^{(1)}_{1, 2}}} \\ {\vdots} \\ {\frac{\partial J}{\partial \Theta^{(L-1)}_{s_{l+1}, s_{l}}}}\end{array}\right]</script>

<blockquote>
  <p>For simplicity, bias terms are ignored.</p>
</blockquote>

<p>And we are done :).</p>

<h2 id="fitting-the-calculus-with-the-intuition">Fitting the Calculus with the Intuition</h2>

<p>Learning of a particular $ \Theta^{(l)}_{i, j} $ is dependent on the partial derivative of the cost function, which in turn is dependent on the error term.</p>

<script type="math/tex; mode=display">\frac{\partial{J(\Theta)}}{\partial \Theta^{(l)}_{i, j}} =a^{(l)}_{j} \delta^{(l+1)}_{i}</script>

<p>This error term is a recurrence relation. In other words, the error term of layer <em>l+1</em> is dependent on the error term of layer <em>l+2</em>, which ends at layer <em>L</em>.</p>

<script type="math/tex; mode=display">\\ \delta^{(l+1)} := \frac{\partial{J(\Theta)}}{\partial z^{(l+1)}} = \delta^{(l+2)} \Theta^{(l+1)} g^{\prime}\left(z^{(l+1)}\right)
\\ \delta^{(L)} = a^{(L)} - y,</script>

<p>Thus Backpropagation is a process wherein the neural network <em>learns from the back (ie the last layer).</em></p>

<h2 id="credits">Credits</h2>
<p>Andrew Ng‚Äôs Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning">here</a>.</p>

<p><a href="https://www.linkedin.com/in/neilostrove">Neil Ostrove</a> for the proof of the partial derivative term.</p>

<p><img src="/assets/img/backpropagation-proof-1.png" alt="Back propagation proof 1" /></p>

<p><img src="/assets/img/backpropagation-proof-2.png" alt="Back propagation proof 2" /></p>
:ET