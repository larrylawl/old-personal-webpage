---
layout: post
title: "NUS, CS3244: Machine Learning"
author: "Larry Law"
categories: notes
image: cs.jpeg
hidden: true
---
Lecturer: Low Kian Hsiang <br>

<!-- omit in toc -->
# Table of Contents
- [Introduction](#introduction)
  - [What is Learning?](#what-is-learning)
- [Concept Learning and the General-To-Specific Ordering](#concept-learning-and-the-general-to-specific-ordering)
  - [Introduction](#introduction-1)
  - [A Concept Learning Task](#a-concept-learning-task)
    - [Notations](#notations)
    - [The Inductive Learning Hypothesis](#the-inductive-learning-hypothesis)
  - [Concept Learning as Search](#concept-learning-as-search)
    - [General-To-Specific Ordering of Hypotheses](#general-to-specific-ordering-of-hypotheses)
  - [Find-S: Finding a maximally specific hypothesis](#find-s-finding-a-maximally-specific-hypothesis)
  - [Version Spaces and the Candidate-Elimination Algorithm](#version-spaces-and-the-candidate-elimination-algorithm)
    - [The List-Then-Eliminate Algorithm](#the-list-then-eliminate-algorithm)
    - [A More compact Representation for Version Spaces](#a-more-compact-representation-for-version-spaces)
    - [Candidate-Elimination Learning Algorithm](#candidate-elimination-learning-algorithm)

# Introduction
## What is Learning?
An agent is said to be *learning* if it improves its *performance P* on *task T* based on *experience/observations/data E*.

> T must be fixed, P must be measurable, E must in.

# Concept Learning and the General-To-Specific Ordering
1. Learning from Examples
2. General-to-specific ordering over hypothesis
3. Version spaces and candidate elimination algorithm
4. Picking new examples
5. The need for inductive bias

## Introduction
**Concept Learning:** Inferring a boolean-valued fn from training examples of its input and output

> Concept learning is also a form of supervised learning

<!-- Q: Trade off between expressive power vs smaller hypothesis space, and how is this related to concept learning? -->

## A Concept Learning Task
### Notations
![Concept Learning Task](/assets/img/2020-16-1-CS3244/concept-learning-task.png)

**Definition.** An input instance \$ x \in X \$ satisfies (all constraints of) a hypothesis \$ h \in H \$ iff *h(x) = 1*. In other words, *h* classifies *x* as a +ve example.

**Objective:** Determine a hypothesis \$ h \in H \$ that is **consistent** with *D*

**Definition.** A hypothesis *h* is **consistent** with a set of training examples *D* iff *h(x) = c(x)* for all \$ \langle x, c(x) \rangle \forall \langle x, c(x) \rangle \in D \$

### The Inductive Learning Hypothesis
**Definition.** Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved
examples 

## Concept Learning as Search
**Goal.** Search for a hypothesis \$ h \in H \$ that is **consistent** with *D*

Since hypothesis space *H* is much larger and possibly infinite, we need to *exploit structure* to search efficiently.

### General-To-Specific Ordering of Hypotheses
**Definition.** \$ h_j \$ is more general than or equal to \$ h_k \$ (denoted by \$ h_j ≥_g h_k \$) iff any input instance x that satisfies \$ h_k \$ also satisfies \$ h_j \$

$$
\forall x \in X (h_{k}(x) = 1) \rightarrow (h_{j}(x) = 1)
$$

> \$ ≥_g \$ defines a [partial order](http://mathworld.wolfram.com/PartialOrder.html)(reflexive, antisymmetric, transitive) over *H* and not [total order](http://mathworld.wolfram.com/TotallyOrderedSet.html) (partial order and comparability condition).

**Definition.** \$ h_j \$ is more general than \$ h_k \$ (denoted by \$ h_j >_g h_k \$) iff \$ h_j ≥_g h_k \$ and \$ h_k ≱_g h_j \$

**Definition.** \$ h_j \$ is more specific than \$ h_k \$ iff \$ h_k \$ is more general than \$ h_j \$.

## Find-S: Finding a maximally specific hypothesis

**Intuition.** Start with most specific hypothesis. Whenever it wrongly classifies a +ve training example as −ve, “minimally” generalize it to satisfy its input instance.

1. Initialize *h* to most specific hypothesis in *H*
2. For each +ve training instance *x*
   1. For each attribute constraint \$ a_i \$ in *h*
      1. If *x* satisfies constraint \$ a_i \$ in *h*, then do nothing.
      2. Else, replace \$ a_i \$ in *h* by the next more general constraint that is satisfied by x
3. Output hypothesis *h*

<!-- Q: Proposition 2 of hypothesis space search -->

Limitations
1. Can't tell whether Find-S has learned target concept
2. Can't tell when training examples are inconsistent (ie contains errors or noise)
3. Picks a maximally spsecific *h*
4. Depending on *H*, there might be several?

<!-- Q: Several H? -->

## Version Spaces and the Candidate-Elimination Algorithm

**Definition.** The **version space** \$ VS_{H,D} \$ wrt hypothesis space *H* and training examples *D*, is the subset of hypothesis from *H* consistent with *D*:

$$
VS_{H,D} = { h \in H | \text{h is consistent with D} }
$$

1. If \$ c \ in H \$, then a large enough *D* can reduce \$ VS_{H,D} \$ to *{c}*.
2. If *D* is insufficient, then \$ VS_{H,D} \$ represents the **uncertainty** of what the target concept is 
<!-- Q: What is uncertainty? -->
3. \$ VS_{H,D} \$ contains all consistent hypothesis, including the maximally specific hypotheses

### The List-Then-Eliminate Algorithm

**Intuition.** List all hypotheses in *H*. Then, eliminate any hypothesis found inconsistent with any training example.

1. *VS*: a list containing every hypothesis in *H*
2. For each training example \$ \langle x, c(x) \rangle \$
   1. Remove from VS any hypothesis *h* for which *h(x) ≠ c(x)*
3. Output the list of hypothesis in VS

**Limitation.** Prohibitively expensive to exhaustively enumerate all hypothesis in finite *H*

### A More compact Representation for Version Spaces

**Definition.** The **general boundary** *G* of \$ VS_{H,D} \$ is the set of maximally general members of *H* consistent with *D:*

$$
G=\left\{g \in H | g \text { consistent with } D \wedge\left(\neg \exists g^{\prime} \in H g^{\prime}>_{g} g \wedge g^{\prime} \text { consistent with } D\right)\right\}
$$

<!-- Q: How to read notation, and what is maximally general members? -->

**Definition.** The **specific boundary** *S* of \$ VS_{H,D} \$ is the set of maximally specific members of *H* consistent with *D:*

$$
S=\left\{s \in H | s \text { consistent with } D \wedge\left(\neg \exists s^{\prime} \in H s>_{s} s^{\prime} \wedge s^{\prime} \text { consistent with } D\right)\right\}
$$

Every member of version space lies between these boundaries. **Version Space Representation Theorem:**

$$
V S_{H, D}=\left\{h \in H | \exists s \in S \exists g \in G g \geq_{g} h \geq_{g} s\right\}
$$

<!-- Q: Don't understand proof -->

### Candidate-Elimination Learning Algorithm
**Intutiion.** Start with most general and specific hypotheses. Each training example "minimally" generalizes *S* and specialises *G* to remove inconsistent hypothesis from version space.

1. For each training example *d*
   1. if *d* is a +ve example
      1. Remove from *G* any hypothesis inconsistent with *d*
      2. for each \$ s \in S \$ not consistsent with *d*
         1. Remove *s* from S
         2. Add to *S* all minimal generalisations of *h* of *s* s.t. *h* is consistent with *d,* and some member of *G* is more general than *h* <!-- Q: minimal generalisations? -->
         3. Remove from *S* any hypothesis that is more general than another hypothesis in *S*
   <!-- 2. Q: Point of Influence of +ve and -ve eg? -->
   3. If *d* is a -ve example
      1. Remove from *S* any hypothesis inconsistent with *d*
      2. For each \$ g \in G \$  not consistent with *d*
         1. Remove *g* from *G*
         2. Add to *G all minimal specialisations *h* of *g* st *h* is consistent with *d*, and some member of *S* is more specific than *h*.
         3. Remove from *G* any hypothesis that is more specifc that another hypothesis in G

<!-- Q: Why does +ve eg minimally generalise S and -ve eg specialise G? Why can't it be the other way round?-->
