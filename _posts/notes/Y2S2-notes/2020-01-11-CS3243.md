---
layout: post
title: "NUS, CS3243: Introduction to AI"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: Zick Yair <br>

<!-- omit in toc -->
# Table of Contents
- [Math Primers](#math-primers)
  - [NP and inherently hard problems](#np-and-inherently-hard-problems)
- [Introduction](#introduction)
  - [When Can a Machine Truly Think?](#when-can-a-machine-truly-think)
  - [Rational Agents](#rational-agents)
  - [Specifying Task Environment: PEAS](#specifying-task-environment-peas)
  - [Agenty Types](#agenty-types)
    - [Simple Reflex Agent](#simple-reflex-agent)
    - [Model-based Reflex Agent](#model-based-reflex-agent)
    - [Goal-based agent](#goal-based-agent)
    - [Utility-based agent](#utility-based-agent)
    - [Learning Agent](#learning-agent)
    - [Tradeoff between Exploitation and Exploration](#tradeoff-between-exploitation-and-exploration)
- [Searching for Solutions](#searching-for-solutions)
  - [Notations](#notations)
  - [Tree-Search vs Graph-Search](#tree-search-vs-graph-search)
- [Uninformed Search](#uninformed-search)
  - [Problem-Solving Agents](#problem-solving-agents)
  - [Uninformed Search Strategies](#uninformed-search-strategies)
    - [Breadth-First-Search (BFS)](#breadth-first-search-bfs)
    - [Uniform-Cost Search (UCS)](#uniform-cost-search-ucs)
    - [Depth-First Search](#depth-first-search)
    - [Depth-Limited Search (DLS)](#depth-limited-search-dls)
    - [Iterative Deepending Search (IDS)](#iterative-deepending-search-ids)
    - [Summary](#summary)
- [Informed Search](#informed-search)
  - [Best-First-Search](#best-first-search)
  - [Greedy best-first search](#greedy-best-first-search)
    - [Conditions for optimality: Admissibility and consistency](#conditions-for-optimality-admissibility-and-consistency)
  - [A* Search](#a-search)
  - [Dominance](#dominance)
  - [Generating admissible heuristics from relaxed problems](#generating-admissible-heuristics-from-relaxed-problems)
  - [Generating admissible heuristics from subproblems: Pattern databases](#generating-admissible-heuristics-from-subproblems-pattern-databases)
- [Local Search](#local-search)
  - [Local Search Algorithms](#local-search-algorithms)
- [Adversarial Search](#adversarial-search)
  - [Game: Problem Formulation](#game-problem-formulation)
  - [Minimax Algorithm](#minimax-algorithm)
  - [Alpha-Beta Pruning](#alpha-beta-pruning)
  - [Imperfect Real-Time Decisions](#imperfect-real-time-decisions)
- [Constraint Satisfaction Problems](#constraint-satisfaction-problems)
  - [Defining CSP](#defining-csp)
    - [Variations on the CSP formalism](#variations-on-the-csp-formalism)
    - [Standard Search Formulation (Incremental)](#standard-search-formulation-incremental)
  - [Inference in CSPs](#inference-in-csps)
  - [Backtracking Search](#backtracking-search)
  - [Inference in CSPs](#inference-in-csps-1)
  - [Local Search for CSPs](#local-search-for-csps)
  - [Structured CSPs](#structured-csps)
- [Reinforcement Learning](#reinforcement-learning)
    - [Markdov Decision Problem](#markdov-decision-problem)
    - [Q-Learning](#q-learning)
- [Regret Minimization](#regret-minimization)

# Math Primers
## NP and inherently hard problems
1. Complexity analysis analyzes problems rather than algos
2. **P:** Class of polynomial problems that can be solved in time \$ O(n^k) \$ for some k
3. **NP:** Class of nondeterministic polynomial problems
4. **NP-complete:** Subclass of NP. "Complete" is used in the sense of "most extreme", and thus refers to the hardest problems in the class NP. It has been proven that either the NP-complete problems are in P or none of them is.
5. **co-NP:** Complement of NP (ie for every decision problem in NP, there is a corresponding problem in co-NP with the "yes" and "no" answers reversed)
6. **co-NP-complete:** Hardest problems in co-NP

> Is P = NP? That is, do NP problems have polynomial-time algorithms? This has never been proven.



# Introduction
## When Can a Machine Truly Think?

**Turing Test:**  A computer passes the test if a human interrogator, after
posing some *written* questions, cannot tell whether the written responses come from a person or from a computer. 

**Winograd Schema:** 
1. You are given *m* Winograd schema, with the context word chosen uniformly at random.
2. Design an AI that can correctly resolve a significant number of them.

> E.g. The city councilmen refused the demonstrators a
permit because they [feared/advocated] violence. Who [feared/advocated] violence? The city councilmen feared violence. The demonstraters advocated violence.

Above examples show that single test for Intelligence is...
1. Difficult to resolve
2. Tests tend to be 1) over-specified or 2) very subjective (tradeoff!)
3. Results will be debatable

## Rational Agents

**Agent:** Function maps *percept histories* to *actions*

> Percept are inputs. Percept histories are input histories.

**Rational agent:**
1. For each possible percept sequence, select an action that is expected to *maximize its performance measure*
2. given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.

## Specifying Task Environment: PEAS

Task environment is used for intelligent agent design. PEAS stands for
1. Performance measure: objective criterion for measuring success of an agent's behaviour.
2. Environment
3. Actuators (acting upon that environment through actuators)
4. Sensors (perceiving its env through sensors)

Properties of Task Environments include
1. **Fully observable (vs. partially observable):** sensors provide access to the complete state of the environment at each point in time.
2. **Deterministic (vs. stochastic):** The next state of the environment is completely determined by the current state and the action executed by the agent
3. **Episodic (or memorylessness) (vs. sequential):** The choice of current action does not depend on actions in past episodes
4. **Static (vs. dynamic):** The environment is unchanged while an agent is
deliberating.
5. **Discrete (vs. continuous):** A finite no. of distinct states, percepts, and actions. (turn based)
6. **Single agent (vs. multi-agent):** An agent operating by itself in an env.

## Agenty Types
### Simple Reflex Agent
1. Passive: only acts when it observes a percept
2. Updates *state* based on *percept* only.
3. Easy to implement.

### Model-based Reflex Agent
1. Passive: only acts when it observes a percept
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

> **Model of the world:** Knowledge about "how the world works"

### Goal-based agent
1. Has **goals**, acts to achieve them (not passive)
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Utility-based agent
1. Has **utility function**, acts to achieve them (not passive)
   1. **Motivation:** Goals alone are not enough to generate high quality behaviour (ie many action sequences can get taxi to its goal destination, but some are quicker, safer, more reliable, or cheaper)
   2. **Utility function allows for a more general performance measure**
> Utility Function vs Performance Measure: Function internally used by the agent to evaluate performance vs Function to evaluate agent's external behaviour
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Learning Agent

> Distinction between learning element, which is responsible for making improvements, and the performance element, which is responsible for selecting external actions. 

The performance element is what we have previously considered
to be the entire agent: it takes in percepts and decides on actions. The learning element uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future.

Learning element is responsible for making improvement

### Tradeoff between Exploitation and Exploration
Tradeoff between
1. **Exploitation**: maximizing its expected utility according to its current knowledge of the world
2. **Exploration**: trying to learn more about the world since this may improve its future gains

# Searching for Solutions

## Notations
1. **State:** the state in the state space to which the node corresponds to.
2. **Explore:** Check whether it's a goal state. If not, add the neighbouring nodes into the frontier.
3. **Frontier:** nodes that we have seen but have yet to explore (at initialisation, the frontier is just the source)
4. **Node:** data structure constituting part of search tree. It includes state, parent node, action, and path cost *g(n)*.

> Nodes vs States: Node is a bookkeeping data structure used to represent the search tree. A state corresponds to a configuration of the world. Thus nodes are on particular paths whereas states are not. Furthermore, two different nodes are allowed to contain the same world state (if that state is generated via two different search paths)

## Tree-Search vs Graph-Search
> Algorithms that forget their history are doomed to repeat it.

Recall that any two vertices in a tree is connected by a unique simple path. Thus Tree-Search algorithm is applied to Trees, wherein there will be no repeated states. In contrast, graph-search algorithm is applied to graphs, thus it handles repeated states (in bold)

![Tree vs Graph Search](/assets/img/2020-16-1-CS3243/tree-vs-graph-search.png)

# Uninformed Search
Uninformed search strategies use only information available in the problem definition

## Problem-Solving Agents
**Environment:** Fully observable, deterministic, discrete

**Problem Formulation** <br />
A problem can be defined formally by 5 components (textbook writes 5, but examples in textbook uses 6.)
1. **State**
2. **Initial State** that the agent starts in.
3. **Actions:** A description of the possible actions available to the agent (`Actions(s)`)
4. **Transition Model:** A description of what each action does (`Result(s,a)`)
> Together, the initial state, actions, and transition model define the **state space** of the problem: the set of all states reachable from the initial state by any sequence of actions.
4. **Goal Test:**
   1. Is the state *s* equal the goal state?
   2. Explicit set of goal states
   3. Implicit function (eg `isCheckmate(s)`)
5. **Path cost**
   1. A path cost function that assigns a numeric cost to each path
   2. In this chapter, assume that the cost of a path can be described as the sum of the step costs along the path.
   3. *c(s, a, s'):* the step cost of taking action a in state s to reach state s'.

> There can be more than 1 goal nodes

## Uninformed Search Strategies
**Assumption:** Search in a *tree* data structure

**Evaluation Criteria:**
1. **Completeness:** always find a solution if exists
2. **optimality:** find a least-cost soln (implies completeness)
3. **time complexity:** no. of nodes generated
4. **space complexity:** max no. of nodes in memory

**Problem Parameters**
1. *b* (branching factor): maximum # of successors of any node
2. *d*: depth of shallowest goal node (not optimal)
3. *m*: max depth of search tree


### Breadth-First-Search (BFS)
**Idea:** Expand shallowest unexpanded node <br />
**Implementation:** Frontier is a FIFO queue

Properties of BFS on Graph-based search
1. **Complete?:** Yes (if *b* is finite)
2. **Optimal:** No (unless step costs are equal, ie unweighted). Counterexample: 2 goal nodes, \$ v_1 \$ \$ v_2 \$, with cost of 100 and 1. BFS will return \$ v_1 \$, which is suboptimal.
3. **Time:** \$ O(b) + O(b^2) + ... + O(b^d) = O(b^d) \$
4. **Space:** Max size of frontier \$ O(b^d) \$

Properties hold for tree-based search

> Memory requirements are a bigger problem: one might wait 13 days for the solution to an important problem with search depth *d* 12, but no personal computer has the petabyte of memory it would take.

### Uniform-Cost Search (UCS)

![Uniform Cost Search](/assets/img/2020-16-1-CS3243/uniform-cost-search-algo.png)

**Idea:** Expand least-path-cost, unexpanded node <br />
**Frontier:** Priority Queue ordered by path cost *g* <br />
Equivalent to BFS if all step costs are equal

> <s', c>, where c refers to the additive sum of step costs from source node to s'

> Dijkstra's algorithm, which is perhaps better-known, can be regarded as a variant of uniform-cost search, **where there is no goal state** and processing continues until all nodes have been removed from the priority queue, i.e. until shortest paths to all nodes (not just a goal node) have been determined

> When a node is explored, the path there is guaranteed to be the cheapest.

1. **Complete?:** Yes (if all step costs are ≥ \$ \epsilon \$)
2. **Optimal:** Yes (shortest path nodes expanded first)
3. **Time:** \$ O(b^{1 + \lfloor \frac{C^*}{\epsilon} \rfloor}) \$, where _C_ is the optimal cost
4. **Space:** \$ O(b^{1 + \lfloor \frac{C^*}{\epsilon} \rfloor}) \$

> Derivation of time complexity

Let e denote \$ \epsilon \$

|Steps  |Worst Case Time Complexity  |Distance  |
|---|---|---|
|1  |b  |≥e  |
|2  |b^2  |≥2e  |
|...  |  |  |
|floor(C*/e)  |b^floor(C*/e)  |≥floor(C*/e)e  |

Note that the lower bound for step floor(C\*/e) is floor(C\*/e)e, which is lesser than C\* in the worse case time scenario. Thus, in order to achieve a distance of at least C\*, the max no. of steps will be floor(C\*/e) + 1.

> At step k, keep ≤ \$ b^k \$ nodes in frontier.

Worse case scenario (upper bound):
1. Step 1: source node expands *b* successors
2. Step 2: For each of these *b* successors, they'll have *b* successors. Total of \$ b^2 \$ successors
3. ...
4. Step k: \$ b^k \$ successors

### Depth-First Search
**Idea:** Expand deepest unexpanded node <br />
**Implementation:** Frontier is a LIFO queue

1. **Complete?:** No on infinite depth graphs. No on tree-search version (because of infinite loops).
2. **Optimal:** No (unless step costs are equal, ie unweighted)
3. **Time:** \$ O(b^m) \$ (ie all of the nodes in the search tree)
4. **Space:** Max size of frontier \$ O(bm) \$ (can be \$ O(m) \$)

> What is the advantage of DFS > BFS?

DFS has a better space complexity for tree search, not graph search. (for graph search, worse case scenario would be the entire graph)

> Why is DFS space complexity \$ O(bm) \$

Depth first tree search needs to store only a single path from the root to a leaf node (*m*), along with the remaining unexpanded sibling nodes for each node on the path (*b*).

However, the runtime can be further improved from *O(bm)* to *O(m)* by using **backtracking search.**

In bactracking, only one successor is generated than all successors; each partially expanded node remembers which successor to generate next (eliminate *b*). The idea of generating a successor by *modifying the current state description* instead of directly copying it.

### Depth-Limited Search (DLS)
**Idea:** run DFS with depth limit *l* <br />

### Iterative Deepending Search (IDS)
**Idea:** perform DLSs with increasing depth limit until goal node is found <br />; better if state space is large and depth of soln is unknown

> *d* is first 0, then 1, then 2, and so on.

1. **Complete?:** Yes (if *b* is finite)
2. **Optimal:** No (unless step costs is 1)
3. **Time:** \$ O(b^d) \$ 
4. **Space:** \$ O(bd) \$ (can be \$ O(d) \$)

> Though IDS may seem wasteful because states are generated multiple times, asymptotically the time complexity is the same as BFS. Intuitively, this is because *in a search tree* most of the nodes are in the bottom level, so it does not matter much that the upper levels are generated multiple times.

### Summary
![uninformed search summary](/assets/img/2020-16-1-CS3243/uninformed-search-summary.png)

> These algos are applied on trees, not graphs.FFor graph searches, the main differences are that DFS is complete for finite state spaces, and that the space and time complexities are bounded by the sisze of the state space.

> Assume b > 1 (else the algo breaks down: e.g. time-complexity of BFS  for b=1 would be O(d) however the result on the slides says O(b^d)=O(1^d)=O(1). Now O(d) is not in O(1))

# Informed Search
*Exploit problem-specific knowledge (beyond the problem definition) to obtain heuristics to guide search*

## Best-First-Search
1. **Idea:** Use an evaluation function *f(n)* for each node *n*
   1. Cost Estimate: Expand node with lowest evaluation/cost first
2. **Implementation:** Identical to uniform-cost search, except for the use of *f* instead of *g* to order the priority queue. Most best-first algo include as a component of *f* a heuristic function denote *h(n):*

$$
h(n) = \text{estimated cost of the cheapest path from the state at node n to a goal state}
$$

> Notice that *h(n)* takes a node as inpute, but unlike *g(n)* it depends only on the *state* at the node.

3. **Special Cases** (different choices of *f*):
   1. Greedy best-first search
   2. A* search

## Greedy best-first search

**Intuition:** Greedy best-first search expands the node that appears to be closest to goal. Thus, it evaluates nodes by just using the heuristic function (ie *f(n) = h(n)*)

**Graph Version**
1. **Complete?:** Yes (if *b* is finite). No if tree search even if *b* is finite: infinite loop (similar to DFS)
2. **Optimal:** No (shortest path to Bucharest: 418km)
3. **Time:** \$ O(b^m) \$ (ie entire graph), but a good heuristic can reduce complexity substantially
4. **Space:** Max size of frontier \$ O(b^m) \$ 

### Conditions for optimality: Admissibility and consistency

**h(n) to be an admissible heuristic:** heuristic that never overestimates the cost to reach the goal. (ie *f(n) = g(n) + h(n)* never overestimates the true cost of a soln along the current path through *n*).

> E.g: Straight-line distance, which is by definition the shortest path, thus it cannot be an overestimate.

**h(n) is consistent:** if, for every node *n* and every successor *n'* of *n* generated by action *a*, the estimated cost of reaching the goal from *n* is no greater than the step cost of getting to *n'* plus the estimated cost of reaching the goal from *n':*

$$
h(n) ≤ c(n, a, n') + h(n')
$$

> Every consistent heuristic is also admissible. Proof [here](http://reason.cs.uiuc.edu/eyal/classes/f06/cs440/hw/hw6/hw6_sol.pdf)

This is a form of the general triangle inequality, which stipulates that each side of a triangle (*h(n)*) cannot be longer than the sum of the other two sides. (*c(n, a, n') + h(n')*)

> Here, the triangle is formed by \$ n \$, \$ n' \$, and the goal \$ G_n \$ closest to \$ n \$. 

Likewise, this triangle inequality applies for an admissible heuristic: if there were a route from *n* to \$ G_n \$ via *n'* that was cheaper than *h(n)*, then \$ h(n) ≥ C^\* \$, wherein \$ C^\* \$ refers to the cost to reach \$ G_n \$ and in example, refers to the route via *n'*.

## A* Search
**Intuition:** Improves on uniform cost search (*g(n)*), by avoiding expanding paths that are already expensive (which is given by *h(n)*)

**Evaluation function:** *f(n) = g(n) + h(n)*, where
1. *g(n)* = cost of reaching *n* from start node
2. *h(n)* = cost estimate from *n* to goal
3. *f(n)* = estimated cost of cheapest path **through** n to goal

**Theorem:** If *h(n)* is admissible, then A* using *Tree-Search* is optimal <br />
**Theorem:** If *h(n)* is consistent, then A* using *graph-search* is optimal (proof in page 95 of textbook)

<!-- Q: Slide 30: graph-search discards new paths to a repeated state. Should be repeated nodes right? -->

> Lemma 1: if *h(n)* is consistent, then the values of *f(n)* along any path are nondecreasing

> Lemma 2: (Given that *h(n)* is consistent) Whenever A* selects a node *n* for expansion, the optimal path to that node has been found.

<!-- > However, note that \$ A^\* \$ search still adds back an explored node into the frontier. However, if *h(n)* is consistent, then the *f(n)* along any path is non decreasing, thus the node will not be explored.  -->

1. **Complete?:** Yes (if there is a finite no. of nodes with \$ f(n) ≤ C^\*, C^\* \text{ is the cost of the optimal solution path} \$)
> 1) \$ A^* \$ expands all nodes with \$ f(n) < C^* \$ 2) \$ A^* \$ expands some nodes right on the 'goal contour' before selecting a goal node
1. **Optimal:** Yes
2. **Time:** \$ O(b^{h^{\ast}(s_0) - h(s_0)}) \$ where \$ h^{\ast}(s_0) \$ is the actual cost of getting from root to goal, and \$ h(s_0) \$ is the approximated cost. The difference represents the relative error of the heuristic.
3. **Space:** Max size of frontier \$ O(b^m) \$ (all nodes)

## Dominance
If \$ h_2(n) ≥ h_1(n) \$ for all n (both admissible), then \$ h_2 \$ dominates \$ h_1 \$. It follows that \$ h_2 \$ incurs lower search cost than \$ h_1 \$.

> Proof in page 104 of textbook

## Generating admissible heuristics from relaxed problems
1. A problem with fewer restrictions on the actions is called a **relaxed problem**
2. The cost of an optimal solution to a relaxed problem is also an admissible heuristic for the original problem

> Manhatten distance/ no. of misplaced tiles for k-puzzle problem

## Generating admissible heuristics from subproblems: Pattern databases
"Lower bound of the cost of the optimal solution of this subproblem is a lower bound on the cost of the complete problem." Thus it is admissible.

**Pattern Databases.** The idea behind pattern databases is to store these exact solution costs for every possible subproblem instance.


# Local Search
1. The **path** to a goal is irrelevant (unlike (un)informed search); the goal state itself is the solution.
2. State space = set of "complete" configurations
3. Find final configuration satisfying constraints

## Local Search Algorithms
Maintain single "current best" state and try to improve it

Advantages
1. Very little/constant memory
2. Find reasonable solutions in large state space

**State-Space Landscape**
![State Space Landscape](/assets/img/2020-16-1-CS3243/state-space-landscape.png)

A landscape has both *location* (defined by the state) and *elevation* (defined by the heuristic cost fn or objective fn)

**Hill Climbing Problem**

> Hill climbing is sometimes called **greedy local search** because it grabs a good neighbor state without thinking ahead about where to go next.

**Drawback.** Depending on initial state, can get stuck in local maxima.

**Non-guaranteed fixes.** sideway moves, random restarts

# Adversarial Search
*In which we examine the problems that arise when we try to plan ahead in a world
where other agents are planning against us.*

## Game: Problem Formulation
1. \$ S_0 \$: Initial state
2. States *s*
3. \$ Player(s) \$: Defines which player has the move in a state
4. \$ Actions(s) \$: Returns the set of legal moves in a state
5. \$ Result(s, a) \$: The **transition model** which defines the result of a move
6. \$ Terminal-Test(s) \$: True when the game is over, and false otherwise. States where the game has ended are called **terminal states.**
7. \$ Utility(s,p) \$: A utility function which defines the final numeric value for a game that ends in terminal state *s*  for a player *p*.

**Strategy.** Let \$ V \$ be a set of Nodes and \$ V_{max} \$ be the set of nodes controlled by the MAX player. The *strategy* for the MAX player is 

$$
s: \forall v_{max} \in V_{max}, \exists v \in V: v_{max} \rightarrow v
$$

> Intuitively, a strategy means that you know what to do at every vertex controlled by you.

**Winning Strategy**

**Definition.** A strategy \$ s^\*_1 \$ for player 1 is called winning if for any strategy \$ s_2 \$ by player 2, the game ends with player 1 as the winner.

**Definition.** A strategy \$ t^\*_1 \$ is called non-losing if for any strategy \$ s_2 \$ by player 2, the game ends in either a tie or a win for player 1.

## Minimax Algorithm
For any particular state *s*,

![minimax](/assets/img/2020-16-1-CS3243/minimax.png)

> **Line 2:** Choose the action \$ a \in Actions(s): MINIMAX(RESULT(s, a)) \$ returns the max value. Returns the output of \$ MINIMAX(RESULT(s, a)) \$ for this particular \$ a \$.

Properties of Minimax
1. **Complete?:** Yes (if game tree is finite)
2. **Optimal:** Yes (optimal gameplay)
3. **Time:** \$ O(b^m) \$
4. **Space:** Like DFS: \$ O(bm) \$

5. Runs in time polynomial in tree size
6. Returns a sub-perfect Nash equilibrium.

**Definition. (Nash Equilibrium)** A pair of strategies \$ s_1^{\*} \in S_1, s_2^{\*} \in S_2 \$ for the MAX player and the MIN player, respectively, is a *Nash* equilibrium if no player can get a strictly higher utility by switching their strategy. In other words,

$$
\forall s \in S_1: u_1(s^{\ast}_1, s^{\ast}_2) \geq u_1(s, s^{\ast}_2) \\
\forall s' \in S_2: u_2(s^{\ast}_1, s^{\ast}_2) \geq u_2(s_1^{\ast}, s')
$$

**Definition. (Subgame-Perfect Nash Equilibrium (SPNE))** A pair of strategies \$ s_1^\* \in S_1, s_2^\* \in S_2 \$ is a sub-perfect Nash equilibrium if it is a Nash equilibrium for any subtree of the original game tree.

> The strict subset of the original game tree; it does not contain the original game tree.

**Theorem.** Minimax algoritm outputs a subgame-perfect Nash equilibrium.

## Alpha-Beta Pruning
**Motivation.** Number of game states minimax has to examine is exponential in the depth, *m*, of the tree.

**Intuition.**
1. Maintain a lower bound \$ \alpha \$ and upper bound \$ \beta \$ of the values of, respectively, MAX's and MIN's nodes seen thus far
2. Prune subtrees that will never affect minimax decision

> "If you have an idea that is surely bad, don't take the time to see how truly awful it is." - Pat Winston

**Definition.**
1. \$ \alpha \$ = highest-value choice we have found so far at any choice point along the path for MAX
2. \$ \beta \$ = lowest-value choice we have found so far at any choice point along the path for MIN

<!-- 1. MAX node n: \$ \alpha(n) = \$ highest observed value found on path from *n*; initially \$ \alpha(n) = -\inf \$
1. MIN node n: \$ \beta(n) =\$ the lowest observed value found on path from *n*; initially \$ \beta(n) = +\inf \$ -->

![Alpha Beta Algo](/assets/img/2020-16-1-CS3243/alpha-beta-algo.png)

1. `for each a in Actions(state)`: For each successor of state *s*
2. \$ If v \geq \beta, \$ then return \$ v \$: At the current MAX node \$ n_{MAX} \$, there exist a lowest-choice value \$ \beta \$ along the path for a MIN node such that \$ v \geq \beta \$. Since the minimum value \$ n_{MAX} \$ will return is v, the MIN node will never select the path for this MAX node. 
> This line explains why \$ \alpha-\beta \$ pruning does not remove any strategies that are played in a Nash equilibrium of an extensive form game
3. \$ If v \leq \alpha, \$ then return \$ v \$: At the current MIN node \$ n_{MIN} \$, there exist a highest-choice value \$ \alpha \$ along the path for a MAX node such that \$ v \leq \alpha \$. Since the maximum value \$ n_{MIN} \$ will return is v, the MAX node will never select the path for this MIN node.
> TODO: Remove above after midterms.
4. \$ \alpha = MAX(\alpha, v) \$: Update the highest-value choice we have found so far at the current choice point along the path for MAX
5. Note that \$ \alpha, \beta \$ are local values. 

> WLOG to `MIN-VALUE` function

> \$ \alpha-\beta \$ pruning cannot be used to find a subgame-perfect Nash equilibrium.

Since the entire tree is not explored, the complete strategy is not specified.

<!-- 
**Pruning**
1. Given a MIN node n, stop searching below *n* if there is some MAX ancestor *i* of *n* with \$ \alpha(i) \geq \beta(n) \$
    1. \$ \beta(n) \$: MIN node n will pick the lowest observed value from n. 
    2. \$ alpha(i) \$: For MAX node *i* such that *i* is an ancestor of *n*, if there exist a successor *n'* of *n* such that along this path, the value \$ alpha (i) \geq \beta(n) \$ , *i* will pick the path *n'* instead of the path that leads to *n*.
2. Given a MAX node *n*, stop searching below *n* if there is some MIN ancestor *i* of *n* with \$ \beta(i) \leq \alpha(n) \$ -->

![Alpha Beta Example](/assets/img/2020-16-1-CS3243/alpha-beta-example.png)

> Note that if all nodes are evaluated, every range is tightened to the exact bound (ie the return value of `Minimax(s)`) since you know which exact successor to take. 

**Analysis of pruning**
1. When we prune a branch, it **never** affects final outcome
2. Good move ordering improves effectiveness of pruning
3. Perfect ordering: \$ O(b^{\frac{m}{2}}) \$

## Imperfect Real-Time Decisions
**Problem.** Maximum depth of tree

**Solution.**
1. Replace utility function by a heuristic evaluation function *Eval*, which estimates the position's utility
2. Replace the terminal test by a **cutoff test** that decides when to apply *Eval*
    1. A more robust approach of choosing *d* to cut off is to apply iterative deepening.

> *Eval(s)* is similar to the heuristic function of Chapter 3

![h-minimax](/assets/img/2020-16-1-CS3243/h-minimax.png)

# Constraint Satisfaction Problems
**Intuition.** CSP search algorithms take advantage of the structure of states and use general-purpose rather than problem-specific heuristics to enable the solution of complex problems. The main idea is to eliminate large portions of the search space all at once by identifying variable/value combinations that violate the constraints.

<!-- Q: Don't get the difference between standard search problems and CSP -->

## Defining CSP
A CSP consists of three components, *X, D,* and *C*:
1. *X* is a set of variables, {\$ X_1, \cdots, X_n \$ }
2. *D* is a set of domains, {\$ D_1, \cdots, D_n \$}, one for each variable.
    1. Each domain \$ D_i \$ consists of a set of allowable values, {\$ v_1, \cdots, v_k \$} for variable \$ X_i \$. 
3. *C* is a set of constraints that specify allowable combinations of values.
    1. Each constraint \$ C_i \$ consists of a pair \$ \langle scope, rel \rangle \$
    2. **Scope:** Tuple of variables that participate in the constraint (can be more than 1!)
    3. **Relation:** Relation that defines the values that those variables **can take on**.

> E.g.: \$ \langle (X_1, X_2), X_1 \neq X_2 \rangle \$

> **Relation:** A relation is any subset of a Cartesian product. 

> **Cartesian product:** The Cartesian product of two sets A and B is defined to be the set of all points (a,b) where a in A and b in B. 

**CSP Constraint Graph**
1. **Nodes:** Variables of the problem
2. **Edges:** Constraint

![CSP Example](/assets/img/2020-16-1-CS3243/CSP-example.png)

To solve a CSP, need to define a state space and the notion of a solution.
1. **State Space:** Assignment of values to some or all of the variables. {\$ X_i = v_i, X_j = v_j, v_l, \cdots \$}
    1. **Consistent Assignment:** An assignment that does not violate any constraints.
    2. **Partial Assignment:** Only some variables are assigned values
    3. **Complete Assignment:** Every variable is assigned
 1. **Solution to CSP:** Consistent and complete assignment.

**Constraint Graph**
1. **Binary CSP:** Each constraint relates two variables
2. **Constraint Graph:** Nodes are variables, links are constraints

**Precedent Constraints.** When a task \$ T_1 \$ must occur before task \$ T_2 \$, and task \$ T_1 \$ takes duration \$ d_1 \$ to complete.

$$
T_1 + d_1 \leq T_2
$$

**Disjunctive Constraints.** \$ Axle_F \$ and \$ Axle_B \$ must not overlap in time; either one comes first or the other does:

$$
(Axle_F + 10 \leq Axle_B) \vee (Axle_B + 10 \leq Axle_F)
$$

### Variations on the CSP formalism
**Discrete Variables**
1. Finite Domains (eg *n* variables, domain size d -> \$ O(d^n) \$ complete assignments)
2. Infinite domains (eg integers, strings)

**Continuous Variables**
1. CSP with continuous domains.
2. e.g. start/end times for Hubble Space Telescope. 

**Constraint Variants**
1. **Unary Constraints:** Restricts the value of a single variable (eg \$ SA \neq green \$)
2. **Binary Constraints:** Relates two variables (eg \$ SA \neq WA \$)
3. **Global constraints:** Constraint involving an arbitrary number of variables (not all!). (e.g. In Sodoku problems, all variables in a row or column must satisfy an *Alldiff* constraint) 

**Constraint Hypergraph.** A hypergraph consists of ordinary nodes (the circles in the figure) and hypernodes (the squares), which represent n-ary constraints

![Constraint Hypergraph](/assets/img/2020-16-1-CS3243/constraint-hypergraph.png)

$$
\begin{array}{l}{O+O=R+10 \cdot C_{10}} \\ {C_{10}+W+W=U+10 \cdot C_{100}} \\ {C_{100}+T+T=O+10 \cdot C_{1000}} \\ {C_{1000}=F}\end{array}
$$

### Standard Search Formulation (Incremental)
1. Each state is a partial variable assignment
2. **Initial State:** The empty assignment []
3. **Transition Function:** Assign a *valid* value to an unassigned variable. Fail if no *valid* assignments.
4. **Goal Test:** All variables have been assigned.
  
Note that
1. Uniform model for all CSPs; not domain-specific
2. Every solution appears at depth *n* (*n* variables assigned)
3. Search path is irrelevant, can also use complete-state formulation

## Inference in CSPs

## Backtracking Search
1. **Commutativity:** order of variable assignment is irrelevant (eg [WA = R then NT = G] is equivalent to [NT = G then WA = R])
2. **At every level, consider assignments to a *single* variable.** \$ O(d^n) \$
3. **DFS for CSPs with single-variable assignments is called Backtracking Search**

![Backtracking search algo](/assets/img/2020-16-1-CS3243/backtracking-search-algo.png)

![Backtracking Example](/assets/img/2020-16-1-CS3243/backtracking-example.png)

**Improving Backtracking Efficiency.** General-purpose heuristics can give huge time gains:
1. \$ SELECT-UNASSIGNED-VARIABLE \$: Which variable should be assigned next?
2. \$ ORDER-DOMAIN-VALUES \$: In what order should its values be tried?
3. \$ INFERENCE \$: How can domain reductions on unassigned variables be inferred?
4. Can we detect inevitable failure early?

**Common Heuristics**
1. **Most Constrained Variable (aka minimum-remaining-values heuristic):** Choose the variable with fewest legal values. (Intuition: Go to failure faster)
2. **Most Constraining (not constraint!) Variable (aka degree heuristic):** It attempts to reduce the branching factor on future choices by selecting the variable that is involved in the largest number of constraints on other unassigned variables (ie variable that is in the highest number of scope of a constraint)
3. **Least Constraining Value:** Given a variable, choose the least constraining value.

> Why should variable selection be fail-first, but value selection be fail-last?

For variable ordering, we CAN choose the variable, thus choosing a variable with the minimum number of remaining values helps minimize the number of nodes in the search tree by pruning larger parts of the tree earlier. For value ordering, we HAVE to choose the value, thus it makes sense to look for the most likely values first; the trick is that we only need one solution.

## Inference in CSPs
**Forward Checking.** Whenever a variable *X* is assigned, the forward-checking process establishes arc consistency for it: for each unassigned variable Y that is connected to X by a constraint, delete from Y’s domain any value that is inconsistent with the value chosen for X. Runtime: \$ O(nd) \$

> Because forward checking only does arc consistency inferences, there is no reason to do forward checking if we have already done arc consistency as a preprocessing step.

1. **Problem:** Forward checking propagates information from assigned to unassigned variables, but does not provide early detection for failures.
2. **Solution:** Constraint propagation repeatedly locally enforces constraints
> **Locally** because arc consistency has a local property. 

**Definition.** (Arc Consistency) \$ X_i \$ is arc-consistent wrt \$ X_j \$ (the arc (\$ X_i, X_j \$)  is consistent) iff \$ \forall x \in D_i \$ there exists come value \$ y \in D_j \$ that satisfies the binary constraint on the arc (\$ X_i, X_j \$).

> Note that arc consistency is not commutative. (ie \$ X_i \$ is arc-consistent with \$ X_j \$ does not necessarily imply that \$ X_j \$ is arc consistent with \$ X_i \$ )

> If a graph is completely arc consistent, will we always find an assignment?

No. Arc consistency has a local property. For instance, consider a CSP of three nodes A, B, C, domains are \$ D_{A,B,C} = \{ 0, 1\} \$, and constraints are \$ A \neq B, B \neq C, A \neq C \$. This CSP is completely arc consistent but there is no consistent assignment.

> Does arc inconsistency mean that there won't be an assignment?

No. Arc inconsistency is a temporary property. After the algorithm, the value that is arc consistent will removed, and the variable will be arc consistent again.

1. If X = x makes a constraint impossible to satisfy, remove \$ x \in D_i \$ from consideration.
2. Reducing \$ D_i \$ may result in a chain reaction of domain reductions for others. (art consistency propagation)

**Arc Consistency Algorithm AC-3**
![Arc Consistency Algorithm AC-3](/assets/img/2020-16-1-CS3243/ac3-algo.png)

1. \$ REVISE \$ only changes the domain of \$ X_i \$.
    1. Domain of \$ X_j \$ remains untouched.
    2. Never assigns values
2. for each \$ X_k \$ in \$ X_i.Neighbors - \{ X_j \} \$ do: Ensures arc consistency from \$ X_k \$ to \$ X_i \$. No need to add \$ X_j \$

> Why is therer no need to add \$ X_j \$?

Claim: after checking (Xi, Xj), even if Xi domain is reduced, it doesn’t affect the consistency of (Xj, Xi) 

Case 1: (Xj, Xi) is arc consistent. This means that for every y in domain of Xj, there’s a value z in Xi that satisfies the contraint. When Xi domain is reduced while checking (Xi, Xj), the value that gets deleted from domain of Xi won’t be this value z anyway (cuz only values that don’t satisfy constraint will get deleted). So (Xj, Xi) remains arc consistent. 

Case 2: (Xj, Xi) is arc inconsistent. Then after reducing Xi domain, (Xj, Xi) will still be inconsistent anyway. 

Therefore, checking (Xi, Xj) and reducing the domain of Xi does not affect the arc consistency of (Xj, Xi)

Time Complexity: \$ O(n^2d^3) \$
1. CSP has at most \$ n^2 \$ directed arcs
2. Each arc \$ (X_i, X_j) \$ can be inserted at most *d* times because \$ X_i \$ has at most *d* values to delete
> If the data structure is a tree, then each arc can be inserted at most 1 time. This is because the arc consistency algorithm only goes down the tree; there's no cycle. Thus after arc consistency is established, the domains of the higher nodes will not change, hence the arc will not be reinserted.
1. \$ REVISE: \$ Checking the consistency of an arc takes \$ O(d^2) \$ time
2. \$ O(n^2 \times d \times d^2) = O(n^2d^3) \$

**k-Consistency** <br />
Extend arc consistency (2-consistency) to k-consistency

## Local Search for CSPs

![CSP local search](/assets/img/2020-16-1-CS3243/csp-local-search.png)

> Current is initialised to a **complete** assignment; it is not consistent

## Structured CSPs
**CSPs with Binary Constraints as Graphs**

![CSP Graph](/assets/img/2020-16-1-CS3243/csp-graph.png)

> Learn how to model CSP problems as graphs.

**Thoerem.** If CSP constraint graph is a tree, then we can compute a satisfying assignment (or decide that one does not exist) in \$ O(nd^2) \$.

1. Make parents arc-consistent with children (from bottom to top). (\$ Revise(csp, X_i, X_j) \$, n steps taking \$ O(d^2 \$) each)
2. If step 1 successful, assign values from root down. (n steps taking *O(d)* time each)

> Why is the arc-consistency propagation from bottom to top, and assignment from top to bottom?

Arc-consistency propagation from bottom to top ensures that the parent node will be arc consistent with its children as the child node's domain will not be revised. Assignment is done from top to bottom because the arc consistent edges are directed from top to bottom.

> Why does the theorem only hold when CSP is a tree?

Assignment for node affects only its subtree.

# Reinforcement Learning
*In which we examine how an agent can learn from success and failure, from reward and punishment.*

![Reinforcement Learning Graph](/assets/img/2020-16-1-CS3243/reinforcement-learning-graph.png)

> **Markov Property:** Next state is determined only based on current action and state, not entire sequence

**Definition.** (Policy \$ \pi \$) Function that specifies what the agent should do for *any* state that the agent might reach.
1. **Deterministic Policy:** \$ a_t = \pi(s_t) \$
> The action a deterministic policy take is fixed, but the state s' the state s transits to is not.
2. **Stochastic Policy:** \$ \pi(a \mid s_t) = P(a_t = a \mid s_t) \$ (ie a probability distribution)
3. **Optimal Policy:** An optimal policy is a policy that yields the highest expected utility.

**Value Function.** Tries to predict how good a state is, given the rewards.

$$
\begin{array}{l}{V^{\pi}\left(s_{t}\right)=r_{t}\left(a_{t}, s_{t}\right)+\gamma r_{t+1}\left(a_{t+1}, s_{t+1}\right)+\gamma^{2} r_{t+2}\left(a_{t+2}, s_{t+2}\right)+\cdots} \\ {=\sum_{\ell=0}^{\infty} \gamma^{\ell} r_{t+\ell}\left(a_{t+\ell}, s_{t+\ell}\right)} \end{array}
$$

where \$ \gamma \in (0, 1) \$ is called the **discount rate**.
1. High value of \$ \gamma \$: long-sighted agent, cares about future reward.
2. Low value of \$ \gamma \$: agent is greedy, who cares about the future?

### Markdov Decision Problem
**Intuition.** "What is the next state and reward given current state and action?"

**Definition.** A sequential decision problem for a fully observable, stochastic environment with a Markovian transition model and additive rewards is called a Markov decision process (MDP), and consists of a set of states (with an intial state \$ s_0 \$); a set of \$ ACTIONS(s) \$ of actions in each state; a transition model \$ P(s' \mid s, a) \$ and a reward function \$ R(s) \$.

Given complete knowledge of MDP below, the optimal policy is deterministic: select optimal action in each state. 

$$
P(s_{t + 1} = s; r_{t + 1} = r \mid s_t, a_t)
$$

**Problem.** However, agent does not know the underlying MDP probability distribution. Thus it needs to perform trial-and-error and not lose too much reward along the way.

### Q-Learning

$$
Q^{\pi}(s, a)=r(s, a)+\gamma V^{\pi}(\delta(s, a))
$$

\$ Q^{\pi}(s, a) \$ is the utility obtained if we take action *a* at state *s*, and then follow the policy \$ \pi \$ from then on.

> \$ Q^{\ast}(s, a) \$ uses an optimal policy.

> \$ \delta(s, a) \$ returns the next state given current state and action

$$
\begin{aligned}
  V^{\ast}(s) &= max_aQ(s,a) \\
  Q^{\pi}(s, a)&=r(s, a)+\gamma V^{\ast}(\delta(s, a)) \\
  &=r(s, a)+\gamma max_aQ(s,a)
\end{aligned}
$$

> A temporal-difference (TD) agent that learns a Q-function does not need a model of the form \$ P(s' \mid s, a) \$, either for learning or for action selection.

> **Definition.** Because this update rule uses the difference in utilities between successive states, it is often called the temporal-difference, or TD, equation.

![Q Learning Algorithm](/assets/img/2020-16-1-CS3243/q-learning-algo.png)

> Is this a recursive function? 

No, it's dynamic programming. It stores the Q value in a table and retrieves it.

When \$ r(s, a) \geq 0 \$ and \$ \hat{Q} = 0, \hat{Q}(s, a) \leq Q^\*(s, a) always. In other words, we always underestimate the optimal Q value; they always increase at every iteration and thus converge to \$ Q^\* \$.

**Problem.** One bad experience can result in a bad underestimate of \$ Q^*(s, a) \$.

**Solution.** Change update rule to below. Intuitively, this update rule penalises actions that are always taken; it takes the average of the utility obtained from these actions.

$$
\hat{Q}\left(s_{t}, a\right) \leftarrow\left(1-\alpha_{t}\right) \hat{Q}\left(s_{t}, a\right)+\alpha_{t}\left(r\left(s_{t}, a\right)+\gamma \max _{a} \hat{Q}_{i}\left(\delta\left(s_{t}, a\right), a^{\prime}\right)\right)
$$

> Notice the exponentially weighted average term (first term).

> Why do we need the coefficient \$ (1 - \alpha_t) \$? 

Maintains expectation of Q within \$ Q^* \$. Suppose that this coefficient is not present, then the expectation will only be increasing. 

**Problem.** How do we select an action \$ a_t \$?

**Solution.** Softmax layer. 

$$
P(a|s) = \frac{\mathrm{e}^{\varepsilon \widehat{\mathrm{Q}}(s, a)}}{\sum_{a^{\prime}} \mathrm{e}^{\varepsilon \widehat{\mathrm{Q}}\left(s, a^{\prime}\right)}}
$$

# Regret Minimization

<!-- Q: Best action vs Best algorithm? Why compare to best Action? Definition of best Action? -->

**Best Action:** Action that yields the highest utility; action is fixed.
**Best Algorithm:** (Intuitively, an optimal policy.)

**Theorem.** For any **deterministic** algorithm A, there is a sequence of rewards for which \$ V_{best}^T \geq \ceil \frac{(n - 1)T}{n} \rceil \$ but \$ V_A^T = 0 \$

> Deterministic is the problem: an adverserial player can choose a sequence of action st the theorem is fulfilled.

E.g. Greedy Algorithm
1. At round *t*, let \$ S_t \$ be the set of actions that maximize the total reward up to time *t*
2. Pick one of the best actions (choose the one with the **lowest index** if \$ |S_t| \geq 2 \$)

**Randomized Greedy Algorithm**
![Randomized Greedy Algorithm](/assets/img/2020-16-1-CS3243/randomized-greedy-algo.png)

**Insight.** The Randomized Greedy algorithm can lose at most \$ logn + 1 \$ times (in expectation) for every loss of the best action.

**Key Insight 1:** When some actions from the best actions \$ S_t \$ lose, we remove them from the set of best actions.

**Key Insight 2:** Losing over time is worse than losing once.

![Randomized Greedy Example](/assets/img/2020-16-1-CS3243/randomized-greedy-example.png)

> Denominator decreases by 1 each time because of key insight 1. 

$$
\frac{1}{\left|s_{t_{j}}\right|}+\frac{1}{\left|s_{t_{j}}\right|-1}+\cdots \frac{1}{\left|s_{t_{j}}\right|-k_{j}+1} \leq \log \left|S_{t}\right|+1 \leq \log n+1
$$

<!-- Q: (S35) Episilon typo? Got negative before? -->
<!-- Q: (S35) What does the weights refer to? Weight of choosing an action? Why is W_T = \sum_i w_i^t? -->

$$
V^t_{best} \leq \frac{1}{\epsilon} \times logn + t \times \epsilon + V^t_{MWU}
$$