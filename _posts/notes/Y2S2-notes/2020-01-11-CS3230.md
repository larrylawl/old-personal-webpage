---
layout: post
title: "NUS, CS3230: Design and Analysis of Algorithms"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: [Professor Sung Wing Kin, Ken](https://www.comp.nus.edu.sg/cs/bio/ksung/) <br>

Questions
1. For the master's theorem, you can only apply when it's polynomially bigger/smaller. What if its exponentially bigger?
2. f(n) in example

<!-- omit in toc -->
# Table of Contents
- [Overview](#overview)
  - [Course Objectives](#course-objectives)
- [Correctness](#correctness)
  - [Reasoning and Asymptotic Analysis](#reasoning-and-asymptotic-analysis)
    - [What is a good algorithm?](#what-is-a-good-algorithm)
  - [Correctness of Iterative Algorithm](#correctness-of-iterative-algorithm)
  - [Correctness of Recursive Algorithm](#correctness-of-recursive-algorithm)
  - [Efficiency](#efficiency)
    - [How to design?](#how-to-design)
  - [Analysis of an algorithm](#analysis-of-an-algorithm)
  - [Asymptotic Analysis (Machine Independent Analysis)](#asymptotic-analysis-machine-independent-analysis)
- [Growth of Functions](#growth-of-functions)
  - [Exponentials](#exponentials)
  - [Logarithms](#logarithms)
    - [Stirling's Approximation](#stirlings-approximation)
  - [Summations](#summations)
  - [Limit](#limit)
    - [L'Hopital's Rule](#lhopitals-rule)
  - [Properties of Big O](#properties-of-big-o)
- [Divide-and-Conquer](#divide-and-conquer)
  - [Substitution Method for solving recurrences](#substitution-method-for-solving-recurrences)
  - [Telescoping method](#telescoping-method)
  - [Recursion Tree](#recursion-tree)
  - [Master Method](#master-method)
  - [The divide-and-conquer design paradigm](#the-divide-and-conquer-design-paradigm)
  - [Summary of D&C example time complexities](#summary-of-dc-example-time-complexities)
- [Sorting in Linear Time](#sorting-in-linear-time)
  - [Sorting Lower Bound](#sorting-lower-bound)
  - [Linear-time sorting](#linear-time-sorting)

# Overview
## Course Objectives
1. Learns tools to analyse the performance of algorithms
2. Learns techniques to design an efficient algorithm.

# Correctness
## Reasoning and Asymptotic Analysis
### What is a good algorithm?
1. Correct
2. Efficient
3. Well-documented and with sufficient details
4. Maintainable

## Correctness of Iterative Algorithm

**Loop Invariant**
1. True at the beginning of an iteration
2. Remains true at the beginning of the next iteration

How to use invariant to show the correctness of an iterative algorithm? 
1. **Initialisation:** The invariant is true before the first iteration of the loop
2. **Maintenance:** If the invariant is true before an iteration, it remains true before the next iteration
3. **Termination:** When the algorithm terminates, the invariant provides a useful property for showing correctness.

## Correctness of Recursive Algorithm
Usually use MI on size of problem
1. Base Case
2. Inductive Step: Using the inductive hypothesis, show that the next step is true.  

> With simple induction you use "if 𝑝(𝑘) is true then 𝑝(𝑘+1) is true" while in strong induction you use "if 𝑝(𝑖) is true for all 𝑖 less than or equal to 𝑘 then 𝑝(𝑘+1) is true", where 𝑝(𝑘) is some statement depending on the positive integer 𝑘.

## Efficiency
**Tradeoff between simplicity and efficiency:** A naive simple algo is slower, while a fast algo tends to be complicated.

### How to design?
1. When the problem occurs a few times and small, use a simple algorithm.
2. When the problem occurs many times and big, use an efficient algorithm.

## Analysis of an algorithm
Indicators: 1) time and 2) space complexity

Two ways to analyze: 1) simulation and 2) mathematical analysis (calculating the running time)

## Asymptotic Analysis (Machine Independent Analysis)

**O-notation (upper bound)**
![O-notation](/assets/img/2020-16-1-CS3230/o-notation-graph.png)

> *O(g(n))* is actually a set of functions

> O-notation vs o-notation: ≤ vs <

**Omega-notation (lower bounds)**
**Intuition:** *f(n)* can never be faster than lower bound for given *n*
![Omega-notation](/assets/img/2020-16-1-CS3230/omega-notation-graph.png)

> Omega-notation vs omega-notation: ≥ vs >

**Theta-notation (tight bounds)**
![Theta-notation](/assets/img/2020-16-1-CS3230/theta-notation-graph.png)

> Common time complexities [here.](https://en.wikipedia.org/wiki/Time_complexity); include in cheatsheet!

<!-- TODO: Add in stirling's complexity into the classes -->

# Growth of Functions
## Exponentials

$$
\begin{aligned} a^{-1} &=1 / a \\\left(a^{m}\right)^{n} &=a^{m n} \\ a^{m} a^{n} &=a^{m+n} \\ e^{x} & \geq 1+x \end{aligned}
$$

1. **Any exponential function with base a > 1 grows faster than any polynomial**
   1. Lemma: For any constants *k > 0* and *a > 1*, \$ n^k = o(a^n) \$

## Logarithms

$$
\begin{aligned} a &=b^{\log _{b} a} \\ \log _{c}(a b) &=\log _{c} a+\log _{c} b \\ \log _{b} a^{n} &=n \log _{b} a \\ \log _{b} a &=\frac{\log _{c} a}{\log _{c} b} \\ \log _{b}(1 / a) &=-\log _{b} a \\ \log _{b} a &=\frac{1}{\log _{a} b} \\ a^{\log _{b} c} &=c^{\log _{b} a} \end{aligned}
$$

1. Base of log does not matter in asymptotics (can use change of bases, and one of which will be a constant)

### Stirling's Approximation
Tighter upper bound for factorial

$$
\begin{array}{l}{n !=\sqrt{2 \pi n}\left(\frac{n}{e}\right)^{n}\left(1+\Theta\left(\frac{1}{n}\right)\right)} \\ {\log (n !)=\Theta(n \lg n)}\end{array}
$$

> Important for exams. Particularly the time complexity!

## Summations
Arithmetic Series

$$
S_n = \frac{n}{2}[2a + (n - 1)d]
$$

Geometric Series

$$
S_n = \frac{a(r^n-1)}{r - 1} \\
S_{\infty} = \frac{a}{1-r}
$$

Harmonic Series

$$
\begin{aligned} H_{n} &=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{n} \\ &=\sum_{k=1}^{n} \frac{1}{k} \\ &=\ln n+O(1) \end{aligned}
$$

> Important for exams

Telescoping Series

$$
\begin{aligned} \sum_{k=1}^{n-1} \frac{1}{k(k+1)} &=\sum_{k=1}^{n-1}\left(\frac{1}{k}-\frac{1}{k+1}\right) \\ &=1-\frac{1}{n} \end{aligned}
$$

## Limit

$$
\begin{array}{l}{\text { Assume } \mathrm{f}(\mathrm{n}), \mathrm{g}(\mathrm{n})>0} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)=0 \rightarrow f(n)=o(g(n))} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)<\infty \rightarrow f(n)=O(g(n))} \\ {0<\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)<\infty \rightarrow f(n)=\Theta(g(n))} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)>0 \rightarrow f(n)=\Omega(g(n))} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)=\infty \rightarrow f(n)=\omega(g(n))}\end{array}
$$

### L'Hopital's Rule
$$
\lim_{x \to \inf}\frac{f(x)}{g(x)} = \lim_{x \to \inf}\frac{f'(x)}{g'(x)} 
$$

## Properties of Big O
![Properties of big O](/assets/img/2020-16-1-CS3230/properties-big-o.png)

# Divide-and-Conquer
How to analyze the running time of a recursive algo?
1. Derive a recurrence
2. Solve the recurrence

## Substitution Method for solving recurrences
The most general method:
1. Guess the form of the solution
2. Verify by induction

> Don't verify inductive hypothesis with O-notation. e.g. Verifying \$ T(k) = c • k^2 \$ with \$ O\(k^2\) \$.

**Heuristics to making a good guess**
1. Use recursion trees to generate good guesses
2. Prove loose upper and lower bounds on the recurrence and then reduce the range of uncertainty

## Telescoping method
![Telescoping method](/assets/img/2020-16-1-CS3230/telescoping-method.png)

## Recursion Tree
![Recursion Tree](/assets/img/2020-16-1-CS3230/recursion-tree.png)

## Master Method
Let *a ≥ 1* and *b > 1* be constants, let *f* to be asymptotically positive, and let T(n) be defined on the non negative integers by the recurrence

$$
T(n) = aT(n/b) + f(n), where
$$

> we interpret *n/b* to mean either floor(*n/b*) or ceiling(*n/b*).

> **a**: # subproblems, **n/b:**  subproblem size, **f(n):** work dividing and combining

Then *T(n)* has the following asymptotic bounds:

1. If \$ f(n) = O(n^{log_ba-\epsilon}) \$ for some constant \$ \epsilon > 0 \$, then \$ T(n) = \Theta(n^{log_ba}) \$
2. If \$ f(n) = \Theta(n^{log_ba}) \$, then \$ T(n) = \Theta(n^{log_ba}lgn) \$
3. If \$ f(n) = \Omega(n^{log_ba+\epsilon}) \$ for some constant \$ \epsilon > 0 \$, and if \$ af(n/b) ≤ cf(n) \$ for some constant *c < 1* and all sufficiently large n, then \$ T(n) = \Theta(f(n)) \$

> In the third case, *af(n/b) ≤ cf(n)* is also called the *regularity condition*

**Intuition:** All three cases are comparing the function *f(n)* with the function \$ n^{log_ba} \$. Intuitively, the larger of the two function determines the solution to the recurrence. 
1. **Case 1:** When \$ n^{log_ba} \$ is **polynomially bigger** (ie \$ \frac{n^{log_ba}}{f(n)} = n^{\epsilon},  \epsilon > 0 \$)
2. **Case 2:** When they are the same size, thus we multiply by a logarithmic factor *lgn* to obtain \$ T(n) = \Theta(n^{log_ba}lgn) = \Theta(f(n)lgn) \$.
3. **Case 3:** When *f(n)* is **polynomially bigger**


> Note that these three cases do not cover all possibilities for *f(n)*. The gaps are...

1. Between case 1 and 2: when f(n) is smaller than \$ n^{log_ba} \$, but **not polynomially smaller**
2. Between case 2 and 3: when f(n) is bigger than \$ n^{log_ba} \$, but **not polynomially bigger**

> What about \$ \frac{f(n)}{n^{log_ba}} = \frac{1}{2^n} \$ (ie f(n) is exponentially smaller than \$ n^{log_ba} \$)? 

Yes.

$$
f(n) = \frac{n^{log_ba}}{2^n} = O(n^{log_ba - \epsilon}), where \epsilon < log_ba
$$

## The divide-and-conquer design paradigm
1. **Divide** the problem into subproblems
2. **Conquer** the subproblems by solving them *recursively*
3. **Combine** subproblem solutions

Challenge is in categorising the problem into the three steps. Example:

![power number](/assets/img/2020-16-1-CS3230/power-number.png)

<!-- Q: Diagonalise matrix -> no need to compute power -> O(1) time? -->

## Summary of D&C example time complexities
![dnc examples](/assets/img/2020-16-1-CS3230/dnc-examples.png)

> Note that master method can be used to analyse the complexity (so practice the math)

# Sorting in Linear Time
**Classfication of the sorting algorithm** 
1. **In-Place:** If it uses very little additional memory beyond that used for the data (usually \$ O(1) \$ or \$ O(lg n) \$). (thus it is sorting in the place of the data)
2. **Stable:** If the original order of equal elements is preserved in the sorted output
3. **Comparison:** Sort the elements by comparing them only

## Sorting Lower Bound
**Decision Tree Model.** A decision tree can model the execution of *any* comparison sort.
1. One tree for each input size *n*
2. Tree contains the comparisons along all possible instruction traces
3. Run-time = length of the path taken
4. Worse-case run time = height of tree

**Lower Bound for decision-tree sorting** <br />
**Theorem.** Any decision tree that can sort *n* elements must haveheight \$ \omega(nlgn) \$.

**Corollary.** Heapsort and mergesort are asymptotically optimal *comparison* sorting algorithms.

> The sorting lower bound assumes we only compare elements. If we do more than comparison, we may be able to break the lower bound.

## Linear-time sorting
**Counting Sort.** Link [here](https://www.geeksforgeeks.org/counting-sort/).
![Counting Sort Analysis](/assets/img/2020-16-1-CS3230/counting-sort-analysis.png)

Properties.
1. Stable
2. No comparison

**Radix Sort** Link [here](https://www.geeksforgeeks.org/radix-sort/).

1. Sort *n* words of *b* bits each
2. Each word can be viewed as having \$ \frac{b}{r} \$ base (base 10 in radix sort for numbers would be: 0, 1, 2, ..., 9). 
3. Since each word is split into blocks of r bits, we have \$ 2^r \$ digits in each block.

$$
T(\text{Counting Sort}) = T(n, k) = \Theta(n + k) \\
T(\text{Radix Sort}) = T(n, b) = \Theta(\frac{b}{r}(n + 2^r))
$$

1. \$ \Theta(n + 2^r) \$: each *b*-bit word is broken into *r*-bit pieces
2. \$ \frac{b}{r} \$: No. of passes

**Choosing r to minimize T(n, b)**

1. Increasing *r* means fewer passes, but as \$ r > lgn \$, the time grows exponentially with respect to n.
2. By differentiation, the optimal *r* is slightly smaller than \$ lgn \$. 
3. By choosing \$ r = lgn \$ implies \$ T(n, b) = \Theta(bn/lgn) \$
    1. For numbers in the range from 0 to \$ n^d - 1 \$, we have \$ b = dlgn \implies \Theta(dn) \$
<!-- Q: Why is b = dlgn -->

**Drawback.** Unlike quicksort, radix sort displays little locality of reference, and thus a well-tuned quicksort fares better on modern processors, which feature steep memory hierarchies.

<!-- Q: What...? -->


