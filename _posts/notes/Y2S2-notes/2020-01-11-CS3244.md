---
layout: post
title: "NUS, CS3244: Machine Learning"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: Low Kian Hsiang <br>

<!-- omit in toc -->
# Table of Contents
- [Introduction](#introduction)
	- [What is Learning?](#what-is-learning)
- [Concept Learning and the General-To-Specific Ordering](#concept-learning-and-the-general-to-specific-ordering)
	- [Introduction](#introduction-1)
	- [How to represent a Hypothesis?](#how-to-represent-a-hypothesis)
	- [A Concept Learning Task](#a-concept-learning-task)
		- [Notations](#notations)
		- [The Inductive Learning Hypothesis](#the-inductive-learning-hypothesis)
	- [Concept Learning as Search](#concept-learning-as-search)
		- [General-To-Specific Ordering of Hypotheses](#general-to-specific-ordering-of-hypotheses)
	- [Find-S: Finding a maximally specific hypothesis](#find-s-finding-a-maximally-specific-hypothesis)
	- [Version Spaces and the Candidate-Elimination Algorithm](#version-spaces-and-the-candidate-elimination-algorithm)
		- [The List-Then-Eliminate Algorithm](#the-list-then-eliminate-algorithm)
		- [A More compact Representation for Version Spaces](#a-more-compact-representation-for-version-spaces)
		- [Candidate-Elimination Learning Algorithm](#candidate-elimination-learning-algorithm)
	- [Inductive Bias](#inductive-bias)
		- [An Unbiased Learner](#an-unbiased-learner)
		- [Inductive Bias](#inductive-bias-1)
		- [Comparing Learners with Different Inductive Biases](#comparing-learners-with-different-inductive-biases)
- [Decision Trees](#decision-trees)
	- [Expressive Power](#expressive-power)
	- [Hypothesis/Search Space](#hypothesissearch-space)
	- [Decision-Tree-Learning](#decision-tree-learning)
		- [Which Attribute is the Best Classifier?](#which-attribute-is-the-best-classifier)
		- [Entropy Measures Homogeneity of Examples](#entropy-measures-homogeneity-of-examples)
		- [Information Gain](#information-gain)
		- [Inductive Bias of Decision-Tree-Learning](#inductive-bias-of-decision-tree-learning)
	- [Occam's Razor](#occams-razor)
		- [Overfitting](#overfitting)
			- [Reduced-Error Pruning](#reduced-error-pruning)
			- [Rule Post-Pruning](#rule-post-pruning)
		- [Continuous-Valued Attributes](#continuous-valued-attributes)
		- [Attributes with Many Values](#attributes-with-many-values)
		- [Attributes with Differing Costs](#attributes-with-differing-costs)
		- [Missing Attribute Values](#missing-attribute-values)
- [Artificial Neural Networks](#artificial-neural-networks)
	- [Gradient Descent](#gradient-descent)
		- [Perceptron Training Rule](#perceptron-training-rule)
	- [Stochastic vs Batch Gradient descent](#stochastic-vs-batch-gradient-descent)
	- [Sigmoid Unit](#sigmoid-unit)
	- [Backpropagation Algorithm](#backpropagation-algorithm)
	- [Alternative Loss Function](#alternative-loss-function)
- [Bayesian Inference](#bayesian-inference)
	- [Why Study Bayesian Inference](#why-study-bayesian-inference)
	- [Baye's Theorem/Belief Update](#bayes-theorembelief-update)
	- [How to choose Hypothesis?](#how-to-choose-hypothesis)
	- [Basic Probability Formulas](#basic-probability-formulas)
	- [Relation to Concept Learning](#relation-to-concept-learning)
	- [Learning a Continuous-Valued Function](#learning-a-continuous-valued-function)
	- [Learning to Predict Probabilities](#learning-to-predict-probabilities)
	- [Minimum Description Length (MDL) Principle](#minimum-description-length-mdl-principle)
	- [Most Probable Classification of New Instances](#most-probable-classification-of-new-instances)
		- [Gibbe's Classifier](#gibbes-classifier)
		- [Naive Bayes Clsasifier](#naive-bayes-clsasifier)
		- [Expectation Maximization (EM)](#expectation-maximization-em)

# Introduction
## What is Learning?
An agent is said to be *learning* if it improves its *performance P* on *task T* based on *experience/observations/data E*.

> T must be fixed, P must be measurable, E must in.

# Concept Learning and the General-To-Specific Ordering
1. Learning from Examples
2. General-to-specific ordering over hypothesis
3. Version spaces and candidate elimination algorithm
4. Picking new examples
5. The need for inductive bias

## Introduction
**Concept Learning:** Inferring a boolean-valued fn from training examples of its input and output

> Concept learning is also a form of supervised learning

## How to represent a Hypothesis?

Quadratic is more expressive than linear, but it is the expense of a larger hypothesis space

## A Concept Learning Task
### Notations
![Concept Learning Task](/assets/img/2020-16-1-CS3244/concept-learning-task.png)

> Hypothesis *h* has a tradeoff between **expressive power vs. smaller hypothesis space.** For eg, compare a linear function *f(x) = mx + c* w a quadratic function *f(x) = ax^2 + bx + c*. The latter is more expressive than the former (it can still rep linear functions), but it has a larger hypothesis space (parameters can take on {A x B x C} set of values, as opposed to {M X C})

**Definition.** An input instance \$ x \in X \$ satisfies (all constraints of) a hypothesis \$ h \in H \$ iff *h(x) = 1*. In other words, *h* classifies *x* as a +ve example.

**Objective:** Determine a hypothesis \$ h \in H \$ that is **consistent** with *D*

**Definition.** A hypothesis *h* is **consistent** with a set of training examples *D* iff \$ \forall \langle x, c(x) \rangle \in D, h(x) = c(x) \$.

> Notice the difference between consistent and satisfies?
An example x is said to satisfy hypothesis h when h(x) = 1, regardless of whether x is a positive or negative example of the target concept. However, whether such an example is consistent with h depends on the target concept, and in particular, whether h(x) = c(x). 

### The Inductive Learning Hypothesis
**Definition.** Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved
examples 

## Concept Learning as Search
**Goal.** Search for a hypothesis \$ h \in H \$ that is **consistent** with *D*

Since hypothesis space *H* is much larger and possibly infinite, we need to *exploit structure* to search efficiently.

> Synthetically vs Semantically distinct

1. **Synthetically Distinct:** Include 1) *?*: don't care and 2) \$ \emptyset \$. "Looks different." *(3x2 -> 5x4)*
2. **Sementically Distinct:** 
   1. Include *?* 
   2. Every hypothesis containing 1 or more \$ \emptyset \$ is equivalent to an empty set of input instances, hence classifying every instance as a negative example. *(3x2 -> 4x3 + 1)*

### General-To-Specific Ordering of Hypotheses
**Definition.** \$ h_j \$ is more general than or equal to \$ h_k \$ (denoted by \$ h_j ≥_g h_k \$) iff any input instance x that satisfies \$ h_k \$ also satisfies \$ h_j \$

$$
\forall x \in X: (h_{k}(x) = 1) \rightarrow (h_{j}(x) = 1)
$$

> \$ ≥_g \$ defines a [partial order](http://mathworld.wolfram.com/PartialOrder.html)(reflexive, antisymmetric, transitive) over *H* and not [total order](http://mathworld.wolfram.com/TotallyOrderedSet.html) (partial order and comparability condition).

The converse of this statement, ie (\$ h_j \ngeq_g h_k \$) iff

$$
\exists x \in X: h_k(x) = 1 \wedge h_j(x) = 0
$$

> Note that the converse is not \$ h_j <_g h_k \$

**Definition.** \$ h_j \$ is more general than \$ h_k \$ (denoted by \$ h_j >_g h_k \$) iff \$ h_j ≥_g h_k \$ and \$ h_k ≱_g h_j \$

**Definition.** \$ h_j \$ is more specific than \$ h_k \$ iff \$ h_k \$ is more general than \$ h_j \$.

## Find-S: Finding a maximally specific hypothesis

**Intuition.** Start with most specific hypothesis. Whenever it wrongly classifies a +ve training example as −ve, “minimally” generalize it to satisfy its input instance.

1. Initialize *h* to most specific hypothesis in *H*
2. For each +ve training instance *x*
   1. For each attribute constraint \$ a_i \$ in *h*
      1. If *x* satisfies constraint \$ a_i \$ in *h*, then do nothing.
      2. Else, replace \$ a_i \$ in *h* by the next more general constraint that is satisfied by x
3. Output hypothesis *h*

> Find-S is sensitive to the ordering of inputs. This is because it only outputs 1 consistent hypothesis (which exists in S)

**Proposition 1.** *h* is consistent with *D* iff every +ve training instance satisfies *h* and every -ve training instance does not satisfy *h*

> This proposition is not related to Find-S; it helps us understand why Find-S is consistent with every preceding instances. This proposition is useful because it **relates consistency with satisfy**.

> If there are no -ve instance, then the latter will be vacuously true.

**Proposition 2.** Suppose that \$ c \in H \$. Then, \$ h_n \$ is consistent with 
$$
D=\left\{\left\langle x_{k}, c\left(x_{k}\right)\right\rangle\right\}_{k=1, \ldots, n}
$$

> \$ h_n \$ is consistent with every preceding instances, including negative training instance which was skipped.

Limitations
1. Can't tell whether Find-S has learned target concept
2. Can't tell when training examples are inconsistent (ie contains errors or noise)
3. Picks a maximally specific *h*
4. Depending on *H*, there might be several

## Version Spaces and the Candidate-Elimination Algorithm

**Definition.** The **version space** \$ VS_{H,D} \$ wrt hypothesis space *H* and training examples *D*, is the subset of hypothesis from *H* consistent with *D*:

$$
VS_{H,D} = \{ h \in H \mid \text{h is consistent with D} \}
$$

1. If \$ c \in H \$, then a large enough *D* can reduce \$ VS_{H,D} \$ to *{c}*.
2. If *D* is insufficient, then \$ VS_{H,D} \$ represents the **uncertainty** of what the target concept is 
   1. If *D* is insufficient, then \$ VS_{H,D} \$ will be too general, thus it represents the uncertainty.
3. \$ VS_{H,D} \$ contains all consistent hypothesis, including the maximally specific hypotheses

### The List-Then-Eliminate Algorithm

**Intuition.** List all hypotheses in *H*. Then, eliminate any hypothesis found inconsistent with any training example.

1. *VS*: a list containing every hypothesis in *H*
2. For each training example \$ \langle x, c(x) \rangle \$
   1. Remove from VS any hypothesis *h* for which *h(x) ≠ c(x)*
3. Output the list of hypothesis in VS

**Limitation.** Prohibitively expensive to exhaustively enumerate all hypothesis in finite *H*

### A More compact Representation for Version Spaces
![Version Space Compact Representation](/assets/img/2020-16-1-CS3244/version-space-compact-rep.png)

**Definition.** The **general boundary** *G* of \$ VS_{H,D} \$ is the set of maximally general members of *H* consistent with *D:*

$$
G=\left\{g \in H \mid g \text { consistent with } D \wedge\left(\neg \exists g^{\prime} \in H: g^{\prime}>_{g} g \wedge g^{\prime} \text { consistent with } D\right)\right\}
$$

**Definition.** The **specific boundary** *S* of \$ VS_{H,D} \$ is the set of maximally specific members of *H* consistent with *D:*

$$
S=\left\{s \in H \mid s \text { consistent with } D \wedge\left(\neg \exists s^{\prime} \in H: s>_{g} s^{\prime} \wedge s^{\prime} \text { consistent with } D\right)\right\}
$$

Every member of version space lies between these boundaries. **Version Space Representation Theorem:** (VSRT)

$$
V S_{H, D}=\left\{h \in H \mid \exists s \in S \exists g \in G g \geq_{g} h \geq_{g} s\right\}
$$

### Candidate-Elimination Learning Algorithm
**Intutiion.** Start with most general and specific hypotheses. Each training example "minimally" generalizes *S* and specialises *G* to remove inconsistent hypothesis from version space.

1. For each training example *d*
   1. if *d* is a +ve example
      1. Remove from *G* any hypothesis inconsistent with *d*
      2. for each \$ s \in S \$ not consistent with *d*
         1. Remove *s* from S
         2. Add to *S* all minimal generalisations of *h* of *s* s.t. *h* is consistent with *d,* and some member of *G* is more general than *h* 
         3. Remove from *S* any hypothesis that is more general than another hypothesis in *S*
   2. If *d* is a -ve example
      1. Remove from *S* any hypothesis inconsistent with *d*
      2. For each \$ g \in G \$  not consistent with *d*
         1. Remove *g* from *G*
         2. Add to *G* all minimal specialisations *h* of *g* st *h* is consistent with *d*, and some member of *S* is more specific than *h*.
         3. Remove from *G* any hypothesis that is more specifc that another hypothesis in G

> Why does +ve eg minimally generalise S and -ve eg specialise G? Why can't it be the other way round? (ie +ve specialise G and -ve generalise S)

The addition of +ve example is more likely to make \$ s \in S \$ inconsistent (which rejects more than accepts), thus +ve examples are used to minimally generalise S. vice versa.

> Why is the final version space the same regardless of the sequence of examples?
Finds all maximally general and specific hypotheses, hence the final version space. G and S are **independent** of the order of training examples in the dataset.

**Remarks on Candidate-Elimination**
1. **Does not handle error/noise in training data**
   1. Suppose *i* training example is wrongly labeled as -ve
   2. Hypothesis inconsistent with *i* removed (which includes target concept *c*)
   3. S and G reduced to \$ \emptyset \$ with sufficiently large data
2. **Insufficiently expressive hypothesis representation** (ie insufficient *x*) --> Biased hypothesis space st \$ c \notin H \$ --> S and G reduced to \$ \emptyset \$ with sufficiently large data
3. **Active learner should query input instance that satisfies exactly half of the hypotheses**
   1. Reason:
      1. Assume that the learner does not have the label for the input instance; it needs to query the instance to get the label
      2. Every instance can either be labelled as positive or negative
      3. Picking exactly half of the hypothesis will eliminate half of them regardless of the label of the instance
   2. At most \$ \lceil log_2{(VS_{H,D})} \rceil \$ examples to find target concept *c*
4. **Proposition 3.** (of CE) An input instance *x* satisfies every hypothesis in \$ VS_{H,D} \$ iff *x* satisfies every member of *S*.
5. **Proposition 4.** (of CE) An input instance *x* satisfies none of the hypothesis in \$ VS_{H,D} \$ iff *x* satisfies none of the members of *G*.
6. How to classify new unobserved input instance?
    1. Majority vote, assuming all hypotheses in *H* are equally probable *a priori*
7. Using the hypothesis representation of conjunction of attribute constraints in H, the set S never grows in size since it maintains only one maximally specific hypothesis in each iteration

## Inductive Bias
### An Unbiased Learner
**Intuition.**  Define a new hypothesis space \$ H' \$ that can represent every subset of instances.(i.e. *H'* is the power set of *X*)

> **Power set**, \$ P(S) \$ of any set *S* is the set of all subsets of *S*, including the empty set itself and S itself.

**Limitation (of using H').** Overfitting - cannot classify new unobserved input instances (aka *generalize* beyond observed training examples)

### Inductive Bias

Given 
- Concept learning algorithm *L*
- Input instances *X*, unkown target concept *c*
- Noise-free training examples \$ D_c = \\{ \langle x_k, c(x_k) \rangle \\}_{k=1, \ldots, n} \$

Let \$ L(x, D_c) \$ denote the classification of input instance *x* by *L* after learning from training examples \$ D_c \$

**Inductive Inference Step performed by L**

$$
\left(D_{c} \wedge x_{i}\right) \succ L\left(x_{i}, D_{c}\right)
$$

where the notation \$ y \succ z \$ indicates that z is *inductively* inferred from *y*.

> LHS denotes the conjunction of the \$ D_c \$ existing and \$ x_i \$ existing. 

**Drawback.** The result \$ L(x_i, D_c) \$ that L infers will not in general be **provably correct**; that is, the classification \$ L(x_i, D_c) \$ need not follow deductively from the training data \$ D_c \$ and the description of the new instance \$ x_i \$. Thus, it's interesting to ask what additional assumptions could be added to \$ D_c \wedge x_i \$ st \$ L(x_i, D_c) \$ would follow deductively.

**Deductive Step performed by L**

**Definition.** The **inductive bias** of *L* is any minimal set of assertions *B* s.t. for any target concept c and corresponding training examples \$ D_c \$,

$$
\forall x \in X: \left(B \wedge D_{c} \wedge x\right) \vDash\left(c(x)=L\left(x, D_{c}\right)\right)
$$

> B is added to justify its inductive inferences as deductive inferences

> \$ y \vDash z \$: z can be deductively proven from y. \$ \vDash \$ notation is the logical entailment.

> **Logical Entailment:** A set of sentences (called premises) logically entails a sentence (called a conclusion) if and only if every truth assignment that satisfies the premises also satisfies the conclusion.

**Inductive Bias of Candidate Elimination**

$$
B = \{ c \in H \}
$$

Proof in slides.

### Comparing Learners with Different Inductive Biases
1. **Rote Learner.** Store examples & classify input instance *x* iff it matches that of previously observed example. No inductive bias. The classifications it provides for new instances follow deductively from the observed training examples, with no additional assumptions required. 
2. **Candidate-Elimination.** Inductive bias: \$ c \in H \$
3. **Find-S.** Inductive bias: \$ c \in H \$ and all instances are -ve unless the opposite is entailed by its other knowledge. All instances are -ve assumption is needed as it uses the proposition 1 of Find-S.

> Find-S proposition 1: *h* is consistent with *D* iff every +ve training instance satisfies *h* and every -ve training instance does not satisfy *h*

> Weak to strong bias from top to bottom. More strongly biased methods make more inductive leaps.


# Decision Trees
1. Decision trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification of the instance
2. **Node:** specifies a test of some attribute *A* of the instance
3. **Edge:** one of the possible values for this attribute.

![Decision Trees Example](/assets/img/2020-16-1-CS3244/decision-tree-eg.png)

**Why Study Decision Tree (DT) Learning?**
![Why decision trees](/assets/img/2020-16-1-CS3244/why-decision-trees.png)

## Expressive Power
Decision Trees can express *any* function of the input attributes
- Trivially, there is a consistent decision tree (with training examples) for any training set with one path to leaf for each example
- Drawback: Overfitting. It won't generalize well to classify unobserved input instances.
- Solution: Find compact decision trees

## Hypothesis/Search Space

Number of distinct binary decision trees with *m* Boolean attributes <br />
= \$ 2^m \$ trees choosing the same attributes (ie for each of the *m* attributes, take or don't take) <br />
= \$ 2^{2^m} \$ distinct trees (ie for any tree, for each attribute, swap the subtrees and you'll get 2 distinct trees (since attributes are boolean))

> \$ 2^m trees \$

## Decision-Tree-Learning
**Aim.** Find a small tree consistent with the training examples <br />
**Idea.** Greedily choose "most important" attribute as root of (sub)tree

![Decision Tree Learning Algo](/assets/img/2020-16-1-CS3244/decision-tree-algo.png)

> `Plurality-Value`: majority voting

> Attributes is empty can be caused by 1) error/noisy data. 2) Nondeterministic domain 3) Can't observe input attribute

> If attribute is empty, there is nothing to learn from other than the examples themselves. (ie in \$ \langle x, c(x) \rangle \$, we are missing *x* and only have *c(x)*.) Thus we take the most common value of *c(x)*.

### Which Attribute is the Best Classifier?

**Intuition.** A good attribute splits the examples into subsets that are (ideally) "all +ve" or "all -ve" (ie. classified exactly)

**Definition.** Information gain measures how well a given attribute separates the training examples according to their target classification

### Entropy Measures Homogeneity of Examples

**Intuition.** Entropy, *H*, measures the **uncertainty of classification**, \$ C \in \\{ c_1, \ldots , c_k \\} \$

$$
H(C) = -\sum^k_{i=1}P(c_i)log_2P(c_i)
$$

Thus, entropy for a boolean r.v (ie C = 2)

$$
H(C)=B\left(\frac{p}{p+n}\right)=-\frac{p}{p+n} \log _{2} \frac{p}{p+n}-\frac{n}{p+n} \log _{2} \frac{n}{p+n} = -(qlog_2q + (1-q)log_2(1-q)), where
$$

$$
q = \frac{p}{p+n} = \text{probability that boolean r.v. C is true}
$$

> Quick way of obtaining \$ B(q) \$ is to plot the graph, and find the output of the corresponding q value.

1. If all members of the collection belong to the same class, entropy = 0 (no uncertainty)
2. If the collection contains an equal no. of +ve and -ve examples, entropy = 1. (maximum uncertainty)
3. If the collection contains unequal no. of +ve and -ve eg, then the entropy \$ \in (0, 1) \$ (some uncertainty)

![Entropy Curve](/assets/img/2020-16-1-CS3244/entropy-curve.png)

### Information Gain
![Information Gain Slide](/assets/img/2020-16-1-CS3244/information-gain.png)

![Information Gain eg](/assets/img/2020-16-1-CS3244/information-gain-eg.png)

1. H(C) refers to the entropy of the node *before classification via "Patron"*
2. \$ H(C \mid A) \$ refers to the expected remaining entropy after classification via "Patron".

More generally, the information gain, \$ Gain(S, A) \$ of an attribute *A*, relative to a collection of examples *S*, is defined as

$$
\operatorname{Gain}(S, A) \equiv Entropy(S)-\sum_{v \in V a l u e s(A)} \frac{\left|S_{v}\right|}{|S|} \text {Entropy}\left(S_{v}\right)
$$

where \$ Values(A) \$ is the set of all possible values for attribute *A*, and \$ S_v \$ is the subset of *S* for which attribute *A* has value *v* (i.e. \$ S_v = \{ s \in S \mid A(s) = v \} \$ ). 

1. **First term:** Entropy of the original collection *S*,
2. **Second term:** Expected value of the entropy after *S* is partitioned using attribute A.


### Inductive Bias of Decision-Tree-Learning
**Motivation.** 
1. Given a collection of training examples, there are typically many decision trees consistent with these examples. 
2. Describing the inductive bias therefore consists of describing the basis by which it chooses one of these consistent hypotheses over the others.

**Approximate Inductive Bias of Decision-Tree-Learning:**
1. **Shorter** trees are preferred
2. Trees that place **high information gain attributes close to the root** are preferred
  
> If only (1) is considered, it is exactly the approximate inductive bias for **BFS** for the shortest consistent DT, which can be prohibitively expensive

## Occam's Razor
Prefer **shortest/simplest** hypothesis that **fits the data.**

Arguments in favor:
- Fewer short hypotheses than long hypotheses
	- Short/simple hypothesis that fits data unlikely to be coincidence
	- Long/complex hypothesis that fits data may be coincidence

Arguments opposed:
- Many ways to define small sets of hypothesis
- Small sets of short/simple hypothesis can be obtained using different hypothesis representations

<!-- Q: Don't understand 2nd opposing arg -->

### Overfitting
**Definition.** Hypothesis \$ h \in H \$ **overfits** the set *D* of training examples iff

$$
\exists h^{\prime} \in H \backslash \{h\}\left(e r r o r_{D}(h)<\text { error}_{D}\left(h^{\prime}\right)\right) \wedge\left(\text {error}_{D_X}(h)>\text {error }_{D_X}\left(h^{\prime}\right)\right)
$$

where \$ error_D(h) \$ and \$ error_{D_X}(h) \$ denotes errors of *h* over *D*  and set \$ D_x \$ of examples corresponding to instance space *X* respectively.

**How to avoid overfitting?**
1. Stop growing DT when expanding a node is not statistically significant
2. Allow DT to grow and overfit the data, then post-prune it

**How to select "best" DT?**
1. Measure performance over training examples/data
2. Measure performance over a separate validation dataset
3. MDL: minimize \$ size(tree) \$ & \$ size(misclassifications(tree)) \$

#### Reduced-Error Pruning

1. Partition data into *training* and *validation* sets
2. Do until further pruning is harmful
    1. Evaluate impact on validation set of pruning each possible node
    2. Greedily remove the one that most improves validation set accuracy

Doing so produces smallest version of most accurate subtree. <br />
**Drawback.** What if data is limited?

#### Rule Post-Pruning
1. Convert learned DT to an equivalent set of rules by creating one rule for each path from the root to a leaf
2. Prune (generalise) each rule by removing any precondition that improves its estimated accuracy. One method to estimate rule accuracy is to use a validation set of examples disjoint (no elts in common) from the training set.
3. Sort pruned rules by estimated accuracy into desired sequence for use when classifying unobserved input instances

> **Precondition/Rule antecednet:** Each attribute test along the path from the root to the leaf

> **Postcondition/Rule consequent:** Classification at the leaf node

These has three main advantages
1. Converting to rules allows distinguishing among the different contexts in which a decision node is used.  Because each distinct path through the decision tree node produces a distinct rule, the pruning decision regarding that attribute test can be made differently for each path. In contrast, if the tree itself were pruned, the only two choices would be to remove the decision node completely, or to retain it in its original form. 
<!-- Q: Don't understand -->
2. Converting to rules removes the distinction between attribute tests that occur near the root of the tree and those that occur near the leaves. Thus, we avoid messy bookkeeping issues such as how to reorganize the tree if the root node is pruned while retaining part of the subtree below this test. 
3. Improves readability.


### Continuous-Valued Attributes
Define a **discrete**-valued input attribute to partition the values of a **continuous** input attribute into a discrete set of intervals for testing.

### Attributes with Many Values
**Problem.** *Gain* will select attribute with many values (e.g. Date).

> *Date* has many values that it is bound to separate the training examples into very small subsets, thus having high information gain (despite being a poor predictor of the target function over unseen instances.)

**Solution.** Use *GainRatio* instead:

$$
GainRatio(C,A) = \frac{Gain(C, A)}{SplitInformation(C, A)} \\
SplitInformation(C, A) = -\sum_{i=1}^{d} \frac{\left|E_{i}\right|}{|E|} \log _{2} \frac{\left|E_{i}\right|}{|E|}
$$

where \$ E_1 \$ through \$ E_d \$ are the *d* subsets of examples resulting from partitioning C by the *d*-valued attribute *A*.

> Note that *SplitInformation* is the entropy of *C* wrt *values of A*. (ie `SplitInformation(C, A)`). This is incontrast to `Entropy(C)`, in which we considered only the entropy of *C* wrt target attribute whose value is to be predicted by the learned tree (ie Concept \$ c_i \in C \$ ).

Notice that the *SplitInformation* term discourages the selection of attributes with many uniformly distributed values
1. If a collection of *n* examples are completely separated by attribute *A*, then \$ SplitInformation = log_2n \$
2. In contrast, a boolean attribute *B* that splits the same *n* examples exactly in half will have *SplitInformation = 1*

### Attributes with Differing Costs
**Problem.** For example, in learning to classify medical diseases we might describe patients in terms of attributes such as *Temperature, BiospyResult, Pulse, BloodTestResults.* These attributes vary significantly in their costs, both in terms of monetary cost and cost to patient comfort. *How to learn consistent DT with **low expected cost?***

**Solution.** Replace *Gain* by

$$
\frac{Gain^2(C,A)}{Cost(A)} \\
\frac{2^{Gain(C,A)} - 1}{(Cost(A) + 1)^w}
$$

where \$ w \in [0,1] \$ is a constant that determines the relative importance of cost versus information gain.

> For instance, by dividing the cost, lower cost attributes are preferred.

### Missing Attribute Values
**Problem.** What if some examples *x* are missing values of attribute *A*?

**Solution.** Use the training example and sort through decision tree. Common solutions include,

1. If node *n* tests A, then assign *most common value* of A among other examples sorted to node *n*
2. Assign node *n* the most common value among examples at node *n* that have the same classification *c(x)*
3. Assign *probability* \$ p_i \$ to each possible values of *A* (rather than assigning the most common value to \$ A(x) \$.)

> For example, given a boolean attribute A, if node n contains six known examples with A = 1 and four with A = 0, then we would say the probability that A(x) = 1 is 0.6, and the probability that A(x) = 0 is 0.4. A fractional 0.6 of instance x is now distributed down the branch for A = 1, and a fractional 0.4 of x down the other tree branch. 

# Artificial Neural Networks
![Why Neural Networks](/assets/img/2020-16-1-CS3244/why-nn.png)

![Perceptron Unit](/assets/img/2020-16-1-CS3244/perceptron-unit.png)

## Gradient Descent
### Perceptron Training Rule

**Idea.** Initialize *w* *randomly*, apply perceptron training rule to every training example, and iterate through all training examples till *w* is consistent.

$$
w_{i} \leftarrow w_{i}+\Delta w_{i}, \quad \Delta w_{i}=\eta(t-o) x_{i}
$$

for \$ i = 0, 1, \cdots, n \$, where 
1. \$ t = c(x) \$ is the output for training example \$ \langle x,c(x) \rangle \$
2. \$ o = o(x) \$ is the perceptron output
3. \$ \eta \$ is small +ve constant called learning rate

To get an intuition of the update rule, consider some specific cases.

1. If the perceptron correctly classifies the training example, \$ (t - 0) = 0 \implies \Delta w_i = 0 \$.
2. If t = +1 and o = -1, \$ (t - o) = 2 \implies \Delta = 2\eta x_i \implies \$ weights will be increased. This aligns with the objective of making \$ t = o \$: in order for \$ o = 1 \$, \$ \overrightarrow{w} \cdot \overrightarrow{x} > 0 \$, thus weights need to be increased.

![Decision Surface](/assets/img/2020-16-1-CS3244/decision-surface.png)

> Note that this learing procedure is used for training examples that are *linearly separable*; the perceptron training rule is guaranteed to converge if 1) training examples are linearly separable and 2) learning rate \$ \eta \$ is sufficiently small.

> Note that points on the line are classified as -1 (since \$ w \cdot x \leq 0 \$)

**Gradient Descent Idea.** Search hypothesis space *H* to find weight vector that "best fits" 

Consider a simple linear unit:

$$
o = w \cdot x
$$

Learn *w* that minimises squared error/loss:

$$
L_{D}(\mathbf{w})=\frac{1}{2} \sum_{d \in D}\left(t_{d}-o_{d}\right)^{2}
$$

where D is the set of training examples, \$ t_d \$ and \$ o_d \$ are, respectively, target output and output of linear unit for training example *d*.

**Idea.** Find *w* that minimizes L by first initializing it *randomly* and then repeatedly updating it in the direction of steepest descent. 

**Gradient Vector.**

$$
\nabla L_{D}(\mathbf{w})=\left[\frac{\partial L_{D}}{\partial w_{0}}, \frac{\partial L_{D}}{\partial w_{1}}, \dots, \frac{\partial L_{D}}{\partial w_{n}}\right]
$$

**Training Rule.**

$$
\mathbf{w} \leftarrow \mathbf{w}+\Delta \mathbf{w}, \quad \Delta \mathbf{w}=-\eta \nabla L_{D}(\mathbf{w}), where
$$

$$
\nabla L_{D}(\mathbf{w}) = \frac{\partial L_{D}}{\partial w_i}
$$

$$
\begin{aligned} \frac{\partial L_{D}}{\partial w_{i}} &=\frac{\partial}{\partial w_{i}} \frac{1}{2} \sum_{d \in D}\left(t_{d}-o_{d}\right)^{2} \\ &=\frac{1}{2} \sum_{d \in D} \frac{\partial}{\partial w_{i}}\left(t_{d}-o_{d}\right)^{2} \\ &=\frac{1}{2} \sum_{d \in D} 2\left(t_{d}-o_{d}\right) \frac{\partial}{\partial w_{i}}\left(t_{d}-o_{d}\right) \\ &=\sum_{d \in D}\left(t_{d}-o_{d}\right) \frac{\partial}{\partial w_{i}}\left(t_{d}-\mathbf{w} \cdot \mathbf{x}_{d}\right) \\ \frac{\partial L_{D}}{\partial w_{i}} &=\sum_{d \in D}\left(t_{d}-o_{d}\right)\left(-x_{i d}\right) \\ \Delta w_{i} &=\eta \sum_{d \in D}\left(t_{d}-o_{d}\right) x_{i d} \end{aligned}
$$

> Why does \$ w_i \$ and \$ x_id \$ share the same subscript *i*?
From \$ w \dot x_d \$, only the ith attribute of the example *x* is differentiated. Thus \$ x_i \$ refers to the ith attribute of the weight.

> Note that \$ \Delta w_i \$ for the linear unit and the perceptron is the cosmetically the same. However, the difference lies in \$ o_d \$: the perceptron ouputs 1 if \$ w \dot x > 0 \$ while the linear unit is \$ o(x) = w \dot x \$. 

![Gradient Descent Algo](/assets/img/2020-16-1-CS3244/gradient-descent-linear-unit.png)

> **Linear unit training rule** utilizing **gradient descent** is guaranteed to converge to hypothesis with min. squared error 1) if learning rate \$ \eta \$ is sufficiently small and 2) even when training examples are noisy and/or linearly non-separable by *H*.

## Stochastic vs Batch Gradient descent 

**Stochastic Gradient Descent.** Replace (T4.2) with

$$
w_i \leftarrow w_i + \eta (t - o)x_i
$$

Differences between standard and stochastic gradient descent:
1. In standard gd, the error is summed over *all examples* before updating weights, whereas in stochastic gradient descent weights are updated upon examining *each training example*.
2. Summing over multiple examples in standard gradient descent requires more computation per weight update step. On the other hand, because it uses the true gradient, standard gd is often used with a *larger step size* per weight update than stochastic gd.
> SGD can approximate standard GD arbitrarily close if learning rate \$ \eta \$ is sufficiently small
1. In cases where there are multiple local minima wrt \$ E(\overrightarrow{w}) \$ (squared error term for each trg example), stochastic gradient descent can avoid falling into these local minima because it uses the various \$ \nabla E(\overrightarrow{w}) \$ rather than \$ \nabla E(\overrightarrow{w}) \$ to guide its search.

## Sigmoid Unit

![Sigmoid unit](/assets/img/2020-16-1-CS3244/sigmoid-unit.png)

$$
\begin{aligned} \frac{\partial L_{D}}{\partial w_{i}} &=\frac{\partial}{\partial w_{i}} \frac{1}{2} \sum_{d \in D}\left(t_{d}-o_{d}\right)^{2} \\ &=\frac{1}{2} \sum_{d} \frac{\partial}{\partial w_{i}}\left(t_{d}-o_{d}\right)^{2} \\ &=\frac{1}{2} \sum_{d} 2\left(t_{d}-o_{d}\right) \frac{\partial}{\partial w_{i}}\left(t_{d}-o_{d}\right) \\ &=\sum_{d}\left(t_{d}-o_{d}\right)\left(-\frac{\partial o_{d}}{\partial w_{i}}\right) \\ &=-\sum_{d}\left(t_{d}-o_{d}\right) \frac{\partial o_{d}}{\partial n e t_{d}} \frac{\partial n e t_{d}}{\partial w_{i}} \\ \frac{\partial o_{d}}{\partial n e t_{d}} &=\frac{\partial \sigma\left(n e t_{d}\right)}{\partial n e t_{d}}=o_{d}\left(1-o_{d}\right) \\ \frac{\partial n e t_{d}}{\partial w_{i}} &=\frac{\partial\left(\mathbf{w} \cdot \mathbf{x}_{d}\right)}{\partial w_{i}}=x_{i d} \\ \frac{\partial L_{D}}{\partial w_{i}} &=-\sum_{d \in D}\left(t_{d}-o_{d}\right) o_{d}\left(1-o_{d}\right) x_{i d} \end{aligned}
$$

## Backpropagation Algorithm
**Learning Problem.** Search a large hypothesis space defined by all possible weight values for all the units in the network. 

**Idea.** Initialize *w* randomly, propagate input forward and then errors backward through the network for each training example.

![Backpropagation Algorithm](/assets/img/2020-16-1-CS3244/backpropagation-algo.png)

1. \$ \delta_n = -\frac{\partial E}{\partial net_n} \$ denotes the error term associated with unit n. It plays a role analagous to the quantity *(t-o)* of the delta training rule.
2. Only 2 layers! thus \$ \delta_h \$ is dependent on output \$ \delta_k \$ rather than \$ delta_{h+1} \$.

> What is the difference between backpropagation and gradient descent?

In gradient descent one is trying to reach the minimum of the loss function with respect to the parameters using the derivatives calculated in the back-propagation.

**Remarks on Backpropagation**
1. \$ L_D \$ has multiple **local minima**. GD is guaranteed to converge to some local min., but not necessarily global min.
2. Often include weight **momentum** \$ \alpha \in [0, 1): \$

$$
\Delta w_{h k} \leftarrow \eta \delta_{k} o_{h}+\alpha \Delta w_{h k}, \quad \Delta w_{i h} \leftarrow \eta \delta_{h} x_{i}+\alpha \Delta w_{i h}
$$

3. Easily generalised to feedforward networks of **arbitrary depth**
4. **Expressive hypothesis space.** Requires limited depth feedforward networks:
	1. Every **Boolean** function can be represented by a network with one hidden layer but may require exponential hidden units in no. of inputs
	2. Every **bounded continuous** function can be approximated with arbtrarily small error by a network with one hidden layer
	3. **Any** function can be approximated to arbitrary accuracy by a network with two hidden layers.
5. **Approximate Inductive Bias.** Smooth interpolation between data points.

## Alternative Loss Function
1. **Penalize large weights:**

$$
L_{D}(\mathbf{w})=\frac{1}{2} \sum_{d \in D} \sum_{k \in K}\left(t_{k d}-o_{k d}\right)^{2}+\gamma \sum_{j, \ell} w_{j \ell}^{2}
$$

> Regularisation term

2. Train on **target values** (first term) as well as **slopes** (second term):

$$
\left.L_{D}(\mathbf{w})=\frac{1}{2} \sum_{d \in D} \sum_{k \in K}\left[\left(t_{k d}-o_{k d}\right)^{2}\right]+\mu \sum_{i=1}^{n}\left(\frac{\partial t_{k d}}{\partial x_{i d}}-\frac{\partial o_{k d}}{\partial x_{i d}}\right)^{2}\right]
$$

3. Tie together weights (e.g. phoneme recognition networks)

# Bayesian Inference
## Why Study Bayesian Inference
Provides **practical** learning algorithms
1. Naive Bayes classifiers & Bayesian belief networks
2. Allows **prior knowledge** to be combined with **observed data** to give **probabilistic prediction.**
3. Allows new input instance to be classifed by **combining predictions of multiple hypotheses** weighted by their beliefs.
4. **Incrementally updates belief** of hypothesis with each training example.
Provides useful **conceptual** framework:
1. Provides "gold standard" to evaluate other learning algorithms
2. Additional insight into Occam's razor

## Baye's Theorem/Belief Update

Let D be the training examples D = \$ \{ \langle x_d, t_d \rangle \} \$, where \$ t_d \$ is the target output for training example d.

> Note that D is not the sequence of target values (as specified in textbook).

$$
P(h \mid D) = \frac{P(D \mid h)P(h)}{P(D)}, where
$$

1. \$ P(h) \$: initial probability that hypothesis *h* holds, before we have observed the training data. 
> *P(h)* is often called the *prior probability* of h.
1. \$ P(D \mid h) \$: likelihood of data D given h; what is the probability that the given hypothesis generates the data.
2. \$ P(D) = \sum_{h \in H} P(D \mid h)P(h) \$: marginal likelihood/evidence of D.
> Interpretation: If P(D) increases, probability of every other hypothesis increases (since its a summation), thus it makes \$ P(h \mid D) \$ decrease.

> Marginal as it marginalises out hypothesis h. Uses law of total probability to do so.
1. \$ P(h \mid D) \$: posterior belief of h given D

Limitations
1. Requires specifying probabilities and underlying distributions
2. Often prohibitively expensive to compute evidence

## How to choose Hypothesis?

We generally want the most probable hypothesis given the training data (ie **maximum a posteriori (MAP)** hypothesis)

$$
\begin{aligned} h_{\mathrm{MAP}} &=\underset{h \in H}{\arg \max } P(h | D) \\
&=\underset{h \in H}{\arg \max } \frac{P(D | h) P(h)}{P(D)} \\ &=\underset{h \in H}{\arg \max } P(D | h) P(h) \text{(since P(D) is constant wrt h)} \end{aligned}
$$

> Since \$ \frac{1}{P(D)} \$ is called the normalising constant, removing it (the result) is called the unnormalized posterior. 

If \$ P(h) = P(h') \$ for any \$ h, h' \in H \$, then we can further simplify and choose the **maximum likelihood** hypothesis:

$$
h_{\mathrm{ML}}=\underset{h \in H}{\arg \max } P(D | h)
$$

> Note that the above is simply a definition of \$ h_{ML} \$; it can still be used even if \$ P(h) \neq P(h') \$ (ie TM6.3).

The above 2 equations imply that...

$$
\forall h, h' \in H: P(h) = P(h') \implies h_{MAP} = h_{ML}
$$

> But is the converse true?

The converse is not true. Proof by counter example. Suppose there exist two hypothesis \$ h, h' \in H \$ such that \$ P(D \mid h) = 1$, $P(h) = 0.95 \$, \$P(h') = 0.05 \$. \$ h_{MAP} = h_{ML} = h \$. However, \$ P(h) \neq P(h') \$. 

**Brute-Force MAP Hypothesis Learner**
1. For each hypothesis \$ h \in H \$, compute posterior belief

$$
P(h \mid D) = \frac{P(D \mid h)P(h)}{P(D)}
$$

2. Output hypothesis \$ h_{MAP} \$ with highest posterior belief

$$
h_{MAP} = \underset{h \in H}{argmax}P(h \mid D)
$$

## Basic Probability Formulas
**Fundamentals**

$$
P(A \mid B) = \frac{P(A, B)}{P(B)} \\
P(A \mid B) + P(A' \mid B) = 1
$$

**Independence**

$$
\begin{align*} 
	P({A}\cap { B}) &= P({A})P({B}) \\
	P({ A}|{ B}) &= P({A})\\
	P(B|A) &= P(B)
\end{align*}
$$

**Mutually Exclusive**

$$
\begin{align*} 
	P({A}\cap {B}) &= 0 \\
	P({A} \cup {B}) &= P(A) + P(B)
\end{align*}
$$

**Baye's Rule**

$$
\begin{align*}
P({ A}|{ B}) &= \frac{P({ B}|{ A})P({ A})}{P({ B})} \\
P({ A}|{ B}, { C}) &= \frac{P({ B}|{ A}, { C})P({ A} | { C})}{P({ B} | { C})} \\
P(A|B,C) &= \frac{P(A,B,C)}{P(B,C)} = \frac{P(B,C|A)P(A)}{P(B,C)} \\
P(h|x_1, x_2) &= \frac{P(x_2|h)P(h|x_1)}{\sum_h P(x_2|h)P(h|x_1)}
\end{align*}
$$

**Chain Rule for joint probability.** Joint probability \$ P(A_1, \cdots, A_n) \$ of a conjunction of *n* events \$ A_1, \cdots, A_n \$: 

$$
P\left(A_{1}, \ldots, A_{n}\right)=\prod_{i=1}^{n} P\left(A_{i} | A_{1}, \ldots, A_{i-1}\right)
$$

**Inclusion-Exclusion Principle.** Probability of a disjunction/union of *n* events \$ A_1, \cdots, A_n \$:

$$
\begin{aligned} P\left(\bigcup_{i=1}^{n} A_{i}\right)=& \sum_{1 \leq i \leq n} P\left(A_{i}\right)-\sum_{1 \leq i<j \leq n} P\left(A_{i}, A_{j}\right) \\ &+\sum_{1 \leq i<j<k \leq n} P\left(A_{i}, A_{j}, A_{k}\right)-\ldots \\ &+(-1)^{n-1} P\left(A_{1}, \ldots, A_{n}\right) \end{aligned}
$$

**Law of Total Probability.** If events \$ A_1, \cdots, A_n \$ are mutually exclusive such that \$ \sum^n_{i = 1}P(A_i) = 1  \$,

$$
P(B) = \sum^n_{i = 1} P(B \mid A_i)P(A_i)
$$

**Conditional Independence of Events.** A and B are conditionally independent given C iff...

$$
P(A \cap B \mid C) = P(A \mid C)P(B \mid C) \\
P(A \mid B \cap C) = P(A \mid C) = P(A \mid B)
$$

## Relation to Concept Learning

> What would Bayes rule produce as the MAP hypothesis?

> Does FIND-S output a MAP hypothesis?

1. Assuming that input instances \$ x_d \$ for \$ d \in D \$ are fixed
2. Choose \$ P(D \mid h) \$:

$$
\begin{equation}
  P(D \mid h)=\begin{cases}
    1, & \text{if h is consistent with D}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}
$$

> Intuitively, this means that hypothesis *h* generates the data *D* (which is the definition of \$ P(D \mid h) \$: h generated D)

3. Choose \$ P(h) \$ to be uniform distribution:

$$
\forall h \in H: P(h) = \frac{1}{|H|} 
$$

Then, 

$$
\begin{equation}
  P(h \mid D)=\begin{cases}
    \frac{1}{|VS_{H,D}|}, & \text{if h is consistent with D}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}
$$

**Every consistent hypothesis in this example is a MAP hypothesis!**

> Proof. Included as it was used in tutorial 5 and 6.

Case 1: h is inconsistent w/ D

$$
P(h|D) = 0 ( P(D|h) = 0 )
$$

Case 2: h is consistent w/ D

$$
\begin{aligned} P(D) &=\sum_{h_{i} \in H} P\left(D | h_{i}\right) P\left(h_{i}\right) \\ &=\sum_{h_{i} \in V S_{H, D}} 1 \cdot \frac{1}{|H|}+\sum_{h_{i} \notin V S_{k}, D} 0 \cdot \frac{1}{|H|} \\ &=\sum_{h_{i} \in V S_{K, D}} 1 \cdot \frac{1}{|H|} \\ &=\frac{\left|V S_{H, D}\right|}{|H|} \end{aligned}
$$

$$
\begin{aligned} P(h | D) &=\frac{1 \cdot \frac{1}{|H|}}{P(D)} \\ &=\frac{1 \cdot \frac{1}{|H|}}{\frac{\left|V S_{h, D}\right|}{|H|}} \\ &=\frac{1}{\left|V S_{H, D}\right|} \text { if } h \text { is consistent with } D \end{aligned}
$$

![Belief Update](/assets/img/2020-16-1-CS3244/belief-update.png)

## Learning a Continuous-Valued Function
Consider any **real-valued** target function *f* & training examples \$ D = \{ \langle x_d, t_d \rangle \} where \$ t_d \$ is a **noisy** target output for training example *d*:

1. \$ t_d = f(x_d) + \epsilon_d \$
2. \$ \epsilon_d \$ is a random noise variable drawn independently for each \$ x_d \$ according to \$ \epsilon_d \sim N(0, \sigma^2) \$

> Note that random noise variable \$ \epsilon_d \$ need have gaussian distribution

Then, the **maximum likelihood** hypothesis \$ h_{ML} \$ is the one that minimized sum of squared errors:

$$
h_{\mathrm{ML}}=\underset{h \in H}{\arg \min } \frac{1}{2} \sum_{d \in D}\left(t_{d}-h\left(\mathbf{x}_{d}\right)\right)^{2}
$$

**Proof as follows (learn the tricks used!):**

Using the maximum likelihood hypothesis \$ h_{ML} \$

$$
h_{\mathrm{ML}}=\underset{h \in H}{\arg \max } P(D | h)
$$

Assuming that the training examples are mutually independent given h and using the chain rule of joint probability,

$$
h_{\mathrm{ML}}=\underset{h \in H}{\arg \max } \prod_{d \in D} p(t_d|h, x_d)
$$

> Note the assumption that the training examples are **mutually** independent of h. Why can we make the assumption? 

h generates the training examples.

Given that the noise \$ e_i \$ obeys a Normal distribution with zero mean and unknown variance \$ \sigma^2 \$, each \$ t_d \$ must also obey a Normal distribution with variance \$ \sigma^2 \$ centered around the true target value \$ f(x_i) \$ rather than zero. Assuming that h is the correct description of the target function *f*, \$ \mu = f(x_i) = h(x_i) \$, yielding

$$
\begin{aligned} h_{M L} &=\underset{h \in H}{\operatorname{argmax}} \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \sigma^{2}}\left(d_{i}-\mu\right)^{2}} \\ &=\underset{h \in H}{\operatorname{argmax}} \prod_{i=1}^{m} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{1}{2 \sigma^{2}}\left(d_{i}-h\left(x_{i}\right)\right)^{2}} \end{aligned}
$$

We now apply a transformation that is common in maximum likelihood calculations: Rather than maximizing the above complicated expression we shall choose to maximize its (less complicated) logarithm. This is justified because lnp is a monotonic function of p. Therefore maximizing lnp also maximizes p.

> Amazing!

$$
h_{M L}=
\underset{h \in H}{\arg \max } \sum_{d \in D} \ln \frac{1}{\sqrt{2 \pi} \sigma}-\frac{\left(t_{d}-h\left(\mathbf{x}_{d}\right)\right)^{2}}{2 \sigma^{2}}
$$

The first term in the expression is a constant independent of h, and can therefore be discarded, yielding

$$
h_{M L}=
\underset{h \in H}{\arg \max } \sum_{d \in D} -\frac{\left(t_{d}-h\left(\mathbf{x}_{d}\right)\right)^{2}}{2 \sigma^{2}}
$$

Discarding constants that are independent of h 

$$
h_{M L}=
\underset{h \in H}{\arg \max } \frac{1}{2} \sum_{d \in D} -\left(t_{d}-h\left(\mathbf{x}_{d}\right)\right)^{2}
$$

Maximising the negative quantity is equivalent to minimizing the corresponding positive quantity.

$$
h_{M L}=
\underset{h \in H}{\arg \min } \frac{1}{2} \sum_{d \in D} \left(t_{d}-h\left(\mathbf{x}_{d}\right)\right)^{2}
$$

## Learning to Predict Probabilities

Learn a neural network to output \$ P(c(x) = 1) \$ via the maximum likelihood hypothesis \$ h_{ML} \$.

$$
h_{\mathrm{ML}}=\underset{h \in H}{\arg \max } \sum_{d \in D} t_{d} \ln h\left(\mathbf{x}_{d}\right)+\left(1-t_{d}\right) \ln \left(1-h\left(\mathbf{x}_{d}\right)\right)
$$

> \$ h_{ML} \$ is the probability of an output instead of binary. This is because the aim in NN is to predict \$ P(c(x) = 1 \$.

> \$ P(C(x_d) = 1) = P(t_D = 1 \mid x_d) \$ is the true probability of the class label \$ c(x_d) \$ of value1 given input instance \$ x_d \$

> Model confidence determined by the distribution of the weights. If the variance is low (ie the model is confident of the weights), then there's no need to continue training.

## Minimum Description Length (MDL) Principle

**Occams Razor.** Prefer shortest hypothesis that fits the data.

$$
\begin{aligned} h_{\mathrm{MAP}} &=\underset{h \in H}{\arg \max } P(D | h) P(h) \\ &=\underset{h \in H}{\arg \max } \log _{2} P(D | h)+\log _{2} P(h) \\ &=\underset{h \in H}{\arg \min }-\log _{2} P(D | h) -\log _{2} P(h)\end{aligned}
$$

Result of information theory. Optimal (shortest expected description length) code for a message with probability *p* is \$ -log_2p \$ bits.

1. \$ -log\_2P(h) \$ is the description length of h under optimal code for H
2. \$ -log\_2P(D \mid h) \$ is the description length of D given h under optimal code for describing data D
3. Both terms are the shortest expected description length (thus satisfying Occam's razor)

<!-- 
> Expected length of 1 message = Entropy

$$
\sum_{\text{possible message}} p(-log_2p) bits
$$ -->

**MDL.** Select hypothesis that minimizes 

$$
h_{\mathrm{MDL}}=\underset{h \in H}{\arg \min } L_{C_{1}}(h)+L_{C_{2}}(D | h)
$$

where \$ L_C(x) \$ is the description length of x under encoding C.

**Example.** H = decision trees.
1. \$ L_{C_{1}}(h) \$ is number of bits to describe tree h
2. \$ L_{C_{2}}(D \mid h) \$ is number of bits to describe D given h.
    1. \$ L_{C_{2}}(D \mid h) = 0 \$ if examples classified perfectly by h. Otherwise, only misclassification need to be classified.

> \$ h_{MDL} \$ trades off tree size for training errors. Mitigates overfitting.

## Most Probable Classification of New Instances
Given new instance *x*, what is its most probable classification given the training data *D*?

> \$ h_{MAP} \$ is the most probable hypothesis, but not necessarily the most probable classification. 

E.g. Consider *H* with 3 possible hypotheses:

$$
P(h_1|D) = .4, P(h_2|D) = .3, P(h_3|D) = .3
$$

Suppose that the new instance *x* is given and

$$
h_1(x) = +, h_2(x) = -, h_3(x) = -
$$

\$ h_{MAP} \$ will pick \$ h_1 \$ but the most probable classfication for the new instance x is -.

**Bayes-optimal classification** <br />
**Intuition.** Taking the consensus; the most probable classification.

$$
\underset{t \in T}{\arg \max } P(t | D)=\underset{t \in T}{\arg \max } \sum_{h \in H} P(t | h) P(h | D)
$$

> More useful if data is limited (prevent overfitting).


### Gibbe's Classifier
Baye's-optimal Classifier provides best performance but is computationally costly if *H* is large.

**Gibbe's Algorithm**
1. Sample a hypothesis *h* from posterior belief \$ P(h \mid D) \$.
2. Use *h* to classify new instance *x*.

**Surprising Result.** Supposing target concepts are sampled from some prior over *H*, expected misclassification error of Gibbs classifier is at most twice that of Bayes-optimal classifier.

**Concept Learning.** Supposing target concepts are sampled from uniform prior over *H*, a hypothesis is sampled from uniform prior over *VS* and its expected misclassification error is no worse than twice that of Bayes-optimal classifier.

### Naive Bayes Clsasifier
One of the most practical ML models like decision trees and neural networks.

**Successful Applications.** Diagnosis, classifying text documents.

**Limitations**
1. Moderate or large training data is available
2. Input attributes are conditionally independent given classification.

Consider target concept \$ c : X \rightarrow T \$ where each instance \$ x \in X \$ is represented by input attributes \$ x = (x_1, \cdots, c_n)^T \$. 

The **most probable classification** of new instance *x* is

$$
\begin{aligned} t_{\mathrm{MAP}} &=\underset{t \in T}{\arg \max } P\left(t | x_{1}, \dots, x_{n}\right) \\ &=\underset{t \in T}{\arg \max } \frac{P\left(x_{1}, \dots, x_{n} | t\right) P(t)}{P\left(x_{1}, \dots, x_{n}\right)} \\ &=\underset{t \in T}{\arg \max } P\left(x_{1}, \dots, x_{n} | t\right) P(t) \end{aligned}
$$

Using **naive Bayes assumption** \$ P(x_1, \cdots, x_n \mid t) = \prod^n_{i = 1}P(x_i \mid t) \$, 

$$
t_{\mathrm{NB}}=\underset{t \in T}{\arg \max } P(t) \prod_{i=1}^{n} P\left(x_{i} ]mid t\right)
$$

![Naive Bayes Algorithm](/assets/img/2020-16-1-CS3244/naive-bayes-algo.png)

**Properties of Naive Bayes**
1. Conditional Independence Assumption is often violated. However...
2. Naive Bayes works surprisingly well in practice
3. Estimated posteriors \$ \hat{P}(t|x) \$ need not be correct; only need that 

$$
\underset{t \in T}{\arg \max } \widehat{P}(t) \prod_{i=1}^{n} \widehat{P}\left(x_{i} \mid t\right)=\underset{t \in T}{\arg \max } P(t) P\left(x_{1}, \ldots, x_{n} \mid t\right)
$$

> So long as the output t is the same in LHS and RHS

![Naive Bayes Property](/assets/img/2020-16-1-CS3244/naive-bayes-properties.png)

1. When m tends to 0, ignore p (reduces to frequency counting)
2. When m tends to infinity, ignore frequency counting.

### Expectation Maximization (EM)
**When to Use?**
1. Data is only partially observable
2. Unsupervised clustering (target output unobservable)
3. Supervised learning (some input attributes unobservable)

**Applications**
1. Training Bayesian belief networks
2. Unsupervised clustering
3. Learning hidden Markov models
4. Inverse reinforcement learning

**EM for Estimating M Means** <br />
Given
1. Instances from *X* generated by mixture of *M* Gaussians with the same known variance \$ \sigma^2 \$.
2. Unknown means \$ \langle \mu_1, \cdots, \mu_M \rangle \$ of the *M* Gaussians.
3. Don't know which instance \$ x_d \$ is generated by which Gaussian

Determine **maximum likelihood (ML)** estimates of \$ \langle \mu_1, \cdots, \mu_M \rangle \$

Consider full description of each instance as \$ d = \langle x_d, z_{d1}, \cdots , z_{dm} \rangle \$ where
1. \$z_{dm}\$ is unobservable and is of value 1 if the *m-th* Gaussian is selected to generate \$ x_d \$, and 0 otherwise.
2. \$ x_d \$ is observable.


**EM Algorithm.** Pick random initial \$ h = \langle \mu_1, \mu_2 \rangle \$. Then iterate
1. **E Step.** Calculate the expected value \$ E\[ z_{dm} \] \$ of each hidden/latent variable \$ z_{dm} \$, assuming the current hypothesis \$ h = \langle \mu_1, \mu_2 \rangle \$ holds.

$$
\mathbb{E}\left[z_{d m}\right]=\frac{p\left(x_{d} \mid \mu_{m}\right)}{\sum_{\ell=1}^{2} p\left(x_{d} \mid \mu_{\ell}\right)}=\frac{\exp \left(-\frac{1}{2 \sigma^{2}}\left(x_{d}-\mu_{m}\right)^{2}\right)}{\sum_{\ell=1}^{2} \exp \left(-\frac{1}{2 \sigma^{2}}\left(x_{d}-\mu_{\ell}\right)^{2}\right)}
$$

> \$ \mathbb{E}[z_{dm}] = \mathbb{E}[z_{dm} \mid h, x_d] =  \$ P(mth gaussian is selected given that \$ x_d \$ is generated)

2. **M Step.** Calculate a new ML hypothesis \$ h' = \langle mu_1', mu_2' \rangle \$, assuming the value taken on by each latent variable \$ z_{dm} \$ is its expected value \$ E \[ z_{dm} \] \$ computed above. Replace h by \$ h' = \langle \mu_1', \mu_2' \rangle \$.

$$
\mu_{m}^{\prime} \leftarrow \frac{\sum_{d \in D} \mathbb{E}\left[z_{d m}\right] x_{d}}{\sum_{d \in D} \mathbb{E}\left[z_{d m}\right]}
$$

> Weighted by the probability that it is generated by the m-th Gaussian (ie \$ E[Z_{dm}] \$)

> Why doesn't the numerator and denominator cancel each other off? 1) \$ x_d \$ depends on d in the summation symbol, thus you cannot simply cancel them off. e.g. (0.6 \* x_1 + 0.7 \* x_2)/ (0.6+0.7) 2) \$ x_d \$ is weighted by the probability that it is generated by the m-th gaussian.

EM Algorithm converges to **local** ML hypothesis *h'* and provides estimates of hidden/latent variables \$ z_{dm} \$. In fact, **local maximum** in \$ mathbb{E}[ln p(D|h')] \$
1. *D* is complete (observable plus unobservable variables) data
2. Expectation is wrt unobserved variables in *D*

**General EM Problem**
Given
Observed data \$ \{x_d \}_{d \in D} \$ <br />
Unobserved data \$ \{ z_d\}_{d \in D} \$ where \$ z_d = \langle z_{d1}, \cdots, z_{dM} \$ <br /> Parameterized probability distribution \$ p(D \mid h) \$ where
1. D = {d} is the complete data where \$ d = \langle x_d, z_d \rangle \$
2. h comprises the parameters

Determine ML hypothesis h' that (locally) maximizes \$ E[ln p(D \mid h')] \$

![General EM Algo 2](/assets/img/2020-16-1-CS3244/general-EM-algo-2.png)