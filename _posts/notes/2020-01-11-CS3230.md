---
layout: post
title: "NUS, CS3230: Design and Analysis of Algorithms"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: [Professor Sung Wing Kin, Ken](https://www.comp.nus.edu.sg/cs/bio/ksung/) <br>

Q:
1. W2Q1

<!-- omit in toc -->
# Table of Contents
- [Overview](#overview)
  - [Course Objectives](#course-objectives)
- [Correctness](#correctness)
  - [Reasoning and Asymptotic Analysis](#reasoning-and-asymptotic-analysis)
    - [What is a good algorithm?](#what-is-a-good-algorithm)
  - [Correctness of Iterative Algorithm](#correctness-of-iterative-algorithm)
  - [Correctness of Recursive Algorithm](#correctness-of-recursive-algorithm)
  - [Efficiency](#efficiency)
    - [How to design?](#how-to-design)
  - [Analysis of an algorithm](#analysis-of-an-algorithm)
  - [Asymptotic Analysis (Machine Independent Analysis)](#asymptotic-analysis-machine-independent-analysis)
- [Growth of Functions](#growth-of-functions)
  - [Exponentials](#exponentials)
  - [Logarithms](#logarithms)
    - [Stirling's Approximation](#stirlings-approximation)
  - [Summations](#summations)
  - [Limit](#limit)
    - [L'Hopital's Rule](#lhopitals-rule)
  - [Properties of Big O](#properties-of-big-o)
- [Divide-and-Conquer](#divide-and-conquer)
  - [Substitution Method for solving recurrences](#substitution-method-for-solving-recurrences)
  - [Telescoping method](#telescoping-method)
  - [Recursion Tree](#recursion-tree)
  - [Master Method](#master-method)

# Overview
## Course Objectives
1. Learns tools to analyse the performance of algorithms
2. Learns techniques to design an efficient algorithm.

# Correctness
## Reasoning and Asymptotic Analysis
### What is a good algorithm?
1. Correct
2. Efficient
3. Well-documented and with sufficient details
4. Maintainable

## Correctness of Iterative Algorithm

**Loop Invariant**
1. True at the beginning of an iteration
2. Remains true at the beginning of the next iteration

How to use invariant to show the correctness of an iterative algo? 
Thus to show that an invariant is true, we need to show 
1. **Initialisation:** The invariant is true before the first iteration of the loop
2. **Maintenance:** If the invariant is true before an iteration, it remains true before the next iteration
3. **Termination:** When the algorithm terminates, the invariant provides a useful property for showing correctness.

## Correctness of Recursive Algorithm
Usually use MI on size of problem
1. Base Case
2. Inductive Step: Using the inductive hypothesis, show that the next step is true.  

> With simple induction you use "if 𝑝(𝑘) is true then 𝑝(𝑘+1) is true" while in strong induction you use "if 𝑝(𝑖) is true for all 𝑖 less than or equal to 𝑘 then 𝑝(𝑘+1) is true", where 𝑝(𝑘) is some statement depending on the positive integer 𝑘.

## Efficiency
**Tradeoff between simplicity and efficiency:** A naive simple algo is slower, while a fast algo tends to be complicated.

### How to design?
1. When the problem occurs a few times and small, use a simple algorithm.
2. When the problem occurs many times and big, use an efficient algorithm.

## Analysis of an algorithm
Indicators: 1) time and 2) space complexity

Two ways to analyze: 1) simulation and 2) mathematical analysis (calculating the running time)

## Asymptotic Analysis (Machine Independent Analysis)

**O-notation (upper bound)**
![O-notation](/assets/img/2020-16-1-CS3230/o-notation-graph.png)

> *O(g(n))* is actually a set of functions

> O-notation vs o-notation: ≤ vs <

**Omega-notation (lower bounds)**
**Intuition:** *f(n)* can never be faster than lower bound for given *n*
![Omega-notation](/assets/img/2020-16-1-CS3230/omega-notation-graph.png)

> Omega-notation vs omega-notation: ≥ vs >

**Theta-notation (tight bounds)**
![Theta-notation](/assets/img/2020-16-1-CS3230/theta-notation-graph.png)

# Growth of Functions
## Exponentials

$$
\begin{aligned} a^{-1} &=1 / a \\\left(a^{m}\right)^{n} &=a^{m n} \\ a^{m} a^{n} &=a^{m+n} \\ e^{x} & \geq 1+x \end{aligned}
$$

1. **Any exponential function with base a > 1 grows faster than any polynomial**
   1. Lemma: For any constants *k > 0* and *a > 1*, \$ n^k = o(a^n) \$

## Logarithms

$$
\begin{aligned} a &=b^{\log _{b} a} \\ \log _{c}(a b) &=\log _{c} a+\log _{c} b \\ \log _{b} a^{n} &=n \log _{b} a \\ \log _{b} a &=\frac{\log _{c} a}{\log _{c} b} \\ \log _{b}(1 / a) &=-\log _{b} a \\ \log _{b} a &=\frac{1}{\log _{a} b} \\ a^{\log _{b} c} &=c^{\log _{b} a} \end{aligned}
$$

1. Base of log does not matter in asymptotics (can use change of bases, and one of which will be a constant)

### Stirling's Approximation
Tighter upper bound for factorial

$$
\begin{array}{l}{n !=\sqrt{2 \pi n}\left(\frac{n}{e}\right)^{n}\left(1+\Theta\left(\frac{1}{n}\right)\right)} \\ {\log (n !)=\Theta(n \lg n)}\end{array}
$$

> Important for exams

## Summations
Arithmetic Series

$$
S_n = \frac{n}{2}[2a + (n - 1)d]
$$

Geometric Series

$$
S_n = \frac{a(r^n-1)}{r - 1} \\
S_\infin = \frac{a}{1-r}
$$

Harmonic Series

$$
\begin{aligned} H_{n} &=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots+\frac{1}{n} \\ &=\sum_{k=1}^{n} \frac{1}{k} \\ &=\ln n+O(1) \end{aligned}
$$

> Important for exams

Telescoping Series

$$
\begin{aligned} \sum_{k=1}^{n-1} \frac{1}{k(k+1)} &=\sum_{k=1}^{n-1}\left(\frac{1}{k}-\frac{1}{k+1}\right) \\ &=1-\frac{1}{n} \end{aligned}
$$

## Limit

$$
\begin{array}{l}{\text { Assume } \mathrm{f}(\mathrm{n}), \mathrm{g}(\mathrm{n})>0} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)=0 \rightarrow f(n)=o(g(n))} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)<\infty \rightarrow f(n)=O(g(n))} \\ {0<\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)<\infty \rightarrow f(n)=\Theta(g(n))} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)>0 \rightarrow f(n)=\Omega(g(n))} \\ {\lim _{n \rightarrow \infty}\left(\frac{f(n)}{g(n)}\right)=\infty \rightarrow f(n)=\omega(g(n))}\end{array}
$$

### L'Hopital's Rule
$$
\lim_{x \to \inf}\frac{f(x)}{g(x)} = \lim_{x \to \inf}\frac{f'(x)}{g'(x)} 
$$

## Properties of Big O
![Properties of big O](/assets/img/2020-16-1-CS3230/properties-big-o.png)

# Divide-and-Conquer
How to analyze the running time of a recursive algo?
1. Derive a recurrence
2. Solve the recurrence

## Substitution Method for solving recurrences
The most general method:
1. Guess the form of the solution
2. Verify by induction

<!-- TODO: Add in tips from lecture for sub method. I think should include egs, which illustrate common technique. -->

## Telescoping method
![Telescoping method](/assets/img/2020-16-1-CS3230/telescoping-method.png)

## Recursion Tree
![Recursion Tree](/assets/img/2020-16-1-CS3230/recursion-tree.png)

## Master Method

$$
T(n) = aT(n/b) + f(n), where
$$

*a ≥ 1*, *b > 1*, and *f* is asymptotically positive

> Asymptotically posistive: An asymptotically positive function is one that is positive for all. sufficiently large n.

<!-- Q: Don't understand regularity condition -->





