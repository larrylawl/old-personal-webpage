I"ª<p>Without loss of generality to $ R_n $, let us consider $ R_2 $.</p>

<h2 id="prerequisite-basis-vectors">Prerequisite: Basis Vectors</h2>

<p>Suppose I have two vectors, $ \overrightarrow{v_i} $ and $ \overrightarrow{v_j} $. So long as these vectors are linearly independent, any vector can be represented as the linear combination of these two vectors. These vectors are also defined as the basis vectors of a vector space $ V $.</p>

<p>In the ‚Äútypical‚Äù world, the basis vectors are $ \overrightarrow{e_x} = (0, 1) $, $ \overrightarrow{e_y} = (1, 0) $ in $ R_2 $. These special vectors are also called the standard basis.</p>

<h2 id="interpretation-of-matrix-multiplication">Interpretation of Matrix Multiplication</h2>

<p>Now suppose I have any vector $ \overrightarrow{x} $ $ \in $ in $ R_2 $ and is represented by my basis vectors $ \overrightarrow{v_i} $ and $ \overrightarrow{v_j} $.</p>

<p><strong>How will $ \overrightarrow{x} $ look like in the standard basis?</strong></p>

<p>$ \overrightarrow{x} $ in the standard basis will equal the summation of the scalar multiple of $ x_1 $ with $ \overrightarrow{v_i} $ and the scalar multiple of $ x_2 $ with $ \overrightarrow{v_j} $. Notice that this is precisely the definition of matrix multiplication.</p>

<script type="math/tex; mode=display">% <![CDATA[
T(\overrightarrow{v}) :=
\begin{bmatrix}
i_1 & j_1 \\
i_2 & j_2
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
=
\begin{bmatrix}
i_1 \\
i_2
\end{bmatrix} x_1 +
\begin{bmatrix}
j_1 \\
j_2
\end{bmatrix} x_2
=
\begin{bmatrix}
i_1x_1 & j_1x_2 \\
i_2x_1 & j_2x_2
\end{bmatrix} %]]></script>

<h2 id="interpretation-of-linear-transformation">Interpretation of Linear Transformation</h2>

<p>Matrix multiplication is the main example of a linear transformation. This linear transformation <em>moves</em> $ \overrightarrow{v} $ from the representation of the basis vectors in the coefficient matrix to the standard basis. Parsing the term Linear Transformation will help us understand it better.</p>

<p><strong>Transformation:</strong> A function that takes in an input vector and <em>moves</em> it.
<strong>Linear:</strong> The <em>movement</em> described above is linear if and only if</p>

<ol>
  <li>the origin remains</li>
  <li>lines stay as lines. (In other words, the set of vectors that have the same span remains the same after the transformation)</li>
</ol>

<p>Notice how matrix multiplication does not violate the two requirements of linear transformation</p>

<h2 id="interpretation-of-inverse-matrix-multiplication">Interpretation of Inverse Matrix Multiplication</h2>

<p>If <em>A</em> is a <em>function</em> that moves the input vector from the representation of the basis vectors in A to the standard basis, then $ A^{-1} $, being the <em>inverse function</em>, will do the opposite: move the input vector from the standard basis back to the basis vectors in A.</p>

<h2 id="linear-transformation-of-vectors-represented-by-other-basis-vectors">Linear Transformation of vectors represented by other basis vectors</h2>
<p>(10:20 - 11.00) of 3B1B video <a href="https://www.youtube.com/watch?v=kYB8IZa5AuE">here</a> animates this really well.</p>

<p><img src="/assets/img/linear-transformation-other-basis.png" alt="linear transformation of vectors represented by basis vectors" /></p>

<!-- omit in toc -->
<h2 id="credits">Credits</h2>
<p>For a more detailed explanation, check out 3Blue1Brown‚Äôs video <a href="https://www.youtube.com/watch?v=kYB8IZa5AuE">here</a>.</p>
:ET