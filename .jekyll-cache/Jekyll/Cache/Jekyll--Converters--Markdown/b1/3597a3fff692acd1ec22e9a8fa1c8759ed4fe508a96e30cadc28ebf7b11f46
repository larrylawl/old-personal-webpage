I"0<h2 id="learning-outcomes">Learning outcomes</h2>
<ol>
  <li>Intuition of Backpropagation</li>
  <li>Proof for the Partial Derivative Term in Backpropagation</li>
  <li>Calculus of the Gradient Vector of Backpropagation</li>
  <li>Fitting the Calculus with the Intuition</li>
</ol>

<!-- Disclaimer that this explains Andrew Ng's course -->
<p><em>(This article explains the calculus of the backpropagation algorithm in Andrew Ng’s machine learning course, which can be found <a href="https://www.coursera.org/learn/machine-learning">here</a>.)</em>
<img src="/assets/img/backpropagation-algo.png" alt="Andrew Ng's course: Backpropagation Algorithm" /></p>

<h2 id="prerequisite">Prerequisite</h2>
<p>Standard notations for neural networks as used in Andrew Ng’s course. With some discrepancies, definition of the notations can be found in this <a href="https://cs230.stanford.edu/files/Notation.pdf">link</a>.</p>

<h2 id="intuition-of-backpropagation">Intuition of Backpropagation</h2>
<p>Recall that the the purpose of Backpropagation is to <em>learn the parameters</em> for a <em>neural network.</em> Backpropagation learns the parameters of layer $ l $, from the error terms of the layer $ l + 1 $. Thus, the parameter learning is propagated from the back.</p>

<h2 id="proof-for-the-partial-derivative-term-in-backpropagation">Proof for the Partial Derivative Term in Backpropagation</h2>
<p>Suppose that the activation function chosen is the sigmoid function.
For simplicity, let us assume that the size of the training set is 1 (ie <em>m</em> = 1).</p>

<script type="math/tex; mode=display">a = g(z) := \sigma(z)</script>

<p>The claim:</p>

<script type="math/tex; mode=display">\frac{\partial{J(\Theta)}}{\partial \Theta^{(l)}_{i, j}} =a^{(l)}_{j} \delta^{(l+1)}_{i}</script>

<p>where $ \delta $, also known as the “error term”, is a recurrence relation such that</p>

<script type="math/tex; mode=display">\\ \delta^{(l+1)} := \frac{\partial{J(\Theta)}}{\partial z^{(l+1)}} = \delta^{(l+2)} \Theta^{(l+1)} g^{\prime}\left(z^{(l+1)}\right),
\\ \delta^{(L)} = a^{(L)} - y,</script>

<p>Proof can be found <a href="https://www.coursera.org/learn/machine-learning/discussions/weeks/5/threads/MVwN-LpLEeiBxhK7qjbMkg">here</a>. Alternatively, I have included pictures of the proof at the bottom of the page.</p>

<h2 id="calculus-of-the-gradient-vector-of-backpropagation">Calculus of the Gradient Vector of Backpropagation</h2>
<p>Since the neural network needs to be trained on <em>m</em> training samples, the partial derivative term needs to be averaged out over all these samples.</p>

<script type="math/tex; mode=display">{\frac{\partial J(\Theta)}{\partial \Theta^{(l)}_{i, j}}} 
= D^{(l)}_{i, j}
= \frac{1}{m} \sum_{k=1}^{m} \frac{\partial J_{k}}{\partial \Theta^{(l)}_{i, j}}</script>

<p>The summation term is precisely the update rule in the algorithm.</p>

<script type="math/tex; mode=display">\Delta_{i, j}^{(l)}:=\Delta_{i, j}^{(l)}+a_{j}^{(l)} \delta_{i}^{(l+1)}</script>

<p>Adding the regularisation terms give us</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \cdot & D_{i, j}^{(l)}:=\frac{1}{m}\left(\Delta_{i, j}^{(l)}+\lambda \Theta_{i, j}^{(l)}\right), \text { if } j \neq 0 \\ \cdot & D_{i, j}^{(l)}:=\frac{1}{m} \Delta_{i, j}^{(l)} \text { lf } j=0 \end{aligned} %]]></script>

<p>Finally, let’s compute the gradient vector, which will be passed as an argument to parameter learning methods.</p>

<blockquote>
  <p>Recall that aradient vector of a multivariable function packages the partial derivative of its parameters into a vector.</p>
</blockquote>

<p>Fitting the partial derivatives into the gradient vector, we get</p>

<script type="math/tex; mode=display">\nabla J=\left[\begin{array}{c}{\frac{\partial J}{\partial \Theta^{(1)}_{1, 1}}} \\ {\frac{\partial J}{\partial \Theta^{(1)}_{1, 2}}} \\ {\vdots} \\ {\frac{\partial J}{\partial \Theta^{(L-1)}_{s_{l+1}, s_{l}}}}\end{array}\right]</script>

<blockquote>
  <p>For simplicity, bias terms are ignored.</p>
</blockquote>

<h2 id="fitting-the-calculus-with-the-intuition">Fitting the Calculus with the Intuition</h2>

<p>Learning of a particular $ \Theta^{(l)}_{i, j} $ is dependent on the partial derivative of the cost function, which in turn is dependent on the error term.</p>

<script type="math/tex; mode=display">\frac{\partial{J(\Theta)}}{\partial \Theta^{(l)}_{i, j}} =a^{(l)}_{j} \delta^{(l+1)}_{i}</script>

<p>This error term is a recurrence relation. In other words, the error term of layer <em>l+1</em> is dependent on the error term of layer <em>l+2</em>, which ends at layer <em>L</em>.</p>

<script type="math/tex; mode=display">\\ \delta^{(l+1)} := \frac{\partial{J(\Theta)}}{\partial z^{(l+1)}} = \delta^{(l+2)} \Theta^{(l+1)} g^{\prime}\left(z^{(l+1)}\right)
\\ \delta^{(L)} = a^{(L)} - y,</script>

<p>Thus Backpropagation is a process wherein the neural network <em>learns from the back (ie the last layer).</em></p>

<h2 id="credits">Credits</h2>
<p>Andrew Ng’s Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning">here</a>.</p>

<p><a href="https://www.linkedin.com/in/neilostrove">Neil Ostrove</a> for the proof of the partial derivative term.</p>

<p><img src="/assets/img/backpropagation-proof-1.png" alt="Back propagation proof 1" /></p>

<p><img src="/assets/img/backpropagation-proof-2.png" alt="Back propagation proof 2" /></p>
:ET