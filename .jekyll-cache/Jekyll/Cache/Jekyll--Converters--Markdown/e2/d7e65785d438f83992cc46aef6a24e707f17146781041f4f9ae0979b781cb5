I"Á<div align="center">
    <i>"Only half of programming is coding. The other half is debugging." - Unknown</i>
</div>

<!-- omit in toc -->
<h2 id="learning-outcomes">Learning outcomes</h2>
<ul>
  <li><a href="#purpose-of-learning-curves">Purpose of Learning Curves</a></li>
  <li><a href="#interpreting-learning-curves">Interpreting Learning Curves</a></li>
  <li><a href="#implementation-of-learning-curves-in-matlaboctave">Implementation of Learning Curves in MatLab/Octave</a></li>
</ul>

<h2 id="purpose-of-learning-curves">Purpose of Learning Curves</h2>
<p>Learning curves diagnose <strong>underfitting/high bias</strong> (left graph) and <strong>overfitting/high variance</strong> (right graph) problems.</p>

<blockquote>
  <p>A model that underfits are biased for its assumption despite the training data saying otherwise.</p>
</blockquote>

<blockquote>
  <p>In statistics, variance can be interpreted as how far the data are spread out. A model that overfits make predictions on new data that are very far off the expected value, thus it has high variance.</p>
</blockquote>

<p><img src="/assets/img/fitting.jpg" alt="fitting" /></p>

<h2 id="interpreting-learning-curves">Interpreting Learning Curves</h2>
<p>Learning curve plots the error over the training set size <em>m</em>. More concretely, it is a function of <em>m</em>, that outputs the cost function at <em>m</em>.</p>

<blockquote>
  <p>Example of the MSE cost function to see the influence of <em>m</em> on <em>J</em>.
<script type="math/tex">J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}_{i}-y_{i}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x_{i}\right)-y_{i}\right)^{2}</script></p>
</blockquote>

<h2 id="implementation-of-learning-curves-in-matlaboctave">Implementation of Learning Curves in MatLab/Octave</h2>

<!-- omit in toc -->
<h2 id="credits">Credits</h2>
<p>Andrew Ngâ€™s Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning">here</a>.</p>
:ET