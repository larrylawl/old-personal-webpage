I"<div align="center">
    <i>"Only half of programming is coding. The other half is debugging." - Unknown</i>
</div>

<!-- omit in toc -->
<h2 id="learning-outcomes">Learning outcomes</h2>
<ul>
  <li><a href="#purpose-of-learning-curves">Purpose of Learning Curves</a></li>
  <li><a href="#definition-of-learning-curves">Definition of Learning Curves</a></li>
  <li><a href="#interpreting-learning-curves">Interpreting Learning Curves</a></li>
  <li><a href="#implementation-of-learning-curves-in-matlaboctave">Implementation of Learning Curves in MatLab/Octave</a></li>
</ul>

<h2 id="purpose-of-learning-curves">Purpose of Learning Curves</h2>
<p>Learning curves diagnose <strong>underfitting/high bias</strong> (left graph) and <strong>overfitting/high variance</strong> (right graph) problems.</p>

<blockquote>
  <p>A model that underfits are biased for its assumption despite the training data saying otherwise.</p>
</blockquote>

<blockquote>
  <p>In statistics, variance can be interpreted as how far the data are spread out. A model that overfits make predictions on new data that are very far off the expected value, thus it has high variance.</p>
</blockquote>

<p><img src="/assets/img/fitting.jpg" alt="fitting" /></p>

<h2 id="definition-of-learning-curves">Definition of Learning Curves</h2>
<p>Let <em>N</em> denote the training set size, and <em>n</em> denote the number of training data used in <em>N</em> (thus <em>n &lt; N</em>). Learning curve plots the training error and cross validation error over the training set size <em>N</em>.</p>

<p>More concretely, it is a function of <em>n</em>. But what does this function do? This function first trains the parameters given <em>n</em>, and outputs (1) <strong>$ J_{train}(\Theta) $</strong>: the cost function of these parameters (y-coordinate) on the <em>training set</em> of size <em>n</em> and (2) <strong>$ J_{CV}(\Theta) $</strong>: the cost function of these parameters on the <em>cross validation</em> set of size <em>N</em>.</p>

<p><img src="/assets/img/learning-curve-underfit.png" alt="Learning Curve of High Bias" /></p>

<p>Note that we used <em>n</em> for the $ J_{train}(\Theta) $ but <em>N</em> for $ J_{CV}(\Theta) $. This is because the learning curve seeks to compare between the cost function that the model was trained on (thus we use <em>n</em> training samples) with the cost function that shows the accurate cost of the current model, (thus we use <em>N</em> cross validation samples).</p>

<blockquote>
  <p>Example cost function (MSE) to illustrate the influence of <em>n</em> on the cost function.</p>
</blockquote>

<script type="math/tex; mode=display">J(\theta)=\frac{1}{2n} \sum_{i=1}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}=\frac{1}{2 n} \sum_{i=1}^{n}\left(h_{\theta}\left(x_{i}\right)-y_{i}\right)^{2}</script>

<h2 id="interpreting-learning-curves">Interpreting Learning Curves</h2>
<p><img src="/assets/img/fitting.jpg" alt="fitting" />
<img src="/assets/img/learning-curve-underfit.png" alt="Learning Curve of High Bias" /></p>

<p>Learning Curve of model with <em>High Bias</em>. In particular,</p>
<ol>
  <li><strong>Training error and test error converges.</strong> Consider the underfitting example on the elft. When <em>N</em> is high, the model is not able to optimise the line much more as it is trying to fit a linear equation to a quadratic/log set of data points. Since the model is not changing much, both the training and test error stagnates.</li>
  <li></li>
</ol>

<p><img src="/assets/img/learning-curve-overfit.png" alt="Learning Curve of High Variance" /></p>

<h2 id="implementation-of-learning-curves-in-matlaboctave">Implementation of Learning Curves in MatLab/Octave</h2>

<!-- omit in toc -->
<h2 id="credits">Credits</h2>
<p>Andrew Ngâ€™s Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning">here</a>.</p>
:ET