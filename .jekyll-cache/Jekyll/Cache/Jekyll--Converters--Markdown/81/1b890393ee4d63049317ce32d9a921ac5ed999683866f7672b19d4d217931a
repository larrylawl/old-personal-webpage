I"§<div align="center">
    <i>"Go down deep enough into anything and you will find mathematics." - Dean Schlicter</i>
</div>

<!-- Write down defn of eigen -->

<!-- omit in toc -->
<h2 id="learning-outcomes">Learning outcomes</h2>
<ul>
  <li><a href="#eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</a></li>
  <li><a href="#eigenbasis-and-diagonal-matrixes">Eigenbasis and Diagonal Matrixes</a></li>
  <li><a href="#prerequisite-geometric-interpretation-of-dot-product">Prerequisite: Geometric Interpretation of Dot Product</a></li>
  <li><a href="#large-margin-intuition">Large Margin Intuition</a></li>
  <li><a href="#math-behind-large-margin">Math Behind Large Margin</a></li>
</ul>

<h2 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</h2>

<h2 id="eigenbasis-and-diagonal-matrixes">Eigenbasis and Diagonal Matrixes</h2>

<ol>
  <li>Linear transformation of the vector retains its span; its only a scalar transformation of the vector . These special vectors are called the eigenvector, and the factor by which it was stretched is called the eigenvalue. Matrix vector multiplication gives the same result as sclaing by some factor lambda.
    <ol>
      <li>Negative â€“&gt; Flip</li>
      <li>Recall that span is all possible lc of the vector</li>
    </ol>
  </li>
  <li>Solving for that eqn â€“&gt; det of coefficient needs to be 0. Find a lambda that the det coefficient becomes 0</li>
  <li>Eigenbasis</li>
  <li>Diagonal Matrix and its usefulness
    <ol>
      <li>Interpretation of diagonal matrix â€“&gt; basis vectors are eigenvectors, diagonal entries being eigenvalues; usefulness of diagonal matrix</li>
    </ol>
  </li>
</ol>

<h2 id="prerequisite-geometric-interpretation-of-dot-product">Prerequisite: Geometric Interpretation of Dot Product</h2>

<p>The dot product is defined for two vectors X and Y by</p>

<script type="math/tex; mode=display">X \cdot Y = |X||Y|cos\theta</script>

<p>where $ \theta $ is the angle between the vectors and $ \lvert X \lvert $ is the length of the vector (aka the norm). Consequently, the dot product has the geometric interpretation of the length of projection X on Y, multiplied by the norm of Y (or vice versa, since dot product is commutative).</p>

<script type="math/tex; mode=display">X \cdot Y = p \times |Y|</script>

<p>where p is the length of projection of X on Y.</p>

<h2 id="large-margin-intuition">Large Margin Intuition</h2>
<p>Support vector machines (SVM) are known as large margin classifiers. Intuitively, this is because the minimisation of the cost function will lead to large margins (ie the margin in the center).</p>

<p><img src="/assets/img/svm-margins.png" alt="Support Vector Machine Margins" /></p>

<p>Let us now look at the math justifying this intuition.</p>

<h2 id="math-behind-large-margin">Math Behind Large Margin</h2>
<p>Recall that the cost function of SVM is as such:</p>

<script type="math/tex; mode=display">J(\theta) = C \sum_{i=1}^{m}\left[y^{(i)} cost_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) cos t_{0}\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}</script>

<blockquote>
  <p><em>C</em> is a penalisation parameter that have the opposite role of the parameter $ \lambda $. Concretely, when C decreases, $ \lambda $ increases, the regularisation term increases, hence it mitigates overfitting.</p>
</blockquote>

<p>If y = 1, the first function (ie <code class="highlighter-rouge">cost</code>) will be the graph on the left. If y = 0, it will be the graph on the right.</p>

<p><img src="/assets/img/svm.png" alt="Support Vector Machine" /></p>

<p>At the optimal minimisation of the cost function, the first term will equals zero. In order for the first term to be zero,</p>
<ol>
  <li>If $ y^{(i)} = 1 $, $ \theta^{T}x^{(i)} $ â‰¥ 1 (refer to the left graph)</li>
  <li>If $ y^{(i)} = 0 $, $ \theta^{T}x^{(i)} $ â‰¤ -1 (refer to the right graph)</li>
</ol>

<p>Thus we can rewrite the cost function as</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}</script>

<p>such that</p>
<ol>
  <li>If $ y^{(i)} = 1 $, $ \theta^{T}x^{(i)} $ â‰¥ 1</li>
  <li>If $ y^{(i)} = 0 $, $ \theta^{T}x^{(i)} $ â‰¤ -1</li>
</ol>

<p>Since</p>

<script type="math/tex; mode=display">\theta^{T}x = \theta \cdot x = p \times |\theta|</script>

<p>We can further rewrite the optimised cost function as</p>

<script type="math/tex; mode=display">J(\theta) = \frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}</script>

<p>such that</p>

<ol>
  <li>If $ y^{(i)} = 1 $, $ p^{(i)} \times \lvert \theta \lvert $ â‰¥ 1</li>
  <li>If $ y^{(i)} = 0 $, $ p^{(i)} \times \lvert \theta \lvert $ â‰¤ -1</li>
  <li>where $ p^{(i)} $ is the projection of $ x^{(i)} $ on the vector $ \theta $</li>
</ol>

<p>Since the optimal cost function is still dependent on $ \lvert \theta \lvert $, $ \lvert \theta \lvert $ will likely be small at the minimum point of the cost function. If $ \lvert \theta \lvert $ is small, then the projection <em>p</em> has to be large, thus the decision boundary is large.</p>

<p><img src="/assets/img/svm-margin-2.png" alt="Support Vector Margin" /></p>

<p>This is why, the SVM is associated as a large boundary classifier.</p>

<!-- omit in toc -->
<h2 id="credits">Credits</h2>
<p>Mathworld for the interpretation of Dot Product. Source <a href="http://mathworld.wolfram.com/DotProduct.html">here</a>.</p>

<p>Andrew Ngâ€™s Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning">here</a>.</p>
:ET