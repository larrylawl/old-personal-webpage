---
layout: post
title: "NUS, CS3243: Introduction to AI"
author: "Larry Law"
categories: notes
image: cs.png
hidden: true
---
Lecturer: Zick Yair <br>

<!-- omit in toc -->
# Table of Contents
- [Math Primers](#math-primers)
  - [NP and inherently hard problems](#np-and-inherently-hard-problems)
- [Introduction](#introduction)
  - [When Can a Machine Truly Think?](#when-can-a-machine-truly-think)
  - [Rational Agents](#rational-agents)
  - [Specifying Task Environment: PEAS](#specifying-task-environment-peas)
  - [Agenty Types](#agenty-types)
    - [Simple Reflex Agent](#simple-reflex-agent)
    - [Model-based Reflex Agent](#model-based-reflex-agent)
    - [Goal-based agent](#goal-based-agent)
    - [Utility-based agent](#utility-based-agent)
    - [Learning Agent](#learning-agent)
    - [Tradeoff between Exploitation and Exploration](#tradeoff-between-exploitation-and-exploration)
- [Searching for Solutions](#searching-for-solutions)
  - [Notations](#notations)
  - [Tree-Search vs Graph-Search](#tree-search-vs-graph-search)
- [Uninformed Search](#uninformed-search)
  - [Problem-Solving Agents](#problem-solving-agents)
  - [Uninformed Search Strategies](#uninformed-search-strategies)
    - [Breadth-First-Search (BFS)](#breadth-first-search-bfs)
    - [Uniform-Cost Search (UCS)](#uniform-cost-search-ucs)
    - [Depth-First Search](#depth-first-search)
    - [Depth-Limited Search (DLS)](#depth-limited-search-dls)
    - [Iterative Deepending Search (IDS)](#iterative-deepending-search-ids)
    - [Summary](#summary)
- [Informed Search](#informed-search)
  - [Best-First-Search](#best-first-search)
  - [Greedy best-first search](#greedy-best-first-search)
    - [Conditions for optimality: Admissibility and consistency](#conditions-for-optimality-admissibility-and-consistency)
  - [A* Search](#a-search)
  - [Dominance](#dominance)
  - [Generating admissible heuristics from relaxed problems](#generating-admissible-heuristics-from-relaxed-problems)
  - [Generating admissible heuristics from subproblems: Pattern databases](#generating-admissible-heuristics-from-subproblems-pattern-databases)
- [Local Search](#local-search)
  - [Local Search Algorithms](#local-search-algorithms)
- [Adversarial Search](#adversarial-search)
  - [Game: Problem Formulation](#game-problem-formulation)
  - [Minimax Algorithm](#minimax-algorithm)
  - [Alpha-Beta Pruning](#alpha-beta-pruning)
  - [Imperfect Real-Time Decisions](#imperfect-real-time-decisions)

# Math Primers
## NP and inherently hard problems
1. Complexity analysis analyzes problems rather than algos
2. **P:** Class of polynomial problems that can be solved in time \$ O(n^k) \$ for some k
3. **NP:** Class of nondeterministic polynomial problems
4. **NP-complete:** Subclass of NP. "Complete" is used in the sense of "most extreme", and thus refers to the hardest problems in the class NP. It has been proven that either the NP-complete problems are in P or none of them is.
5. **co-NP:** Complement of NP (ie for every decision problem in NP, there is a corresponding problem in co-NP with the "yes" and "no" answers reversed)
6. **co-NP-complete:** Hardest problems in co-NP

> Is P = NP? That is, do NP problems have polynomial-time algorithms? This has never been proven.



# Introduction
## When Can a Machine Truly Think?

**Turing Test:**  A computer passes the test if a human interrogator, after
posing some *written* questions, cannot tell whether the written responses come from a person or from a computer. 

**Winograd Schema:** 
1. You are given *m* Winograd schema, with the context word chosen uniformly at random.
2. Design an AI that can correctly resolve a significant number of them.

Above examples show that single test for Intelligence is...
1. Difficult to resolve
2. Tests tend to be 1) over-specified or 2) very subjective (tradeoff!)
3. Results will be debatable

## Rational Agents

**Agent:** Function maps *percept histories* to *actions*

**Rational agent:**
1. For each possible percept sequence, select an action that is expected to *maximize its performance measure*
2. given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.

## Specifying Task Environment: PEAS

Task environment is used for intelligent agent design. PEAS stands for
1. Performance measure: objective criterion for measuring success of an agent's behaviour.
2. Environment
3. Actuators
4. Sensors

Properties of Task Environments include
1. **Fully observable (vs. partially observable):** sensors provide access to the complete state of the environment at each point in time.
2. **Deterministic (vs. stochastic):** The next state of the environment is completely determined by the current state and the action executed by the agent
3. **Episodic (or memorylessness) (vs. sequential):** The choice of current action does not depend on actions in past episodes
4. **Static (vs. dynamic):** The environment is unchanged while an agent is
deliberating.
5. **Discrete (vs. continuous):** A finite no. of distinct states, percepts, and actions. (turn based)
6. **Single agent (vs. multi-agent):** An agent operating by itself in an env.

## Agenty Types
### Simple Reflex Agent
1. Passive: only acts when it observes a percept
2. Updates *state* based on *percept* only.
3. Easy to implement.

### Model-based Reflex Agent
1. Passive: only acts when it observes a percept
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Goal-based agent
1. Has **goals**, acts to achieve them (not passive)
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Utility-based agent
1. Has **utility function**, acts to achieve them (not passive)
   1. **Motivation:** Goals alone are not enough to generate high quality behaviour (ie many action sequences can get taxi to its goal destination, but some are quicker, safer, more reliable, or cheaper)
   2. **Utility function allows for a more general performance measure**
> Utility Function vs Performance Measure: Function internally used by the agent to evaluate performance vs Function to evaluate agent's external behaviour
2. Updates *state* based on *percept*, current *state*, most recent *action*, and *model of the world*

### Learning Agent

> Distinction between learning element, which is responsible for making improvements, and the performance element, which is responsible for selecting external actions. 

The performance element is what we have previously considered
to be the entire agent: it takes in percepts and decides on actions. The learning element uses feedback from the critic on how the agent is doing and determines how the performance element should be modified to do better in the future.

Learning element is responsible for making improvement

### Tradeoff between Exploitation and Exploration
Tradeoff between
1. **Exploitation**: maximizing its expected utility according to its current knowledge of the world
2. **Exploration**: trying to learn more about the world since this may improve its future gains

# Searching for Solutions

## Notations
1. **State:** the state in the state space to which the node coressponds
2. **Explore:** Check whether it's a goal state. If not, add the neighbouring nodes into the frontier.
3. **Frontier:** nodes that we have seen but have yet to explore (at initialisation, the frontier is just the source)
4. **Node:** data structure constituting part of sesarch tree. It includes state, parent node, action, and path cost *g(n)*.

> Nodes vs States: Node is a bookkeeping data structure used to represent the search tree. A state corresponds to a configuration of the world. Thus nodes are on particular paths whereas states are not. Furthermore, two different nodes are allowed to contain the same world state (if that state is generated via two different search paths)

## Tree-Search vs Graph-Search
> Algorithms that forget their history are doomed to repeat it.

Recall that any two vertices in a tree is connected by a unique simple path. Thus Tree-Search algorithm is applied to Trees, wherein there will be no repeated states. In contrast, graph-search algorithm is applied to graphs, thus it handles repeated states (in bold)

![Tree vs Graph Search](/assets/img/2020-16-1-CS3243/tree-vs-graph-search.png)

# Uninformed Search
Uninformed search strategies use only information available in the problem definition

## Problem-Solving Agents
**Environment:** Fully observable, deterministic, discrete

**Problem Formulation** <br />
A problem can be defined formally by 5 components (textbook writes 5, but examples in textbook uses 6.)
1. **State**
2. **Initial State** that the agent starts in.
3. **Actions:** A description of the possible actions available to the agent (`Actions(s)`)
4. **Transition Model:** A description of what each action does (`Result(s,a)`)
> Together, the initial state, actions, and transition model define the **state space** of the problem: the set of all states reachable from the initial state by any sequence of actions.
4. **Goal Test:**
   1. Is the state *s* equal the goal state?
   2. Explicit set of goal states
   3. Implicit function (eg `isCheckmate(s)`)
5. **Path cost**
   1. *c(s, a, s'):* the step cost of taking action a in state s to reach state s'.
   2. Additive: Sum of the step costs

> There can be more than 1 goal nodes

## Uninformed Search Strategies
**Assumption:** Search in a *tree* data structure

**Evaluation Criteria:**
1. **Completeness:** always find a solution if exists
2. **optimality:** find a least-cost soln (implies completeness)
3. **time complexity:** no. of nodes generated
4. **space complexity:** max no. of nodes in memory

**Problem Parameters**
1. *b*: maximum # of successors of any node
2. *d*: depth of shallowest goal node (not optimal)
3. *m*: max depth of search tree


### Breadth-First-Search (BFS)
**Idea:** Expand shallowest unexpanded node <br />
**Implementation:** Frontier is a FIFO queue

Properties of BFS on Graph-based search
1. **Complete?:** Yes (if *b* is finite)
2. **Optimal:** No (unless step costs are equal, ie unweighted). Counterexample: 2 goal nodes, \$ v_1 \$ \$ v_2 \$, with cost of 100 and 1. BFS will return \$ v_1 \$, which is suboptimal.
3. **Time:** \$ O(b) + O(b^2) + ... + O(b^d) = O(b^d) \$
4. **Space:** Max size of frontier \$ O(b^d) \$

Properties hold for tree-based search

> Memory requirements are a bigger problem: one might wait 13 days for the solution to an important problem with search depth *d* 12, but no personal computer has the petabyte of memory it would take.

### Uniform-Cost Search (UCS)
**Idea:** Expand least-path-cost, unexpanded node <br />
**Frontier:** Priority Queue ordered by path cost *g* <br />
Equivalent to BFS if all step costs are equal

> <s', c>, where c refers to the additive sum of step costs from source node to s'

> Dijkstra's algorithm, which is perhaps better-known, can be regarded as a variant of uniform-cost search, where there is no goal state and processing continues until all nodes have been removed from the priority queue, i.e. until shortest paths to all nodes (not just a goal node) have been determined

> When a node is explored, the path there is guaranteed to be the cheapest.

1. **Complete?:** Yes (if all step costs are ≥ \$ \epsilon \$)
2. **Optimal:** Yes (shortest path nodes expanded first)
3. **Time:** \$ O(b^{1 + \lfloor \frac{C^*}{\epsilon} \rfloor}) \$, where _C_ is the optimal cost
4. **Space:** \$ O(b^{1 + \lfloor \frac{C^*}{\epsilon} \rfloor}) \$

> Derivation of time complexity

Let e denote \$ \epsilon \$

|Steps  |Worst Case Time Complexity  |Distance  |
|---|---|---|
|1  |b  |≥e  |
|2  |b^2  |≥2e  |
|...  |  |  |
|floor(C*/e)  |b^floor(C*/e)  |≥floor(C*/e)e  |

Note that the lower bound for step floor(C\*/e) is floor(C\*/e)e, which is lesser than C\* in the worse case time scenario. Thus, in order to achieve a distance of at least C\*, the max no. of steps will be floor(C\*/e) + 1.

> At step k, keep ≤ \$ b^k \$ nodes in frontier.

Worse case scenario (upper bound):
1. Step 1: source node expands *b* successors
2. Step 2: For each of these *b* successors, they'll have *b* successors. Total of \$ b^2 \$ successors
3. ...
4. Step k: \$ b^k \$ successors

### Depth-First Search
**Idea:** Expand deepest unexpanded node <br />
**Implementation:** Frontier is a LIFO queue

1. **Complete?:** No on infinite depth graphs. No on tree-search version. (CA: infinite loop)
2. **Optimal:** No (unless step costs are equal, ie unweighted)
3. **Time:** \$ O(b^m) \$ (ie all of the nodes in the search tree)
4. **Space:** Max size of frontier \$ O(bm) \$ (can be \$ O(m) \$)

> What is the advantage of DFS > BFS?

DFS has a better space complexity for tree search, not graph search. (for graph search, worse case scenario would be the entire graph)

> Why is DFS space complexity \$ O(bm) \$
Depth first tree search needs to store only a single path from the root to a leaf node (*m*), along with the remaining unexpanded sibling nodes for each node on the path (*b*).

However, the runtime can be further improved from *O(bm)* to *O(m)* by using **backtracking search.**

In bactracking, only one successor is generated than all successors; each partially expanded node remembers which successor to generate next (eliminate *b*). The idea of generating a successor by *modifying the current state description* instead of directly copying it.

### Depth-Limited Search (DLS)
**Idea:** run DFS with depth limit *l* <br />

### Iterative Deepending Search (IDS)
**Idea:** perform DLSs with increasing depth limit until goal node is found <br />; better if state space is large and depth of soln is unknown

1. **Complete?:** Yes (if *b* is finite)
2. **Optimal:** No (unless step costs is 1)
3. **Time:** \$ O(b^d) \$ 
4. **Space:** \$ O(bd) \$ (can be \$ O(d) \$)

> Though IDS may seem wasteful because states are generated multiple times, asymptotically the time complexity is the same as BFS. Intuitively, this is because *in a search tree* most of the nodes are in the bottom level, so it does not matter much that the upper levels are generated multiple times.

### Summary
![uninformed search summary](/assets/img/2020-16-1-CS3243/uninformed-search-summary.png)

> These algos are applied on graphs, not trees.

> Assume b > 1 (else the algo breaks down: e.g. time-complexity of BFS  for b=1 would be O(d) however the result on the slides says O(b^d)=O(1^d)=O(1). Now O(d) is not in O(1))

# Informed Search
Exploit problem-specific knowledge (beyond the problem definition) to obtain heuristics to guide search

## Best-First-Search
1. **Idea:** Use an evaluation function *f(n)* for each node *n*
   1. Cost Estimate: Expand node with lowest evaluation/cost first
2. **Implementation:** Identical to uniform-cost search, except for the use of *f* instead of *g* to order the priority queue. Most best-first algo include as a component of *f* a heuristic function denote *h(n):*

$$
h(n) = \text{estimated cost of the cheapest path from the state at node n to a goal state}
$$

> Notice that *h(n)* takes a node as inpute, but unlike *g(n)* it depends only on the *state* at the node.

3. **Special Cases** (different choices of *f*):
   1. Greedy best-first search
   2. A* search

## Greedy best-first search

**Intuition:** Greedy best-first search expands the node that appears to be closest to goal. Thus, it evaluates nodes by just using the heuristic function (ie *f(n) = h(n)*)

**Graph Version**
1. **Complete?:** Yes (if *b* is finite). No if tree search even if *b* is finite: infinite loop (similar to DFS)
2. **Optimal:** No (shortest path to Bucharest: 418km)
3. **Time:** \$ O(b^m) \$ (ie entire graph), but a good heuristic can reduce complexity substantially
4. **Space:** Max size of frontier \$ O(b^m) \$ 

### Conditions for optimality: Admissibility and consistency

**h(n) to be an admissible heuristic:** heuristic that never overestimates the cost to reach the goal. (ie *f(n) = g(n) + h(n)* never overestimates the true cost of a soln along the current path through *n*).

> E.g: Straight-line distance, which is by definition the shortest path, thus it cannot be an overestimate.

**h(n) is consistent:** if, for every node *n* and every successor *n'* of *n* generated by action *a*, the estimated cost of reaching the goal from *n* is no greater than the step cost of getting to *n'* plus the estimated cost of reaching the goal from *n':*

$$
h(n) ≤ c(n, a, n') + h(n')
$$

> Every consistent heuristic is also admissible. Proof [here](http://reason.cs.uiuc.edu/eyal/classes/f06/cs440/hw/hw6/hw6_sol.pdf)

This is a form of the general triangle inequality, which stipulates that each side of a triangle (*h(n)*) cannot be longer than the sum of the other two sides. (*c(n, a, n') + h(n')*)

> Here, the triangle is formed by \$ n \$, \$ n' \$, and the goal \$ G_n \$ closest to \$ n \$. 

Likewise, this triangle inequality applies for an admissible heuristic: if there were a route from *n* to \$ G_n \$ via *n'* that was cheaper than *h(n)*, then \$ h(n) ≥ C^\* \$, wherein \$ C^\* \$ refers to the cost to reach \$ G_n \$ and in example, refers to the route via *n'*.

## A* Search
**Intuition:** Improves on uniform cost search (*g(n)*), by avoiding expanding paths that are already expensive (which is given by *h(n)*)

**Evaluation function:** *f(n) = g(n) + h(n)*, where
1. *g(n)* = cost of reaching *n* from start node
2. *h(n)* = cost estimate from *n* to goal
3. *f(n)* = estimated cost of cheapest path **through** n to goal

**Theorem:** If *h(n)* is admissible, then A* using *Tree-Search* is optimal <br />
**Theorem:** If *h(n)* is consistent, then A* using *graph-search* is optimal (proof in page 95 of textbook)

<!-- Q: Slide 30: graph-search discards new paths to a repeated state. Should be repeated nodes right? -->

> Lemma 1: if *h(n)* is consistent, then the values of *f(n)* along any path are nondecreasing

> Lemma 2: (Given that *h(n)* is consistent) Whenever A* selects a node *n* for expansion, the optimal path to that node has been found.

> However, note that \$ A^\* \$ search still adds back an explored node into the frontier. However, if *h(n)* is consistent, then the *f(n)* along any path is non decreasing, thus the node will not be explored. 

1. **Complete?:** Yes (if there is a finite no. of nodes with \$ f(n) ≤ C^\*, C^\* \text{ is the cost of the optimal solution path} \$)
> 1) \$ A^* \$ expands all nodes with \$ f(n) < C^* \$ 2) \$ A^* \$ expands some nodes right on the 'goal contour' before selecting a goal node
1. **Optimal:** Yes
2. **Time:** \$ O(b^{h^{\ast}(s_0) - h(s_0)}) \$ where \$ h^{\ast}(s_0) \$ is the actual cost of getting from root to goal, and \$ h(s_0) \$ is the approximated cost. The difference represents the relative error of the heuristic.
3. **Space:** Max size of frontier \$ O(b^m) \$ (all nodes)

## Dominance
If \$ h_2(n) ≥ h_1(n) \$ for all n (both admissible), then \$ h_2 \$ dominates \$ h_1 \$. It follows that \$ h_2 \$ incurs lower search cost than \$ h_1 \$.

> Proof in page 104 of textbook

## Generating admissible heuristics from relaxed problems
1. A problem with fewer restrictions on the actions is called a **relaxed problem**
2. The cost of an optimal solution to a relaxed problem is also an admissible heuristic for the original problem

> Manhatten distance/ no. of misplaced tiles for k-puzzle problem

## Generating admissible heuristics from subproblems: Pattern databases
"Lower bound of the cost of the optimal solution of this subproblem is a lower bound on the cost of the complete problem." Thus it is admissible.

**Pattern Databases.** The idea behind pattern databases is to store these exact solution costs for every possible subproblem instance.


# Local Search
1. The **path** to a goal is irrelevant (unlike (un)informed search); the goal state itself is the solution.
2. State space = set of "complete" configurations
3. Find final configuration satisfying constraints

## Local Search Algorithms
Maintain single "current best" state and try to improve it

Advantages
1. Very little/constant memory
2. Find reasonable solutions in large state space

**State-Space Landscape**
![State Space Landscape](/assets/img/2020-16-1-CS3243/state-space-landscape.png)

A landscape has both *location* (defined by the state) and *elevation* (defined by the heuristic cost fn or objective fn)

**Hill Climbing Problem**

> Hill climbing is sometimes called **greedy local search** because it grabs a good neighbor state without thinking ahead about where to go next.

**Drawback.** Depending on initial state, can get stuck in local maxima.

**Non-guaranteed fixes.** sideway moves, random restarts

# Adversarial Search
*In which we examine the problems that arise when we try to plan ahead in a world
where other agents are planning against us.*

## Game: Problem Formulation
1. \$ S_0 \$: Initial state
2. \$ Player(s) \$: Defines which player has the move in a state
3. \$ Actions(s) \$: Returns the set of legal moves in a state
4. \$ Result(s, a) \$: The **transition model** which defines the result of a move
5. \$ Terminal-Test(s) \$: True when the game is over, and false otherwise. States where the game has ended are called **terminal states.**
6. \$ Utility(s,p) \$: A utility function which defines the final numeric value for a game that ends in terminal state *s*  for a player *p*.

**Winning Strategy**

**Definition.** A strategy \$ s^\*_1 \$ for player 1 is called winning if for any strategy \$ s_2 \$ by player 2, the game ends with player 1 as the winner.

**Definition.** A strategy \$ t^\*_1 \$ is called non-losing if for any strategy \$ s_2 \$ by player 2, the game ends in either a tie or a win for player 1.

## Minimax Algorithm

![minimax](/assets/img/2020-16-1-CS3243/minimax.png)

> \$ max_{a \in Actions(s)} \$: Choose the action *a* st *Result(s,a)* returns the max value. 

Properties of Minimax
1. **Complete?:** Yes (if game tree is finite)
2. **Optimal:** Yes (optimal gameplay)
3. **Time:** \$ O(b^m) \$
4. **Space:** Like DFS: \$ O(bm) \$

5. Runs in time polynomial in tree size
6. Returns a sub-perfect Nash equilibrium: the best action at every choice node.

## Alpha-Beta Pruning
**Motivation.** Number of game states minimax has to examine is exponential in the depth, *m*, of the tree.

**Intuition.**
1. Maintain a lower bound \$ \alpha \$ and upper bound \$ \beta \$ of the values of, respectively, MAX's and MIN's nodes seen thus far
2. Prune subtrees that will never affect minimax decision

> "If you have an idea that is surely bad, don't take the time to see how truly awful it is." - Pat Winston

**Definition.**
1. MAX node n: \$ \alpha(n) = \$ highest observed value found on path from *n*; initially \$ \alpha(n) = -\inf \$
2. MIN node n: \$ \beta(n) =\$ the lowest observed value found on path from *n*; initially \$ \beta(n) = +\inf \$

**Pruning**
1. Given a MIN node n, stop searching below *n* if there is some MAX ancestor *i* of *n* with \$ \alpha(i) \geq \beta(n) \$
2. Given a MAX node *n*, stop searching below *n* if there is some MIN ancestor *i* of *n* with \$ \beta(i) \leq \alpha(n) \$


**Analysis of pruning**
1. When we prune a branch, it **never** affects final outcome
2. Good move ordering improves effectiveness of pruning
3. Perfect ordering: \$ O(b^{\frac{m}{2}}) \$

## Imperfect Real-Time Decisions
**Problem.** Maximum depth of tree

![h-minimax](/assets/img/2020-16-1-CS3243/h-minimax.png)

**Solution.**
1. Replace utility function by a heuristic evaluation function *Eval*, which estimates the position's utility
2. Replace the terminal test by a **cutoff test** that decides when to apply *Eval*
    1. A more robust approach of choosing *d* to cut off is to apply iterative deepening.

> *Eval* is simiilar to the heuristic function of Chapter 3

