I"l<h2 id="learning-outcomes">Learning outcomes</h2>
<ol>
  <li>Why do we need feature scaling?</li>
  <li>How is feature scaling implemented?</li>
</ol>

<h2 id="why-do-we-need-feature-scaling">Why do we need feature scaling?</h2>
<p><strong>To speed up gradient descent</strong>. Recall that the purpose of gradient descent is to <em>learn our model’s parameters.</em> Suppose that the range of our parameters are uneven. A larger range will lead to a slower descent, while a smaller range will lead to a faster descent. The uneven simultaneous update of our parameters will cause the descent to <em>oscillate</em> which is inefficient.</p>
<blockquote>
  <p>Not sure what is gradient descent? Simultaenous update? Read more in the post about it <a href="./gradient-descent.html">here</a></p>
</blockquote>

<!-- TODO: Include example -->

<h2 id="how-do-we-implement-feature-scaling">How do we implement feature scaling?</h2>
<p>One common way is to <strong>normalise the feature</strong> (ie subtracting the mean and dividing by the standard deviation).</p>

<script type="math/tex; mode=display">x_{i}:=\frac{x_{i}-\mu_{i}}{s_{i}}</script>

<h2 id="credits">Credits</h2>
<p>Andrew Ng’s Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning?utm_source=gg&amp;utm_medium=sem&amp;utm_content=93-BrandedSearch-INTL&amp;campaignid=1599063752&amp;adgroupid=58953588605&amp;device=c&amp;keyword=coursera%20courses&amp;matchtype=b&amp;network=g&amp;devicemodel=&amp;adpostion=1t1&amp;creativeid=303554599611&amp;hide_mobile_promo&amp;gclid=EAIaIQobChMIvfCauaSo5gIVF4iPCh1U1gK3EAAYASABEgLY6vD_BwE">here</a></p>

:ET