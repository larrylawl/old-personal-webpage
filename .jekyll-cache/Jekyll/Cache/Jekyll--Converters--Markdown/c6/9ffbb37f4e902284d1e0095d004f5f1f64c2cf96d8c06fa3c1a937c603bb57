I"l<div align="center">
    <i>"Only half of programming is coding. The other half is debugging." - Unknown</i>
</div>

<!-- I'm writing this because...
It's a useful debugging method. -->

<!-- omit in toc -->
<h2 id="learning-outcomes">Learning outcomes</h2>
<ul>
  <li><a href="#purpose-of-learning-curves">Purpose of Learning Curves</a></li>
  <li><a href="#definition-of-learning-curves">Definition of Learning Curves</a></li>
  <li><a href="#interpreting-learning-curves">Interpreting Learning Curves</a></li>
  <li><a href="#implementation-of-learning-curves-in-matlaboctave">Implementation of Learning Curves in MatLab/Octave</a></li>
</ul>

<h2 id="purpose-of-learning-curves">Purpose of Learning Curves</h2>
<p>Learning curves diagnose <strong>underfitting/high bias</strong> (left graph) and <strong>overfitting/high variance</strong> (right graph) problems.</p>

<p><img src="/assets/img/fitting.jpg" alt="fitting" /></p>

<blockquote>
  <p>A model that underfits are biased for its assumption despite the training data saying otherwise.</p>
</blockquote>

<blockquote>
  <p>In statistics, variance can be interpreted as how far the data are spread out. A model that overfits make predictions on new data that are very far off the expected value, thus it has high variance.</p>
</blockquote>

<h2 id="definition-of-learning-curves">Definition of Learning Curves</h2>
<p>Let <em>N</em> denote the training set size, and <em>n</em> denote the number of training data used in <em>N</em> (thus <em>n &lt; N</em>). Learning curve plots the training error and cross validation error over the training set size <em>N</em>.</p>

<p>More concretely, it is a function of <em>n</em>. But what does this function do? This function first trains the parameters given <em>n</em> training samples, and outputs (1) <strong>$ J_{train}(\Theta) $</strong>: the cost function of these parameters (y-coordinate) on the <em>training set of size n</em> and (2) <strong>$ J_{CV}(\Theta) $</strong>: the cost function of these parameters on the <em>cross validation set of size N</em>.</p>

<p><img src="/assets/img/learning-curve-underfit.png" alt="Learning Curve of High Bias" /></p>

<p>Note that we used <em>n</em> for the $ J_{train}(\Theta) $ but <em>N</em> for $ J_{CV}(\Theta) $. This is because the learning curve seeks to compare between the cost function that the model was trained on (which is over <em>n</em> training samples) with the cost function that shows the accurate cost of the current model, (which is over <em>N</em> cross validation samples).</p>

<blockquote>
  <p>Example cost function (MSE) to illustrate the influence of <em>n</em> on the cost function.</p>
</blockquote>

<script type="math/tex; mode=display">J(\theta)=\frac{1}{2n} \sum_{i=1}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}=\frac{1}{2 n} \sum_{i=1}^{n}\left(h_{\theta}\left(x_{i}\right)-y_{i}\right)^{2}</script>

<h2 id="interpreting-learning-curves">Interpreting Learning Curves</h2>
<p><img src="/assets/img/fitting.jpg" alt="fitting" />
<img src="/assets/img/learning-curve-underfit.png" alt="Learning Curve of High Bias" /></p>

<p>Learning Curve of model with <em>High Bias</em>. In particular, note that the <strong>training error and test error converges.</strong> Consider the underfitting example on the left. When <em>N</em> is high, the model is not able to optimise the line much more as it is trying to fit a linear equation to a data points of quadratic/log nature. Since the model is not changing much, both the training and test error stagnates. Consequently, <strong>collecting more training data</strong> will not improve a model with high bias.</p>

<p><img src="/assets/img/learning-curve-overfit.png" alt="Learning Curve of High Variance" /></p>

<p>Learning Curve of model with <em>High Variance</em>. In particular, note that <strong>test error decreases as N increases</strong>. Consider the overfitting example on the right. With more data points, the overfitted model forces the polynomial equation to fit the data points. While it gets harder to fit the equation (thus training error increases), the equation also gets closer to the correct structure of the data points (thus test error decreases). Consequently, <strong>collecting more training data</strong> will improve a model with high variance.</p>

<h2 id="implementation-of-learning-curves-in-matlaboctave">Implementation of Learning Curves in MatLab/Octave</h2>
<div class="language-m highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span> <span class="p">[</span><span class="n">error_train</span><span class="p">,</span> <span class="n">error_val</span><span class="p">]</span> <span class="o">=</span> <span class="k">...</span>
    <span class="n">learningCurve</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
<span class="k">...</span>
<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">m</span>
    <span class="c1">% Training on i training samples to obtain parameters.</span>
    <span class="n">X_train_i</span> <span class="o">=</span> <span class="n">X</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="p">,</span> <span class="p">:);</span>
    <span class="n">y_train_i</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="p">);</span>
    <span class="p">[</span><span class="n">theta</span><span class="p">]</span> <span class="o">=</span> <span class="n">trainLinearReg</span><span class="p">(</span><span class="n">X_train_i</span><span class="p">,</span> <span class="n">y_train_i</span><span class="p">,</span> <span class="n">lambda</span><span class="p">);</span> 

    <span class="c1">% Compute and store J_train and J_val.</span>
    <span class="p">[</span><span class="n">error_train</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">linearRegCostFunction</span><span class="p">(</span><span class="n">X_train_i</span><span class="p">,</span> <span class="n">y_train_i</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

    <span class="p">[</span><span class="n">error_val</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">linearRegCostFunction</span><span class="p">(</span><span class="n">Xval</span><span class="p">,</span> <span class="n">yval</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span> <span class="c1">% Note that it over the entire cross validation set.</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Now simply plot <code class="highlighter-rouge">error_train</code> and <code class="highlighter-rouge">error_val</code> over <code class="highlighter-rouge">m</code> :).</p>

<!-- omit in toc -->
<h2 id="credits">Credits</h2>
<p>Andrew Ngâ€™s Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning">here</a>.</p>
:ET