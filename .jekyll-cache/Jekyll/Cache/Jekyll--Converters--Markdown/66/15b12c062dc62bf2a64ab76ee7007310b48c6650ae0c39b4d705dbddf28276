I"d<h2 id="learning-outcomes">Learning outcomes</h2>
<ol>
  <li>Purpose for Random Initialisation in Neural Networks</li>
  <li>Heuristic Justifying (1)</li>
</ol>

<h2 id="purpose-for-random-initialisation-in-neural-networks">Purpose for Random Initialisation in Neural Networks</h2>
<p>In Backpropagation, parameters are randomly initialised in order to avoid the case wherein they are <em>symmetrical</em> (aka <em>breaking symmetry</em>). More concretely, they are symmetrical in the sense that all parameters of a particular layer <em>l</em> are the same.</p>

<blockquote>
  <p>Parameters of different layers need not necessarily be the same for the symmetric property to hold.</p>
</blockquote>

<h3 id="what-is-wrong-with-symmetrical-initialisation">What is wrong with symmetrical initialisation?</h3>

<p>The claim is that <em>the symmetrical property largely persists even after backpropagation</em>. Assuming that this claim holds, for all units of layer <em>l</em>, $ \Theta^{(l - 1)} $ is the same (by assumption) and $ a^{(l - 1)} $ is the same as well (by the neural networks model). Thus $ a^{(l)} $ will be the same, which makes the units of layer <em>l</em> <strong>redundant.</strong></p>

<p>Let us look at the heuristic justifying the claim made above, and explain the qualifier <em>‚Äúlargely‚Äù</em> later.</p>

<h2 id="heuristic-justifying-random-initialisation">Heuristic Justifying Random Initialisation</h2>
<p>Suppose symmetrical intialisation of parameters.</p>

<p>Let‚Äôs find out the effect of backpropagation on the parameters using <em>first-step analysis</em>. That is, we analyse the effect of one iteration, and make a generalised conclusion for <em>n</em> iterations from there.</p>

<p>Recall that the Backpropagation learns parameters using the partial derivative of the cost function with respect to that parameter. More concretely,</p>

<script type="math/tex; mode=display">\frac{\partial{J(\Theta)}}{\partial \Theta^{(l)}} =a^{(l)} \delta^{(l+1)}, \delta \text{ to be defined}</script>

<p>Since the parameters are the same in the first iteration, $ a^{(l)} $ will be the same for all units in layer <em>l</em>. Let us take a look at $ \delta $, which is also known as the ‚Äúerror term‚Äù.</p>

<script type="math/tex; mode=display">\\ \delta^{(l+1)} := \frac{\partial{J(\Theta)}}{\partial z^{(l+1)}} = \delta^{(l+2)} \Theta^{(l+1)} g^{\prime}\left(z^{(l+1)}\right),
\\ \delta^{(L)} = a^{(L)} - y,</script>

<ol>
  <li><em>Suppose that the neural network has only 1 output.</em> $ \delta^{(L)} $ will thus be a real number.</li>
  <li>$ \Theta^{(l+1)} $ is the same (by assumption)</li>
  <li>Derivative of activation function <code class="highlighter-rouge">g</code> is the same across all layers. As explained earlier, z of any layer <em>l</em> will be the same. Thus this derivative term will be the same.</li>
  <li>From (1) - (3), all error terms of layer <em>l</em> will be the same.</li>
</ol>

<p>Putting it all together, we have shown that the partial derivative for units of the same layer will be the same. Consequently, the output from the parameter learning function will be the same, thus the symmetrical property persists after <em>1</em> iteration. Since the parameters are still symmetrical, we can expect them to still be symmetric after <em>n</em> iterations.</p>

<h2 id="explaining-the-qualifier---largely-persists">Explaining the qualifier - ‚Äúlargely persists‚Äù</h2>
<p>We have made an assumption that the neural network only has 1 output. What if there are more than 1 output? $ \delta^{(L)} $ will now be a matrix, with all entries being the same except for the entry wherein y = 1.</p>

<p>However, since all other entries remain the same, the symmetrical property <em>largely</em> persists after Backpropagation.</p>

<h2 id="credits">Credits</h2>
<p>Andrew Ng‚Äôs Machine Learning course. Source <a href="https://www.coursera.org/learn/machine-learning">here</a>.</p>
:ET